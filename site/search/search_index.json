{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#ovn-kubernetes-a-robust-kubernetes-networking-provider","title":"OVN-Kubernetes: A Robust Kubernetes Networking Provider","text":"<p>OVN-Kubernetes (Open Virtual Networking - Kubernetes) is an open-source project that provides a robust networking solution for Kubernetes clusters with OVN (Open Virtual Networking) and Open vSwitch (Open Virtual Switch) at its core. It is a Kubernetes networking conformant plugin written according to the CNI (Container Network Interface) specifications.</p>"},{"location":"#challenges-for-cluster-networking-in-kubernetes-ecosystem","title":"Challenges for Cluster Networking in Kubernetes Ecosystem","text":"<ul> <li>Kubernetes at its core is a bunch of powerful APIs like Pods, Services, EndpointSlices   and NetworkPolicies.<ul> <li>If we look at The Kubernetes Network Model it imposes a set of fundamental requirements for how networking is expected to behave for pods in a cluster.</li> <li>At the end of the day we need a robust networking platform to fulfill those requirements.</li> </ul> </li> <li>Handling complicated telco and enterprise networking scenarios.<ul> <li>Kubernetes Cluster Networking Model addresses the basic set of problems such as communication between containers in a pod, communication between pods, allowing external (outside the cluster) entities to talk to pods via services, ingress and gateway-api on a primary network level using clusterCIDRs and serviceCIDRs.</li> <li>However when we start talking about more powerful networking abstractions like secondary networks, multi-homing and more fine grained cluster egress controls outside the normal node SNATing the Kubernetes Networking Model falls short</li> </ul> </li> <li>Lifecycle management of networking infrastructure.<ul> <li>In addition to having an implementation that fulfills the basic requirements of the Kubernetes Networking model, we also need a level driven controller that automatically takes care of the lifecycle and health of the critical networking aspects.</li> </ul> </li> <li>Networking on Kubernetes is not simple.<ul> <li>There are complicated features such as service traffic policies, terminating endpoints, topology aware hints which involve multiple moving pieces and components that needs a \"driver\" that ensures they work as expected.</li> </ul> </li> </ul>"},{"location":"#what-is-ovn-kubernetes","title":"What is OVN-Kubernetes?","text":"<p>OVN-Kubernetes was designed to precisely solve the above problems in a Kubernetes cluster. The OVN-Kubernetes plugin watches the Kubernetes API. It acts on the generated Kubernetes cluster events by creating and configuring the corresponding OVN logical constructs in the OVN database for those events. OVN (which is an abstraction on top of Open vSwitch) converts these logical constructs into logical flows in its database and programs the OpenFlow flows on the node, which enables networking on a Kubernetes cluster.</p> <p></p> <p>The key functionalities and features that OVN-Kubernetes provides include:</p> <ul> <li>Kubernetes Core Networking Conformance<ul> <li>Creates pod networking including IP Address Management (IPAM) allocation and virtual ethernet (veth) interface for the pod.</li> <li>Programs overlay based networking implementation for Kubernetes clusters using Generic Network Virtualization Encapsulation GENEVE tunnels that enables pod-to-pod communication.</li> <li>Implements Kubernetes Services &amp; EndpointSlices through OVN Load Balancers.</li> <li>Implements Kubernetes NetworkPolicies and AdminNetworkPolicies through OVN Access Control Lists (ACLs).</li> <li>Supports IPv4/IPv6 Dual-Stack clusters.</li> </ul> </li> <li>Fine grained Cluster Egress Traffic Controls<ul> <li>Multiple External Gateways (MEG) allows for multiple dynamically or statically assigned egress next-hop gateways by utilizing OVN ECMP routing features.</li> <li>Implements Quality of Service (QoS) Differentiated Services Code Point (DSCP) for traffic egressing the cluster through OVN QoS.</li> <li>Provides ability to send egress traffic from cluster workloads using an admin-configured source IP (EgressIP) to outside the cluster using OVN Logical Router Policies and Network Address Translations.</li> <li>Provides ability to send egress traffic from cluster workloads using the service load balancer IP (EgressService) to outside the cluster using OVN Logical Router Policies and Network Address Translations.</li> <li>Provides the ability to restrict egress traffic from cluster workloads (Egress Firewall) using OVN Access Control Lists.</li> </ul> </li> <li>Advanced Networking Features<ul> <li>Implements Hybrid Networking to provide support for mixed Windows/Linux clusters using VXLAN tunnels.</li> <li>Provides IP Multicast using OVN IGMP snooping and relays.</li> <li>Provides ability to offload networking tasks from CPU to NIC using OVS Hardware Offload thus providing increased data-plane performance.</li> <li>Adds support for creating secondary and local networks in addition to the default primary networks</li> </ul> </li> </ul>"},{"location":"#why-choose-ovn-kubernetes-in-kubernetes-ecosystem","title":"Why choose OVN-Kubernetes in Kubernetes ecosystem?","text":"<p>Networking is the backbone for any Kubernetes cluster. Kubernetes at its core provides an extensive set of Networking APIs and features that need to be implemented in a Kubernetes conformant manner for networking to work properly in a Kubernetes cluster. The aim of OVN-Kubernetes project is to be able to provide a pluggable and seamless networking platform for Kubernetes end users. The project focuses strictly on enhancing networking for the Kubernetes platform and includes a wide variety of features that are critical to enterprise and telco users. OVN-Kubernetes community members are active in upstream Kubernetes (particularly in SIG Network) to create new features and then realize them in OVN-Kubernetes. In addition to a feature rich platform, the project also aims to be a highly scalable and performant Networking Platform.</p> <p>For more details, please see the following:</p> <ul> <li>OVN-Kubernetes Architecture</li> <li>Deploying OVN-Kubernetes cluster using KIND</li> <li>Deploying OVN-Kubernetes CNI using Helm</li> <li>Setup and Building OVN-Kubernetes for instructions   on setting up your development environment and building ovn-kubernetes.</li> <li>CLI Guide \u2014 hello</li> <li>Contributing to OVN-Kubernetes for how to get involved   in our project</li> <li>Meet the Community for details on community   meeting details.</li> </ul>"},{"location":"api-reference/admin-epbr-api-spec/","title":"API Reference","text":""},{"location":"api-reference/admin-epbr-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/admin-epbr-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/admin-epbr-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>AdminPolicyBasedExternalRoute</li> </ul>"},{"location":"api-reference/admin-epbr-api-spec/#adminpolicybasedexternalroute","title":"AdminPolicyBasedExternalRoute","text":"<p>AdminPolicyBasedExternalRoute is a CRD allowing the cluster administrators to configure policies for external gateway IPs to be applied to all the pods contained in selected namespaces. Egress traffic from the pods that belong to the selected namespaces to outside the cluster is routed through these external gateway IPs.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>AdminPolicyBasedExternalRoute</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> AdminPolicyBasedExternalRouteSpec Required: {}  <code>status</code> AdminPolicyBasedRouteStatus"},{"location":"api-reference/admin-epbr-api-spec/#adminpolicybasedexternalroutespec","title":"AdminPolicyBasedExternalRouteSpec","text":"<p>AdminPolicyBasedExternalRouteSpec defines the desired state of AdminPolicyBasedExternalRoute</p> <p>Appears in: - AdminPolicyBasedExternalRoute</p> Field Description Default Validation <code>from</code> ExternalNetworkSource From defines the selectors that will determine the target namespaces to this CR. <code>nextHops</code> ExternalNextHops NextHops defines two types of hops: Static and Dynamic. Each hop defines at least one external gateway IP. MinProperties: 1"},{"location":"api-reference/admin-epbr-api-spec/#adminpolicybasedroutestatus","title":"AdminPolicyBasedRouteStatus","text":"<p>AdminPolicyBasedRouteStatus contains the observed status of the AdminPolicyBased route types.</p> <p>Appears in: - AdminPolicyBasedExternalRoute</p> Field Description Default Validation <code>lastTransitionTime</code> Time Captures the time when the last change was applied. <code>messages</code> string array An array of Human-readable messages indicating details about the status of the object. <code>status</code> StatusType A concise indication of whether the AdminPolicyBasedRoute resource is applied with success"},{"location":"api-reference/admin-epbr-api-spec/#dynamichop","title":"DynamicHop","text":"<p>DynamicHop defines the configuration for a dynamic external gateway interface. These interfaces are wrapped around a pod object that resides inside the cluster. The field NetworkAttachmentName captures the name of the multus network name to use when retrieving the gateway IP to use. The PodSelector and the NamespaceSelector are mandatory fields.</p> <p>Appears in: - ExternalNextHops</p> Field Description Default Validation <code>podSelector</code> LabelSelector PodSelector defines the selector to filter the pods that are external gateways. Required: {}  <code>namespaceSelector</code> LabelSelector NamespaceSelector defines a selector to filter the namespaces where the pod gateways are located. Required: {}  <code>networkAttachmentName</code> string NetworkAttachmentName determines the multus network name to use when retrieving the pod IPs that will be used as the gateway IP.When this field is empty, the logic assumes that the pod is configured with HostNetwork and is using the node's IP as gateway. <code>bfdEnabled</code> boolean BFDEnabled determines if the interface implements the Bidirectional Forward Detection protocol. Defaults to false. false"},{"location":"api-reference/admin-epbr-api-spec/#externalnetworksource","title":"ExternalNetworkSource","text":"<p>ExternalNetworkSource contains the selectors used to determine the namespaces where the policy will be applied to</p> <p>Appears in: - AdminPolicyBasedExternalRouteSpec</p> Field Description Default Validation <code>namespaceSelector</code> LabelSelector NamespaceSelector defines a selector to be used to determine which namespaces will be targeted by this CR"},{"location":"api-reference/admin-epbr-api-spec/#externalnexthops","title":"ExternalNextHops","text":"<p>ExternalNextHops contains slices of StaticHops and DynamicHops structures. Minimum is one StaticHop or one DynamicHop.</p> <p>Validation: - MinProperties: 1</p> <p>Appears in: - AdminPolicyBasedExternalRouteSpec</p> Field Description Default Validation <code>static</code> StaticHop array StaticHops defines a slice of StaticHop. This field is optional. <code>dynamic</code> DynamicHop array DynamicHops defines a slices of DynamicHop. This field is optional."},{"location":"api-reference/admin-epbr-api-spec/#statichop","title":"StaticHop","text":"<p>StaticHop defines the configuration of a static IP that acts as an external Gateway Interface. IP field is mandatory.</p> <p>Appears in: - ExternalNextHops</p> Field Description Default Validation <code>ip</code> string IP defines the static IP to be used for egress traffic. The IP can be either IPv4 or IPv6. Pattern: <code>^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])$|^s*((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]d|1dd|[1-9]?d)(.(25[0-5]|2[0-4]d|1dd|[1-9]?d)){3}))|:)))(%.+)?s*</code> Required: {}  <code>bfdEnabled</code> boolean BFDEnabled determines if the interface implements the Bidirectional Forward Detection protocol. Defaults to false. false"},{"location":"api-reference/admin-epbr-api-spec/#statustype","title":"StatusType","text":"<p>Underlying type: string</p> <p>StatusType defines the types of status used in the Status field. The value determines if the deployment of the CR was successful or if it failed.</p> <p>Appears in: - AdminPolicyBasedRouteStatus</p>"},{"location":"api-reference/egress-firewall-api-spec/","title":"API Reference","text":""},{"location":"api-reference/egress-firewall-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/egress-firewall-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/egress-firewall-api-spec/#egressfirewalldestination","title":"EgressFirewallDestination","text":"<p>EgressFirewallDestination is the target that traffic is either allowed or denied to</p> <p>Validation: - MaxProperties: 1 - MinProperties: 1</p> <p>Appears in: - EgressFirewallRule</p> Field Description Default Validation <code>cidrSelector</code> string cidrSelector is the CIDR range to allow/deny traffic to. If this is set, dnsName and nodeSelector must be unset. <code>dnsName</code> string dnsName is the domain name to allow/deny traffic to. If this is set, cidrSelector and nodeSelector must be unset. Pattern: <code>^([A-Za-z0-9-]+\\.)*[A-Za-z0-9-]+\\.?$</code> <code>nodeSelector</code> LabelSelector nodeSelector will allow/deny traffic to the Kubernetes node IP of selected nodes. If this is set,cidrSelector and DNSName must be unset."},{"location":"api-reference/egress-firewall-api-spec/#egressfirewallport","title":"EgressFirewallPort","text":"<p>EgressFirewallPort specifies the port to allow or deny traffic to</p> <p>Appears in: - EgressFirewallRule</p> Field Description Default Validation <code>protocol</code> string protocol (tcp, udp, sctp) that the traffic must match. Pattern: <code>^TCP|UDP|SCTP$</code> <code>port</code> integer port that the traffic must match Maximum: 65535 Minimum: 1"},{"location":"api-reference/egress-firewall-api-spec/#egressfirewallrule","title":"EgressFirewallRule","text":"<p>EgressFirewallRule is a single egressfirewall rule object</p> <p>Appears in: - EgressFirewallSpec</p> Field Description Default Validation <code>type</code> EgressFirewallRuleType type marks this as an \"Allow\" or \"Deny\" rule Pattern: <code>^Allow|Deny$</code> <code>ports</code> EgressFirewallPort array ports specify what ports and protocols the rule applies to <code>to</code> EgressFirewallDestination to is the target that traffic is allowed/denied to MaxProperties: 1 MinProperties: 1"},{"location":"api-reference/egress-firewall-api-spec/#egressfirewallruletype","title":"EgressFirewallRuleType","text":"<p>Underlying type: string</p> <p>EgressNetworkFirewallRuleType indicates whether an EgressNetworkFirewallRule allows or denies traffic</p> <p>Validation: - Pattern: <code>^Allow|Deny$</code></p> <p>Appears in: - EgressFirewallRule</p>"},{"location":"api-reference/egress-firewall-api-spec/#egressfirewallspec","title":"EgressFirewallSpec","text":"<p>EgressFirewallSpec is a desired state description of EgressFirewall.</p> <p>Appears in: - EgressFirewall</p> Field Description Default Validation <code>egress</code> EgressFirewallRule array a collection of egress firewall rule objects"},{"location":"api-reference/egress-firewall-api-spec/#egressfirewallstatus","title":"EgressFirewallStatus","text":"<p>Appears in: - EgressFirewall</p> Field Description Default Validation <code>status</code> string <code>messages</code> string array"},{"location":"api-reference/egress-ip-api-spec/","title":"API Reference","text":""},{"location":"api-reference/egress-ip-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/egress-ip-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/egress-ip-api-spec/#egressipspec","title":"EgressIPSpec","text":"<p>EgressIPSpec is a desired state description of EgressIP.</p> <p>Appears in: - EgressIP</p> Field Description Default Validation <code>egressIPs</code> string array EgressIPs is the list of egress IP addresses requested. Can be IPv4 and/or IPv6.This field is mandatory. <code>namespaceSelector</code> LabelSelector NamespaceSelector applies the egress IP only to the namespace(s) whose labelmatches this definition. This field is mandatory. <code>podSelector</code> LabelSelector PodSelector applies the egress IP only to the pods whose labelmatches this definition. This field is optional, and in case it is not set:results in the egress IP being applied to all pods in the namespace(s)matched by the NamespaceSelector. In case it is set: is intersected withthe NamespaceSelector, thus applying the egress IP to the pods(in the namespace(s) already matched by the NamespaceSelector) whichmatch this pod selector."},{"location":"api-reference/egress-ip-api-spec/#egressipstatus","title":"EgressIPStatus","text":"<p>Appears in: - EgressIP</p> Field Description Default Validation <code>items</code> EgressIPStatusItem array The list of assigned egress IPs and their corresponding node assignment."},{"location":"api-reference/egress-ip-api-spec/#egressipstatusitem","title":"EgressIPStatusItem","text":"<p>The per node status, for those egress IPs who have been assigned.</p> <p>Appears in: - EgressIPStatus</p> Field Description Default Validation <code>node</code> string Assigned node name <code>egressIP</code> string Assigned egress IP"},{"location":"api-reference/egress-qos-api-spec/","title":"API Reference","text":""},{"location":"api-reference/egress-qos-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/egress-qos-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/egress-qos-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>EgressQoS</li> </ul>"},{"location":"api-reference/egress-qos-api-spec/#egressqos","title":"EgressQoS","text":"<p>EgressQoS is a CRD that allows the user to define a DSCP value for pods egress traffic on its namespace to specified CIDRs. Traffic from these pods will be checked against each EgressQoSRule in the namespace's EgressQoS, and if there is a match the traffic is marked with the relevant DSCP value.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>EgressQoS</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> EgressQoSSpec <code>status</code> EgressQoSStatus"},{"location":"api-reference/egress-qos-api-spec/#egressqosrule","title":"EgressQoSRule","text":"<p>Appears in: - EgressQoSSpec</p> Field Description Default Validation <code>dscp</code> integer DSCP marking value for matching pods' traffic. Maximum: 63 Minimum: 0  <code>dstCIDR</code> string DstCIDR specifies the destination's CIDR. Only traffic headingto this CIDR will be marked with the DSCP value.This field is optional, and in case it is not set the rule is appliedto all egress traffic regardless of the destination. Format: cidr  <code>podSelector</code> LabelSelector PodSelector applies the QoS rule only to the pods in the namespace whose labelmatches this definition. This field is optional, and in case it is not setresults in the rule being applied to all pods in the namespace."},{"location":"api-reference/egress-qos-api-spec/#egressqosspec","title":"EgressQoSSpec","text":"<p>EgressQoSSpec defines the desired state of EgressQoS</p> <p>Appears in: - EgressQoS</p> Field Description Default Validation <code>egress</code> EgressQoSRule array a collection of Egress QoS rule objects"},{"location":"api-reference/egress-qos-api-spec/#egressqosstatus","title":"EgressQoSStatus","text":"<p>EgressQoSStatus defines the observed state of EgressQoS</p> <p>Appears in: - EgressQoS</p> Field Description Default Validation <code>status</code> string A concise indication of whether the EgressQoS resource is applied with success. <code>conditions</code> Condition array An array of condition objects indicating details about status of EgressQoS object."},{"location":"api-reference/egress-service-api-spec/","title":"API Reference","text":""},{"location":"api-reference/egress-service-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/egress-service-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/egress-service-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>EgressService</li> </ul>"},{"location":"api-reference/egress-service-api-spec/#egressservice","title":"EgressService","text":"<p>EgressService is a CRD that allows the user to request that the source IP of egress packets originating from all of the pods that are endpoints of the corresponding LoadBalancer Service would be its ingress IP. In addition, it allows the user to request that egress packets originating from all of the pods that are endpoints of the LoadBalancer service would use a different network than the main one.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>EgressService</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> EgressServiceSpec <code>status</code> EgressServiceStatus"},{"location":"api-reference/egress-service-api-spec/#egressservicespec","title":"EgressServiceSpec","text":"<p>EgressServiceSpec defines the desired state of EgressService</p> <p>Appears in: - EgressService</p> Field Description Default Validation <code>sourceIPBy</code> SourceIPMode Determines the source IP of egress traffic originating from the pods backing the LoadBalancer Service.When <code>LoadBalancerIP</code> the source IP is set to its LoadBalancer ingress IP.When <code>Network</code> the source IP is set according to the interface of the Network,leveraging the masquerade rules that are already in place.Typically these rules specify SNAT to the IP of the outgoing interface,which means the packet will typically leave with the IP of the node. Enum: [LoadBalancerIP Network]  <code>nodeSelector</code> LabelSelector Allows limiting the nodes that can be selected to handle the service's traffic when sourceIPBy=LoadBalancerIP.When present only a node whose labels match the specified selectors can be selectedfor handling the service's traffic.When it is not specified any node in the cluster can be chosen to manage the service's traffic. <code>network</code> string The network which this service should send egress and corresponding ingress replies to.This is typically implemented as VRF mapping, representing a numeric id or string nameof a routing table which by omission uses the default host routing."},{"location":"api-reference/egress-service-api-spec/#egressservicestatus","title":"EgressServiceStatus","text":"<p>EgressServiceStatus defines the observed state of EgressService</p> <p>Appears in: - EgressService</p> Field Description Default Validation <code>host</code> string The name of the node selected to handle the service's traffic.In case sourceIPBy=Network the field will be set to \"ALL\"."},{"location":"api-reference/egress-service-api-spec/#sourceipmode","title":"SourceIPMode","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [LoadBalancerIP Network]</p> <p>Appears in: - EgressServiceSpec</p>"},{"location":"api-reference/introduction/","title":"OVN-Kubernetes API Reference Guide","text":"<p>OVN-Kubernetes implements core Kubernetes Networking features as well as advanced features using Custom Resource Definitions.</p>"},{"location":"api-reference/introduction/#core-kubernetes-features-api-reference","title":"Core Kubernetes features API Reference","text":"<p>This section contains the API Reference guide for core Kubernetes APIs that is watched by OVN-Kubernetes</p> <ul> <li>Nodes</li> <li>Namespaces</li> <li>Pods</li> <li>Services</li> <li>Endpoints</li> <li>EndpointSlices</li> <li>NetworkPolicies</li> </ul>"},{"location":"api-reference/introduction/#extended-kubernetes-features-api-reference","title":"Extended Kubernetes features API Reference","text":"<p>This section contains the API Reference guide for extended Kubernetes ecosystem APIs that is watched and implemented by OVN-Kubernetes</p> <ul> <li>AdminNetworkPolicies - shipped by SIG-Network-Policy-API</li> <li>BaselineAdminNetworkPolicies - shipped by SIG-Network-Policy-API</li> <li>NetworkAttachmentDefinitions - shipped by Kubernetes-Network-Plumbing-WG</li> <li>MultiNetworkPolicies - shipped by Kubernetes-Network-Plumbing-WG</li> </ul>"},{"location":"api-reference/introduction/#ovn-kubernetes-features-api-reference","title":"OVN-Kubernetes features API Reference","text":"<p>This section contains the API Reference guide for features designed and implemented by OVN-Kubernetes</p> <ul> <li>EgressIP</li> <li>EgressService</li> <li>EgressQoS</li> <li>EgressFirewall</li> <li>AdminPolicyBasedExternalRoutes</li> <li>UserDefinedNetwork</li> <li>RouteAdvertisements</li> <li>VTEP</li> </ul>"},{"location":"api-reference/routeadvertisements-api-spec/","title":"API Reference","text":""},{"location":"api-reference/routeadvertisements-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/routeadvertisements-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the RouteAdvertisements v1 API group</p>"},{"location":"api-reference/routeadvertisements-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>RouteAdvertisements</li> <li>RouteAdvertisementsList</li> </ul>"},{"location":"api-reference/routeadvertisements-api-spec/#advertisementtype","title":"AdvertisementType","text":"<p>Underlying type: string</p> <p>AdvertisementType determines the type of advertisement.</p> <p>Validation: - Enum: [PodNetwork EgressIP]</p> <p>Appears in: - RouteAdvertisementsSpec</p> Field Description <code>PodNetwork</code> PodNetwork determines that the pod network is advertised. <code>EgressIP</code> EgressIP determines that egress IPs are being advertised."},{"location":"api-reference/routeadvertisements-api-spec/#routeadvertisements","title":"RouteAdvertisements","text":"<p>RouteAdvertisements is the Schema for the routeadvertisements API</p> <p>Appears in: - RouteAdvertisementsList</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>RouteAdvertisements</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> RouteAdvertisementsSpec <code>status</code> RouteAdvertisementsStatus"},{"location":"api-reference/routeadvertisements-api-spec/#routeadvertisementslist","title":"RouteAdvertisementsList","text":"<p>RouteAdvertisementsList contains a list of RouteAdvertisements</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>RouteAdvertisementsList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> RouteAdvertisements array"},{"location":"api-reference/routeadvertisements-api-spec/#routeadvertisementsspec","title":"RouteAdvertisementsSpec","text":"<p>RouteAdvertisementsSpec defines the desired state of RouteAdvertisements</p> <p>Appears in: - RouteAdvertisements</p> Field Description Default Validation <code>targetVRF</code> string targetVRF determines which VRF the routes should be advertised in. Optional: {}  <code>networkSelectors</code> NetworkSelectors networkSelectors determines which network routes should be advertised.Only ClusterUserDefinedNetworks and the default network can be selected. Required: {}  <code>nodeSelector</code> LabelSelector nodeSelector limits the advertisements to selected nodes. This fieldfollows standard label selector semantics. Required: {}  <code>frrConfigurationSelector</code> LabelSelector frrConfigurationSelector determines which FRRConfigurations will theOVN-Kubernetes driven FRRConfigurations be based on. This field followsstandard label selector semantics. Required: {}  <code>advertisements</code> AdvertisementType array advertisements determines what is advertised. Enum: [PodNetwork EgressIP] MaxItems: 2 MinItems: 1 Required: {}"},{"location":"api-reference/routeadvertisements-api-spec/#routeadvertisementsstatus","title":"RouteAdvertisementsStatus","text":"<p>RouteAdvertisementsStatus defines the observed state of RouteAdvertisements. It should always be reconstructable from the state of the cluster and/or outside world.</p> <p>Appears in: - RouteAdvertisements</p> Field Description Default Validation <code>status</code> string status is a concise indication of whether the RouteAdvertisementsresource is applied with success. Optional: {}  <code>conditions</code> Condition array conditions is an array of condition objects indicating details aboutstatus of RouteAdvertisements object. Optional: {}"},{"location":"api-reference/userdefinednetwork-api-spec/","title":"API Reference","text":""},{"location":"api-reference/userdefinednetwork-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/userdefinednetwork-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>ClusterUserDefinedNetwork</li> <li>ClusterUserDefinedNetworkList</li> <li>UserDefinedNetwork</li> <li>UserDefinedNetworkList</li> </ul>"},{"location":"api-reference/userdefinednetwork-api-spec/#accessvlanconfig","title":"AccessVLANConfig","text":"<p>AccessVLANConfig describes an access VLAN configuration.</p> <p>Appears in: - VLANConfig</p> Field Description Default Validation <code>id</code> integer id is the VLAN ID (VID) to be set for the network.id should be higher than 0 and lower than 4095. Maximum: 4094 Minimum: 1"},{"location":"api-reference/userdefinednetwork-api-spec/#cidr","title":"CIDR","text":"<p>Underlying type: string</p> <p>Validation: - MaxLength: 43</p> <p>Appears in: - DualStackCIDRs - Layer2Config - Layer3Subnet - LocalnetConfig</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#clusteruserdefinednetwork","title":"ClusterUserDefinedNetwork","text":"<p>ClusterUserDefinedNetwork describe network request for a shared network across namespaces.</p> <p>Appears in: - ClusterUserDefinedNetworkList</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>ClusterUserDefinedNetwork</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> ClusterUserDefinedNetworkSpec Required: {}  <code>status</code> ClusterUserDefinedNetworkStatus"},{"location":"api-reference/userdefinednetwork-api-spec/#clusteruserdefinednetworklist","title":"ClusterUserDefinedNetworkList","text":"<p>ClusterUserDefinedNetworkList contains a list of ClusterUserDefinedNetwork.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>ClusterUserDefinedNetworkList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> ClusterUserDefinedNetwork array"},{"location":"api-reference/userdefinednetwork-api-spec/#clusteruserdefinednetworkspec","title":"ClusterUserDefinedNetworkSpec","text":"<p>ClusterUserDefinedNetworkSpec defines the desired state of ClusterUserDefinedNetwork.</p> <p>Appears in: - ClusterUserDefinedNetwork</p> Field Description Default Validation <code>namespaceSelector</code> LabelSelector NamespaceSelector Label selector for which namespace network should be available for. Required: {}  <code>network</code> NetworkSpec Network is the user-defined-network spec Required: {}"},{"location":"api-reference/userdefinednetwork-api-spec/#clusteruserdefinednetworkstatus","title":"ClusterUserDefinedNetworkStatus","text":"<p>ClusterUserDefinedNetworkStatus contains the observed status of the ClusterUserDefinedNetwork.</p> <p>Appears in: - ClusterUserDefinedNetwork</p> Field Description Default Validation <code>conditions</code> Condition array Conditions slice of condition objects indicating details about ClusterUserDefineNetwork status."},{"location":"api-reference/userdefinednetwork-api-spec/#dualstackcidrs","title":"DualStackCIDRs","text":"<p>Underlying type: CIDR</p> <p>Validation: - MaxItems: 2 - MaxLength: 43 - MinItems: 1</p> <p>Appears in: - Layer2Config - Layer3Config - LocalnetConfig</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#dualstackips","title":"DualStackIPs","text":"<p>Underlying type: IP</p> <p>Validation: - MaxItems: 2 - MinItems: 1</p> <p>Appears in: - Layer2Config</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#evpnconfig","title":"EVPNConfig","text":"<p>EVPNConfig contains configuration options for networks operating in EVPN mode.</p> <p>Appears in: - NetworkSpec</p> Field Description Default Validation <code>vtep</code> string VTEP is the name of the VTEP CR that defines VTEP IPs for EVPN. MinLength: 1 Required: {}  <code>macVRF</code> VRFConfig MACVRF contains the MAC-VRF configuration for Layer 2 EVPN.This field is required for Layer2 topology and forbidden for Layer3 topology. <code>ipVRF</code> VRFConfig IPVRF contains the IP-VRF configuration for Layer 3 EVPN.This field is required for Layer3 topology and optional for Layer2 topology."},{"location":"api-reference/userdefinednetwork-api-spec/#ip","title":"IP","text":"<p>Underlying type: string</p> <p>Appears in: - DualStackIPs</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#ipamconfig","title":"IPAMConfig","text":"<p>Validation: - MinProperties: 1</p> <p>Appears in: - Layer2Config - LocalnetConfig</p> Field Description Default Validation <code>mode</code> IPAMMode Mode controls how much of the IP configuration will be managed by OVN.<code>Enabled</code> means OVN-Kubernetes will apply IP configuration to the SDN infrastructure and it will also assign IPsfrom the selected subnet to the individual pods.<code>Disabled</code> means OVN-Kubernetes will only assign MAC addresses and provide layer 2 communication, letting usersconfigure IP addresses for the pods.<code>Disabled</code> is only available for Secondary networks.By disabling IPAM, any Kubernetes features that rely on selecting pods by IP will no longer function(such as network policy, services, etc). Additionally, IP port security will also be disabled for interfaces attached to this network.Defaults to <code>Enabled</code>. Enum: [Enabled Disabled]  <code>lifecycle</code> NetworkIPAMLifecycle Lifecycle controls IP addresses management lifecycle.The only allowed value is Persistent. When set, the IP addresses assigned by OVN-Kubernetes will be persisted in an<code>ipamclaims.k8s.cni.cncf.io</code> object. These IP addresses will be reused by other pods if requested.Only supported when mode is <code>Enabled</code>. Enum: [Persistent]"},{"location":"api-reference/userdefinednetwork-api-spec/#ipammode","title":"IPAMMode","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [Enabled Disabled]</p> <p>Appears in: - IPAMConfig</p> Field Description <code>Enabled</code> <code>Disabled</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#layer2config","title":"Layer2Config","text":"<p>Appears in: - NetworkSpec - UserDefinedNetworkSpec</p> Field Description Default Validation <code>role</code> NetworkRole Role describes the network role in the pod.Allowed value is \"Secondary\".Secondary network is only assigned to pods that use <code>k8s.v1.cni.cncf.io/networks</code> annotation to select given network. Enum: [Primary Secondary] Required: {}  <code>mtu</code> integer MTU is the maximum transmission unit for a network.MTU is optional, if not provided, the globally configured value in OVN-Kubernetes (defaults to 1400) is used for the network. Maximum: 65536 Minimum: 576  <code>subnets</code> DualStackCIDRs Subnets are used for the pod network across the cluster.Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.The format should match standard CIDR notation (for example, \"10.128.0.0/16\").This field must be omitted if <code>ipam.mode</code> is <code>Disabled</code>. MaxItems: 2 MaxLength: 43 MinItems: 1  <code>reservedSubnets</code> CIDR array reservedSubnets specifies a list of CIDRs reserved for static IP assignment, excluded from automatic allocation.reservedSubnets is optional. When omitted, all IP addresses in <code>subnets</code> are available for automatic assignment.IPs from these ranges can still be requested through static IP assignment.Each item should be in range of the specified CIDR(s) in <code>subnets</code>.The maximum number of entries allowed is 25.The format should match standard CIDR notation (for example, \"10.128.0.0/16\").This field must be omitted if <code>subnets</code> is unset or <code>ipam.mode</code> is <code>Disabled</code>. MaxItems: 25 MaxLength: 43 MinItems: 1  <code>infrastructureSubnets</code> CIDR array infrastructureSubnets specifies a list of internal CIDR ranges that OVN-Kubernetes will reserve for internal network infrastructure.Any IP addresses within these ranges cannot be assigned to workloads.When omitted, OVN-Kubernetes will automatically allocate IP addresses from <code>subnets</code> for its infrastructure needs.When there are not enough available IPs in the provided infrastructureSubnets, OVN-Kubernetes will automatically allocate IP addresses from subnets for its infrastructure needs.When <code>reservedSubnets</code> is also specified the CIDRs cannot overlap.When <code>defaultGatewayIPs</code> is also specified, the default gateway IPs must belong to one of the infrastructure subnet CIDRs.Each item should be in range of the specified CIDR(s) in <code>subnets</code>.The maximum number of entries allowed is 4.The format should match standard CIDR notation (for example, \"10.128.0.0/16\").This field must be omitted if <code>subnets</code> is unset or <code>ipam.mode</code> is <code>Disabled</code>. MaxItems: 4 MaxLength: 43 MinItems: 1  <code>defaultGatewayIPs</code> DualStackIPs defaultGatewayIPs specifies the default gateway IP used in the internal OVN topology.Dual-stack clusters may set 2 IPs (one for each IP family), otherwise only 1 IP is allowed.This field is only allowed for \"Primary\" network.It is not recommended to set this field without explicit need and understanding of the OVN network topology.When omitted, an IP from the subnets field is used. MaxItems: 2 MinItems: 1  <code>joinSubnets</code> DualStackCIDRs JoinSubnets are used inside the OVN network topology.Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.This field is only allowed for \"Primary\" network.It is not recommended to set this field without explicit need and understanding of the OVN network topology.When omitted, the platform will choose a reasonable default which is subject to change over time. MaxItems: 2 MaxLength: 43 MinItems: 1  <code>ipam</code> IPAMConfig IPAM section contains IPAM-related configuration for the network. MinProperties: 1"},{"location":"api-reference/userdefinednetwork-api-spec/#layer3config","title":"Layer3Config","text":"<p>Appears in: - NetworkSpec - UserDefinedNetworkSpec</p> Field Description Default Validation <code>role</code> NetworkRole Role describes the network role in the pod.Allowed values are \"Primary\" and \"Secondary\".Primary network is automatically assigned to every pod created in the same namespace.Secondary network is only assigned to pods that use <code>k8s.v1.cni.cncf.io/networks</code> annotation to select given network. Enum: [Primary Secondary] Required: {}  <code>mtu</code> integer MTU is the maximum transmission unit for a network.MTU is optional, if not provided, the globally configured value in OVN-Kubernetes (defaults to 1400) is used for the network. Maximum: 65536 Minimum: 576  <code>subnets</code> Layer3Subnet array Subnets are used for the pod network across the cluster.Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.Given subnet is split into smaller subnets for every node. MaxItems: 2 MinItems: 1  <code>joinSubnets</code> DualStackCIDRs JoinSubnets are used inside the OVN network topology.Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.This field is only allowed for \"Primary\" network.It is not recommended to set this field without explicit need and understanding of the OVN network topology.When omitted, the platform will choose a reasonable default which is subject to change over time. MaxItems: 2 MaxLength: 43 MinItems: 1"},{"location":"api-reference/userdefinednetwork-api-spec/#layer3subnet","title":"Layer3Subnet","text":"<p>Appears in: - Layer3Config</p> Field Description Default Validation <code>cidr</code> CIDR CIDR specifies L3Subnet, which is split into smaller subnets for every node. MaxLength: 43  <code>hostSubnet</code> integer HostSubnet specifies the subnet size for every node.When not set, it will be assigned automatically. Maximum: 127 Minimum: 1"},{"location":"api-reference/userdefinednetwork-api-spec/#localnetconfig","title":"LocalnetConfig","text":"<p>Appears in: - NetworkSpec</p> Field Description Default Validation <code>role</code> NetworkRole role describes the network role in the pod, required.Controls whether the pod interface will act as primary or secondary.Localnet topology supports <code>Secondary</code> only.The network will be assigned to pods that have the <code>k8s.v1.cni.cncf.io/networks</code> annotation in place pointingto subject. Enum: [Secondary]  <code>physicalNetworkName</code> string physicalNetworkName points to the OVS bridge-mapping's network-name configured in the nodes, required.Min length is 1, max length is 253, cannot contain <code>,</code> or <code>:</code> characters.In case OVS bridge-mapping is defined by Kubernetes-nmstate with <code>NodeNetworkConfigurationPolicy</code> (NNCP),this field should point to the NNCP <code>spec.desiredState.ovn.bridge-mappings</code> item's <code>localnet</code> value. MaxLength: 253 MinLength: 1  <code>subnets</code> DualStackCIDRs subnets is a list of subnets used for pods in this localnet network across the cluster.The list may be either 1 IPv4 subnet, 1 IPv6 subnet, or 1 of each IP family.When set, OVN-Kubernetes assigns an IP address from the specified CIDRs to the connected pod,eliminating the need for manual IP assignment or reliance on an external IPAM service (e.g., a DHCP server).subnets is optional. When omitted OVN-Kubernetes won't assign IP address automatically.Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.The format should match standard CIDR notation (for example, \"10.128.0.0/16\").This field must be omitted if <code>ipam.mode</code> is <code>Disabled</code>.When physicalNetworkName points to the OVS bridge mapping of a network that provides IPAM services(e.g., a DHCP server), ipam.mode should be set to Disabled. This turns off OVN-Kubernetes IPAM and avoidsconflicts with the existing IPAM services on this localnet network. MaxItems: 2 MaxLength: 43 MinItems: 1  <code>excludeSubnets</code> CIDR array excludeSubnets is a list of CIDRs to be removed from the specified CIDRs in <code>subnets</code>.The CIDRs in this list must be in range of at least one subnet specified in <code>subnets</code>.excludeSubnets is optional. When omitted no IP address is excluded and all IP addresses specified in <code>subnets</code>are subject to assignment.The format should match standard CIDR notation (for example, \"10.128.0.0/16\").This field must be omitted if <code>subnets</code> is unset or <code>ipam.mode</code> is <code>Disabled</code>.When <code>physicalNetworkName</code> points to OVS bridge mapping of a network with reserved IP addresses(which shouldn't be assigned by OVN-Kubernetes), the specified CIDRs will not be assigned. For example:Given: <code>subnets: \"10.0.0.0/24\"</code>, <code>excludeSubnets: \"10.0.0.200/30\", the following addresses will not be assigned&lt;br /&gt;to pods:</code>10.0.0.201<code>,</code>10.0.0.202`. MaxItems: 25 MaxLength: 43 MinItems: 1  <code>ipam</code> IPAMConfig ipam configurations for the network.ipam is optional. When omitted, <code>subnets</code> must be specified.When <code>ipam.mode</code> is <code>Disabled</code>, <code>subnets</code> must be omitted.<code>ipam.mode</code> controls how much of the IP configuration will be managed by OVN.   When <code>Enabled</code>, OVN-Kubernetes will apply IP configuration to the SDN infra and assign IPs from the selected   subnet to the pods.   When <code>Disabled</code>, OVN-Kubernetes only assigns MAC addresses, and provides layer2 communication, and enables users   to configure IP addresses on the pods.<code>ipam.lifecycle</code> controls IP addresses management lifecycle.   When set to 'Persistent', the assigned IP addresses will be persisted in <code>ipamclaims.k8s.cni.cncf.io</code> object.   Useful for VMs, IP address will be persistent after restarts and migrations. Supported when <code>ipam.mode</code> is <code>Enabled</code>. MinProperties: 1  <code>mtu</code> integer mtu is the maximum transmission unit for a network.mtu is optional. When omitted, the configured value in OVN-Kubernetes (defaults to 1500 for localnet topology)is used for the network.Minimum value for IPv4 subnet is 576, and for IPv6 subnet is 1280.Maximum value is 65536.In a scenario <code>physicalNetworkName</code> points to OVS bridge mapping of a network configured with certain MTU settings,this field enables configuring the same MTU on pod interface, having the pod MTU aligned with the network MTU.Misaligned MTU across the stack (e.g.: pod has MTU X, node NIC has MTU Y), could result in network disruptionsand bad performance. Maximum: 65536 Minimum: 576  <code>vlan</code> VLANConfig vlan configuration for the network.vlan.mode is the VLAN mode.  When \"Access\" is set, OVN-Kubernetes configures the network logical switch port in access mode.vlan.access is the access VLAN configuration.vlan.access.id is the VLAN ID (VID) to be set on the network logical switch port.vlan is optional, when omitted the underlying network default VLAN will be used (usually <code>1</code>).When set, OVN-Kubernetes will apply VLAN configuration to the SDN infra and to the connected pods."},{"location":"api-reference/userdefinednetwork-api-spec/#networkipamlifecycle","title":"NetworkIPAMLifecycle","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [Persistent]</p> <p>Appears in: - IPAMConfig</p> Field Description <code>Persistent</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#networkrole","title":"NetworkRole","text":"<p>Underlying type: string</p> <p>Appears in: - Layer2Config - Layer3Config - LocalnetConfig</p> Field Description <code>Primary</code> <code>Secondary</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#networkspec","title":"NetworkSpec","text":"<p>NetworkSpec defines the desired state of UserDefinedNetworkSpec.</p> <p>Appears in: - ClusterUserDefinedNetworkSpec</p> Field Description Default Validation <code>topology</code> NetworkTopology Topology describes network configuration.Allowed values are \"Layer3\", \"Layer2\" and \"Localnet\".Layer3 topology creates a layer 2 segment per node, each with a different subnet. Layer 3 routing is used to interconnect node subnets.Layer2 topology creates one logical switch shared by all nodes.Localnet topology is based on layer 2 topology, but also allows connecting to an existent (configured) physical network to provide north-south traffic to the workloads. Enum: [Layer2 Layer3 Localnet] Required: {}  <code>layer3</code> Layer3Config Layer3 is the Layer3 topology configuration. <code>layer2</code> Layer2Config Layer2 is the Layer2 topology configuration. <code>localnet</code> LocalnetConfig Localnet is the Localnet topology configuration. <code>transport</code> TransportOption Transport describes the transport technology for pod-to-pod traffic.Allowed values are \"NoOverlay\", \"Geneve\", and \"EVPN\".- \"NoOverlay\": The network operates in no-overlay mode.- \"Geneve\": The network uses Geneve overlay.- \"EVPN\": The network uses EVPN transport.When omitted, the default behaviour is Geneve. Enum: [NoOverlay Geneve EVPN]  <code>noOverlay</code> NoOverlayConfig NoOverlay contains configuration for no-overlay mode.This is only allowed when Transport is \"NoOverlay\". <code>evpn</code> EVPNConfig EVPN contains configuration for EVPN mode.This is only allowed when Transport is \"EVPN\"."},{"location":"api-reference/userdefinednetwork-api-spec/#networktopology","title":"NetworkTopology","text":"<p>Underlying type: string</p> <p>Appears in: - NetworkSpec - UserDefinedNetworkSpec</p> Field Description <code>Localnet</code> <code>Layer2</code> <code>Layer3</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#nooverlayconfig","title":"NoOverlayConfig","text":"<p>NoOverlayConfig contains configuration options for networks operating in no-overlay mode.</p> <p>Appears in: - NetworkSpec</p> Field Description Default Validation <code>outboundSNAT</code> SNATOption OutboundSNAT defines the SNAT behavior for outbound traffic from pods. Enum: [Enabled Disabled]  <code>routing</code> RoutingOption Routing specifies whether the pod network routing is managed by OVN-Kubernetes or users. Enum: [Managed Unmanaged]"},{"location":"api-reference/userdefinednetwork-api-spec/#routetargetstring","title":"RouteTargetString","text":"<p>Underlying type: string</p> <p>RouteTargetString represents the 6-byte value of a BGP extended community route target (RFC 4360). BGP Extended Communities are 8 bytes total: 2-byte type field + 6-byte value field. This string encodes the 6-byte value, split between a global administrator (Autonomous System or IPv4) and a local administrator.</p> <p>When auto-generated, the local administrator is set to the VNI, creating a natural mapping between Route Targets and VXLAN network segments (e.g., \"65000:100\" for AS 65000 and VNI 100). When explicitly specified, the local administrator can be any value within the type constraints.</p> <p>FRR EVPN L3 Route-Targets use format (A.B.C.D:MN|EF:OPQR|GHJK:MN|:OPQR|:MN) where:   - EF:OPQR   = 2-byte AS (1-65535) : local administrator (4 bytes, 0-4294967295)   - GHJK:MN   = 4-byte AS (65536-4294967295) : local administrator (2 bytes, 0-65535)   - A.B.C.D:MN = IPv4 address (4 bytes) : local administrator (2 bytes, 0-65535)   - :OPQR    = wildcard AS : local administrator (4 bytes, 0-4294967295) - for import matching   - :MN      = wildcard AS : local administrator (2 bytes, 0-65535) - for import matching</p> <p>The 6-byte constraint means: if AS is 4 bytes, local admin can only be 2 bytes, and vice versa. Wildcard (*) matches any AS and is useful for import rules in Downstream VNI scenarios. Note: VNI is 24-bit (max 16777215), so auto-generation with 4-byte AS or IPv4 only works if VNI &lt;= 65535. See: https://docs.frrouting.org/en/stable-8.5/bgp.html#evpn-l3-route-targets</p> <p>Validation: - MaxLength: 21</p> <p>Appears in: - VRFConfig</p>"},{"location":"api-reference/userdefinednetwork-api-spec/#routingoption","title":"RoutingOption","text":"<p>Underlying type: string</p> <p>Appears in: - NoOverlayConfig</p> Field Description <code>Managed</code> <code>Unmanaged</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#snatoption","title":"SNATOption","text":"<p>Underlying type: string</p> <p>Appears in: - NoOverlayConfig</p> Field Description <code>Enabled</code> <code>Disabled</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#transportoption","title":"TransportOption","text":"<p>Underlying type: string</p> <p>Appears in: - NetworkSpec</p> Field Description <code>NoOverlay</code> <code>Geneve</code> <code>EVPN</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#userdefinednetwork","title":"UserDefinedNetwork","text":"<p>UserDefinedNetwork describe network request for a Namespace.</p> <p>Appears in: - UserDefinedNetworkList</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>UserDefinedNetwork</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> UserDefinedNetworkSpec Required: {}  <code>status</code> UserDefinedNetworkStatus"},{"location":"api-reference/userdefinednetwork-api-spec/#userdefinednetworklist","title":"UserDefinedNetworkList","text":"<p>UserDefinedNetworkList contains a list of UserDefinedNetwork.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>UserDefinedNetworkList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> UserDefinedNetwork array"},{"location":"api-reference/userdefinednetwork-api-spec/#userdefinednetworkspec","title":"UserDefinedNetworkSpec","text":"<p>UserDefinedNetworkSpec defines the desired state of UserDefinedNetworkSpec.</p> <p>Appears in: - UserDefinedNetwork</p> Field Description Default Validation <code>topology</code> NetworkTopology Topology describes network configuration.Allowed values are \"Layer3\", \"Layer2\".Layer3 topology creates a layer 2 segment per node, each with a different subnet. Layer 3 routing is used to interconnect node subnets.Layer2 topology creates one logical switch shared by all nodes. Enum: [Layer2 Layer3] Required: {}  <code>layer3</code> Layer3Config Layer3 is the Layer3 topology configuration. <code>layer2</code> Layer2Config Layer2 is the Layer2 topology configuration."},{"location":"api-reference/userdefinednetwork-api-spec/#userdefinednetworkstatus","title":"UserDefinedNetworkStatus","text":"<p>UserDefinedNetworkStatus contains the observed status of the UserDefinedNetwork.</p> <p>Appears in: - UserDefinedNetwork</p> Field Description Default Validation <code>conditions</code> Condition array"},{"location":"api-reference/userdefinednetwork-api-spec/#vlanconfig","title":"VLANConfig","text":"<p>VLANConfig describes the network VLAN configuration.</p> <p>Appears in: - LocalnetConfig</p> Field Description Default Validation <code>mode</code> VLANMode mode describe the network VLAN mode.Allowed value is \"Access\".Access sets the network logical switch port in access mode, according to the config. Enum: [Access]  <code>access</code> AccessVLANConfig Access is the access VLAN configuration"},{"location":"api-reference/userdefinednetwork-api-spec/#vlanmode","title":"VLANMode","text":"<p>Underlying type: string</p> <p>Validation: - Enum: [Access]</p> <p>Appears in: - VLANConfig</p> Field Description <code>Access</code>"},{"location":"api-reference/userdefinednetwork-api-spec/#vrfconfig","title":"VRFConfig","text":"<p>VRFConfig contains configuration for a VRF in EVPN.</p> <p>Appears in: - EVPNConfig</p> Field Description Default Validation <code>vni</code> integer VNI is the Virtual Network Identifier for this VRF.VNI is a 24-bit field in the VXLAN header (RFC 7348), allowing values from 1 to 16777215.but in the future this could be having different limit for other dataplane implementations.Must be unique across all EVPN configurations in the cluster. Maximum: 1.6777215e+07 Minimum: 1 Required: {}  <code>routeTarget</code> RouteTargetString RouteTarget is the import/export route target for this VRF.If not specified, it will be auto-generated as \":\".Auto-generation will use 2-byte AS if VNI &gt; 65535, since 4-byte AS/IPv4 only allows 2-byte local admin.Follows FRR EVPN L3 Route-Target format (A.B.C.D:MN|EF:OPQR|GHJK:MN|:OPQR|:MN):  - EF:OPQR   = 2-byte AS (1-65535) : local admin (4 bytes, 1-4294967295)  - GHJK:MN   = 4-byte AS (65536-4294967295) : local admin (2 bytes, 1-65535)  - A.B.C.D:MN = IPv4 address : local admin (2 bytes, 1-65535)  - :OPQR    = wildcard AS : local admin (4 bytes, 1-4294967295) - for import matching  - :MN      = wildcard AS : local admin (2 bytes, 1-65535) - for import matchingThe 6-byte value constraint (RFC 4360) means AS size + local admin size = 6 bytes. MaxLength: 21"},{"location":"api-reference/vtep-api-spec/","title":"API Reference","text":""},{"location":"api-reference/vtep-api-spec/#packages","title":"Packages","text":"<ul> <li>k8s.ovn.org/v1</li> </ul>"},{"location":"api-reference/vtep-api-spec/#k8sovnorgv1","title":"k8s.ovn.org/v1","text":"<p>Package v1 contains API Schema definitions for the network v1 API group</p>"},{"location":"api-reference/vtep-api-spec/#resource-types","title":"Resource Types","text":"<ul> <li>VTEP</li> <li>VTEPList</li> </ul>"},{"location":"api-reference/vtep-api-spec/#cidr","title":"CIDR","text":"<p>Underlying type: string</p> <p>CIDR represents a CIDR notation IP range.</p> <p>Validation: - MaxLength: 43</p> <p>Appears in: - DualStackCIDRs</p>"},{"location":"api-reference/vtep-api-spec/#dualstackcidrs","title":"DualStackCIDRs","text":"<p>Underlying type: CIDR</p> <p>DualStackCIDRs is a list of CIDRs that supports dual-stack (IPv4 and IPv6).</p> <p>Validation: - MaxItems: 2 - MaxLength: 43 - MinItems: 1</p> <p>Appears in: - VTEPSpec</p>"},{"location":"api-reference/vtep-api-spec/#vtep","title":"VTEP","text":"<p>VTEP defines VTEP (VXLAN Tunnel Endpoint) IP configuration for EVPN.</p> <p>Appears in: - VTEPList</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>VTEP</code> <code>metadata</code> ObjectMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>spec</code> VTEPSpec Spec defines the desired VTEP configuration. Required: {}  <code>status</code> VTEPStatus Status contains the observed state of the VTEP."},{"location":"api-reference/vtep-api-spec/#vteplist","title":"VTEPList","text":"<p>VTEPList contains a list of VTEP.</p> Field Description Default Validation <code>apiVersion</code> string <code>k8s.ovn.org/v1</code> <code>kind</code> string <code>VTEPList</code> <code>metadata</code> ListMeta Refer to Kubernetes API documentation for fields of <code>metadata</code>. <code>items</code> VTEP array"},{"location":"api-reference/vtep-api-spec/#vtepmode","title":"VTEPMode","text":"<p>Underlying type: string</p> <p>VTEPMode defines the mode of VTEP IP allocation.</p> <p>Validation: - Enum: [Managed Unmanaged]</p> <p>Appears in: - VTEPSpec</p> Field Description <code>Managed</code> VTEPModeManaged means OVN-Kubernetes allocates and assigns VTEP IPs per node automatically. <code>Unmanaged</code> VTEPModeUnmanaged means an external provider handles IP assignment;OVN-Kubernetes discovers existing IPs on nodes."},{"location":"api-reference/vtep-api-spec/#vtepspec","title":"VTEPSpec","text":"<p>VTEPSpec defines the desired state of VTEP.</p> <p>Appears in: - VTEP</p> Field Description Default Validation <code>cidrs</code> DualStackCIDRs CIDRs is the list of IP ranges from which VTEP IPs are allocated.Dual-stack clusters may set 2 CIDRs (one for each IP family), otherwise only 1 CIDR is allowed.The format should match standard CIDR notation (for example, \"100.64.0.0/24\" or \"fd00::/64\"). MaxItems: 2 MaxLength: 43 MinItems: 1 Required: {}  <code>mode</code> VTEPMode Mode specifies how VTEP IPs are managed.\"Managed\" means OVN-Kubernetes allocates and assigns VTEP IPs per node automatically.\"Unmanaged\" means an external provider handles IP assignment; OVN-Kubernetes discovers existing IPs on nodes.Defaults to \"Managed\". Managed Enum: [Managed Unmanaged]"},{"location":"api-reference/vtep-api-spec/#vtepstatus","title":"VTEPStatus","text":"<p>VTEPStatus contains the observed state of the VTEP.</p> <p>Appears in: - VTEP</p> Field Description Default Validation <code>conditions</code> Condition array Conditions slice of condition objects indicating details about VTEP status."},{"location":"blog/","title":"Blog","text":""},{"location":"ci/ci/","title":"CI Tests","text":"<p>For CI, OVN-Kubernetes runs the Kubernetes E2E tests and some locally defined tests.  GitHub Actions are used to run a subset of the Kubernetes E2E tests on each pull request. The local workflow that controls the test run is located in ovn-kubernetes/.github/workflows/test.yml.</p> <p>The following tasks are performed:</p> <ul> <li>Build OVN-Kubernetes</li> <li>Check out the Kubernetes source tree and compiles some dependencies</li> <li>Install KIND</li> <li>Run a matrix of End-To-End Tests using KIND</li> <li>Ensure that documentation builds successfully</li> </ul> <p>The full matrix of e2e tests found here are also run periodically (twice daily) using an OVN-Kubernetes build based on the currently merged code base.</p> <p>The following sections should help you understand (and if needed modify) the set of tests that run and how to run these tests locally.</p>"},{"location":"ci/ci/#ci-fails-what-do-i-do","title":"CI fails: what do I do?","text":"<p>Some tests are known to be flaky, see <code>kind/ci-flake</code> issues. At the end of your failed test run, you will see something like:</p> <p><pre><code>Summarizing 1 Failure:\n  [FAIL] e2e egress firewall policy validation with external containers [It] Should validate the egress firewall policy functionality for allowed IP\n  /home/runner/work/ovn-kubernetes/ovn-kubernetes/test/e2e/egress_firewall.go:130\n</code></pre> then search for \"e2e egress firewall policy validation\" in the open issues. </p> <p>If you find an issue that matches your failure, update the issue with your job link.  If the issue doesn't exist, it either means the failure is introduced in your PR or it is a new flake.  Try to run the same test locally multiple times, and if doesn't fail, report a new flake. Reporting a new flake is fairly straightforward, but you can use already open issues as an example. It may also be useful sometimes to search through the closed issues to see if the flake was reported previously and (not really) fixed, then reopening it with the new job failure.</p> <p>Only after following these steps ^, you can comment <code>/retest-failed</code> on your PR to trigger a retest of the failed tests. A rocket emoji reaction on your comment should apper when the retest is triggered.</p> <p>Before running this command, please reference existing or newly opened issues to justify the retest request.</p>"},{"location":"ci/ci/#understanding-the-ci-test-suite","title":"Understanding the CI Test Suite","text":"<p>The tests are broken into 2 categories, <code>shard</code> tests which execute tests from the Kubernetes E2E test suite and the <code>control-plane</code> tests which run locally defined tests.</p>"},{"location":"ci/ci/#shard-tests","title":"Shard tests","text":"<p>The shard tests are broken into a set of shards, which is just a grouping of tests, and each shard is run in a separate job in parallel. Shards execute the <code>shard-%</code> target in  ovn-kubernetes/test/Makefile. The set of shards may change in the future. Below is an example of the shards at time of this writing:</p> <ul> <li>shard-network</li> <li>All E2E tests that match <code>[sig-network]</code></li> <li>shard-conformance</li> <li>All E2E tests that match <code>[Conformance]|[sig-network]</code></li> <li>shard-test</li> <li>Single E2E test that matches the name of the test specified with a regex. </li> <li>When selecting the <code>shard-test</code> target, you focus on a specific test by appending <code>WHAT=&lt;test name&gt;</code> to the make command.</li> <li>See bottom of this document for an example.</li> </ul> <p>Shards use the E2E framework. By selecting a specific shard, you modify ginkgo's <code>--focus</code> parameter.</p> <p>The regex expression for determining which E2E test is run in which shard, as well as the list of skipped tests is defined in ovn-kubernetes/test/scripts/e2e-kind.sh.</p>"},{"location":"ci/ci/#control-plane-tests","title":"Control-plane tests","text":"<p>In addition to the <code>shard-%</code> tests, there is also a <code>control-plane</code> target in  ovn-kubernetes/test/Makefile. Below is a description of this target:</p> <ul> <li>control-plane</li> <li>All locally defined tests by default.</li> <li>You can focus on a specific test by appending <code>WHAT=&lt;test name&gt;</code> to the make command.</li> <li>See bottom of this document for an example.</li> </ul> <p>All local tests are run by <code>make control-plane</code>. The local tests are controlled in ovn-kubernetes/test/scripts/e2e-cp.sh and the actual tests are defined in the directory ovn-kubernetes/test/e2e/.</p>"},{"location":"ci/ci/#node-ip-migration-tests","title":"Node IP migration tests","text":"<p>The node IP migration tests are part of the control-plane tests but due to their impact they cannot be run concurrently with other tests and they are disabled when running <code>make control-plane</code>. Instead, they must explicitly be requested with <code>make -C test control-plane WHAT=\"Node IP address migration\"</code>.</p>"},{"location":"ci/ci/#github-ci-integration-through-github-actions-matrix","title":"Github CI integration through Github Actions Matrix","text":"<p>Each of these shards and control-plane tests can then be run in a Github Actions matrix of: * HA setup (3 masters and 0 workers) and a non-HA setup (1 master and 2 workers) * Local Gateway Mode and Shared Gateway Mode. See: Enable Node-Local Services Access in Shared Gateway Mode * IPv4 Only, IPv6 Only and Dualstack * Disabled SNAT Multiple Gateways or Enabled SNAT Gateways * Single bridge or two bridges</p> <p>To reduce the explosion of tests being run in CI, the test cases run are limited using an <code>exclude:</code> statement in  ovn-kubernetes/.github/workflows/test.yml.</p>"},{"location":"ci/ci/#conformance-tests","title":"Conformance Tests","text":"<p>We have a conformance test suit that can be invoked using the <code>make conformance</code> command. Currently we run the <code>TestNetworkPolicyV2Conformance</code> tests there. The actual tests are defined in https://github.com/kubernetes-sigs/network-policy-api/tree/master/conformance and then invoked from this repo. Any changes to the tests first have to be submitted upstream to <code>network-policy-api</code> repo and then brought downstream into the ovn-kubernetes repo through version bump.</p>"},{"location":"ci/ci/#documentation-build-check","title":"Documentation Build Check","text":"<p>To catch any potential documentation build breakages which would prevent any docs changes from being deployed to our GitHub Pages site. The build check will produce the html docs and will be available in the job artifacts for review. There is a link printed in the job run logs inside the step \"Upload Artifact\". Download and unzip that locally  to view the resulting docs after they are built to see what would be deployed to github pages.</p>"},{"location":"design/acls/","title":"ACLs","text":""},{"location":"design/acls/#introduction","title":"Introduction","text":"<p>ovn-k uses ACLs to implement multiple features, this doc is intended to list all of them, explaining their IDs,  priorities, and dependencies.</p> <p>OVN has 3 independent sets of ACLs, based on direction and pipeline stage, which are applied in the following order:  1. <code>direction=\"from-lport\"</code>  2. <code>direction=\"from-lport\", options={\"apply-after-lb\": \"true\"}</code> 3. <code>direction=\"to-lport\"</code></p> <p>The priorities between these stages are independent!</p> <p>OVN will apply the <code>from-lport</code> ACLs in two stages. ACLs without <code>apply-after-lb</code> set, will be applied before the  load balancer stage, and ACLs with this option set will be applied after the load balancer stage. <code>to-lport</code> are always  applied after the load balancer stage.</p> <p>For now, ovn-k doesn't use <code>direction=\"from-lport\"</code> ACLs, since most of the time we need to apply ACLs after loadbalancing. Here is the current order in which ACLs for different objects are applied (check the rest of the doc for details)</p>"},{"location":"design/acls/#directionfrom-lport-optionsapply-after-lb-true","title":"<code>direction=\"from-lport\", options={\"apply-after-lb\": \"true\"}</code>","text":"<ol> <li>egress multicast, allow priority = <code>1012</code>,  deny priority = <code>1011</code></li> <li>egress network policy, default deny priority = <code>1000</code>, allow priority = <code>1001</code></li> </ol>"},{"location":"design/acls/#directionto-lport","title":"<code>direction=\"to-lport\"</code>","text":"<ol> <li>egress firewall, priorities = <code>2000</code>-<code>10000</code> (egress firewall is applied on this stage to be independently applied after egress network policy)</li> <li>ingress multicast, allow priority = <code>1012</code>,  deny priority = <code>1011</code></li> <li>ingress network policy, default deny priority = <code>1000</code>, allow priority = <code>1001</code></li> </ol>"},{"location":"design/acls/#egress-firewall","title":"Egress Firewall","text":"<p>Egress Firewall creates 1 ACL for every specified rule, with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=EgressFirewall</code> e.g. given object:</p> <pre><code>kind: EgressFirewall\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n    - type: Allow\n      to:\n        dnsName: www.openvswitch.org\n    - type: Allow\n      to:\n        cidrSelector: 1.2.3.0/24\n      ports:\n        - protocol: UDP\n          port: 55\n    - type: Deny\n      to:\n        cidrSelector: 0.0.0.0/0\n</code></pre> <p>will create 3 ACLs:</p> <pre><code>action              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:10000\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"0\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == $a5272457940446250407) &amp;&amp; ip4.src == $a4322231855293774466\"\nmeter               : acl-logging\nname                : \"EF:default:10000\"\noptions             : {}\npriority            : 10000\nseverity            : []\n\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:9999\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == 1.2.3.0/24) &amp;&amp; ip4.src == $a4322231855293774466 &amp;&amp; ((udp &amp;&amp; ( udp.dst == 55 )))\"\nmeter               : acl-logging\nname                : \"EF:default:9999\"\noptions             : {}\npriority            : 9999\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:EgressFirewall:default:9998\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=EgressFirewall, \n    rule-idx=\"2\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.dst == 0.0.0.0/0 &amp;&amp; ip4.dst != 10.244.0.0/16) &amp;&amp; ip4.src == $a4322231855293774466\"\nmeter               : acl-logging\nname                : \"EF:default:9998\"\noptions             : {}\npriority            : 9998\nseverity            : []\n</code></pre> <p>Egress firewall should be applied after egress network policy independently, to make sure that connection that are allowed by network policy, but denied by egress firewall will be dropped.</p>"},{"location":"design/acls/#multicast","title":"Multicast","text":"<p>For more details about Multicast in ovn see Multicast docs Multicast creates 2 types of ACLs: global default ACLs with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=MulticastCluster</code> and per-namespace ACLs with <code>ExternalIDs[\"owner_object_type\"]=\"MulticastNS\"</code>. </p> <p>When multicast is enabled for ovn-k, you will find 4 default global ACLs: - 2 ACLs (ingress/egress) dropping all multicast traffic - on all switches (via clusterPortGroup) - 2 ACLs (ingress/egress) allowing all multicast traffic - on clusterRouterPortGroup   (that allows multicast between pods that reside on different nodes, see   https://github.com/ovn-org/ovn-kubernetes/commit/3864f2b6463392ae2d80c18d06bd46ec44e639f9 for more details)</p> <pre><code>action              : allow\ndirection           : from-lport\nexternal_ids        : {\n  direction=Egress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:AllowInterNode:Ingress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=AllowInterNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1012\nseverity            : []\n\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n  direction=Ingress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:AllowInterNode:Ingress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=AllowInterNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1012\nseverity            : []\n\naction              : drop\ndirection           : from-lport\nexternal_ids        : {\n  direction=Egress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:DefaultDeny:Egress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=DefaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1011\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n  direction=Ingress, \n  \"k8s.ovn.org/id\"=\"default-network-controller:MulticastCluster:DefaultDeny:Egress\", \n  \"k8s.ovn.org/owner-controller\"=default-network-controller, \n  \"k8s.ovn.org/owner-type\"=MulticastCluster, \n  type=DefaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1011\nseverity            : []\n</code></pre> <p>For every namespace with enabled multicast, there are 2 more ACLs, e.g. for default namespace: <pre><code>action              : allow\ndirection           : to-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:MulticastNS:default:Allow_Ingress\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=MulticastNS, \n    direction=Ingress\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; (igmp || (ip4.src == $a4322231855293774466 &amp;&amp; ip4.mcast))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1012\nseverity            : []\n\naction              : allow\ndirection           : from-lport\nexternal_ids        : {\n    \"k8s.ovn.org/id\"=\"default-network-controller:MulticastNS:default:Allow_Egress\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=MulticastNS, \n    tdirection=Egress\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; ip4.mcast\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1012\nseverity            : []\n</code></pre></p>"},{"location":"design/acls/#network-policy","title":"Network Policy","text":"<p>Every node has 1 ACL to allow traffic from that node's management port IP with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolNode</code>, like</p> <pre><code>action              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    ip=\"10.244.2.2\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNode:ovn-worker:10.244.2.2\", \n    \"k8s.ovn.org/name\"=ovn-worker, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNode\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src==10.244.2.2\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nseverity            : []\n</code></pre> <p>There are also 2 Default Allow ACLs for the hairpinned traffic (pod-&gt;svc-&gt;same pod) with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolDefault</code>, which match on the special V4OVNServiceHairpinMasqueradeIP and V6OVNServiceHairpinMasqueradeIP addresses. These IPs are assigned as src IP to the hairpinned packets.</p> <pre><code>action              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolDefault:allow-hairpinning:Ingress\", \n    \"k8s.ovn.org/name\"=allow-hairpinning, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolDefault\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src == 169.254.169.5\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nseverity            : []\n\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolDefault:allow-hairpinning:Egress\", \n    \"k8s.ovn.org/name\"=allow-hairpinning, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolDefault\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src == 169.254.169.5\"\nmeter               : acl-logging\nname                : []\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n</code></pre> <p>Network Policy creates default deny ACLs for every namespace that has at least 1 network policy with  <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetpolNamespace</code>. There are 2 ACL types (<code>defaultDeny</code> and <code>arpAllow</code>) for every policy direction (Ingress/Egress) e.g. for <code>default</code> namespace,</p> <p>Egress: <pre><code>action              : allow\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Egress:arpAllow\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=arpAllow\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782_egressDefaultDeny &amp;&amp; (arp || nd)\"\nmeter               : acl-logging\nname                : \"NP:default:Egress\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n\naction              : drop\ndirection           : from-lport\nexternal_ids        : {\n    direction=Egress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Egress:defaultDeny\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=defaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a16982411286042166782_egressDefaultDeny\"\nmeter               : acl-logging\nname                : \"NP:default:Egress\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1000\nseverity            : []\n</code></pre></p> <p>Ingress: <pre><code>_uuid               : a6ffa9d4-e811-4aaf-9505-87cc0b2f442a\naction              : allow\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Ingress:arpAllow\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=arpAllow\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782_ingressDefaultDeny &amp;&amp; (arp || nd)\"\nmeter               : acl-logging\nname                : \"NP:default:Ingress\"\noptions             : {}\npriority            : 1001\nseverity            : []\n\naction              : drop\ndirection           : to-lport\nexternal_ids        : {\n    direction=Ingress, \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetpolNamespace:default:Ingress:defaultDeny\", \n    \"k8s.ovn.org/name\"=default, \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetpolNamespace, \n    type=defaultDeny\n}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a16982411286042166782_ingressDefaultDeny\"\nmeter               : acl-logging\nname                : \"NP:default:Ingress\"\noptions             : {}\npriority            : 1000\nseverity            : []\n</code></pre></p> <p>There are also ACLs owned by every network policy object with <code>ExternalIDs[\"k8s.ovn.org/owner-type\"]=NetworkPolicy</code>, e.g. for the following object</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n name: test-policy\n namespace: default\nspec:\n podSelector: {}\n policyTypes:\n - Ingress\n - Egress\n ingress:\n - from:\n   - namespaceSelector:\n       matchLabels:\n         kubernetes.io/metadata.name: default\n     podSelector:\n       matchLabels:\n         app: demo\n egress:\n  - to:\n    - ipBlock:\n        cidr: 10.244.1.5/32\n</code></pre> <p>2 ACLs will be created:</p> <pre><code>action              : allow-related\ndirection           : from-lport\nexternal_ids        : {\n    direction=egress, \n    gress-index=\"0\", \n    ip-block-index=\"0\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetworkPolicy:default:test-policy:egress:0:-1:0\", \n    \"k8s.ovn.org/name\"=\"default:test-policy\", \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetworkPolicy, \n    port-policy-index=\"-1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.dst == 10.244.1.5/32 &amp;&amp; inport == @a2653181086423119552\"\nmeter               : acl-logging\nname                : \"NP:default:test-policy:egress:0:-1:0\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1001\nseverity            : []\n\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {\n    direction=ingress, \n    gress-index=\"0\", \n    ip-block-index=\"-1\", \n    \"k8s.ovn.org/id\"=\"default-network-controller:NetworkPolicy:default:test-policy:ingress:0:-1:-1\", \n    \"k8s.ovn.org/name\"=\"default:test-policy\", \n    \"k8s.ovn.org/owner-controller\"=default-network-controller, \n    \"k8s.ovn.org/owner-type\"=NetworkPolicy, \n    port-policy-index=\"-1\"\n}\nlabel               : 0\nlog                 : false\nmatch               : \"(ip4.src == {$a3733136965153973077} || (ip4.src == 169.254.169.5 &amp;&amp; ip4.dst == {$a3733136965153973077})) &amp;&amp; outport == @a2653181086423119552\"\nmeter               : acl-logging\nname                : \"NP:default:test-policy:ingress:0:-1:-1\"\noptions             : {}\npriority            : 1001\nseverity            : []\n</code></pre> <p><code>\"k8s.ovn.org/name\"</code> is the <code>&lt;namespace&gt;:&lt;name&gt;</code> of network policy object, <code>gress-index</code> is the index of gress policy in the <code>NetworkPolicy.Spec.[In/E]gress</code>, check <code>gress_policy.go:getNetpolACLDbIDs</code> for more details on the rest of the fields.</p>"},{"location":"design/architecture/","title":"OVN-Kubernetes Architecture","text":"<p>There are two deployment modes for ovn-kubernetes depending on which the architecture is drastically different:</p> <ul> <li>central mode (centralized control plane architecture) -- is DEPRECATED starting 1.2 release</li> <li>interconnect mode (distributed control plane architecture) -- is the default mode</li> </ul> <p>End users are recommended to pick interconnect mode to deploy new clusters since central mode will be removed in future releases. Many of the new features already don't have support on central mode and are only available in interconnect mode.</p> <p>Let's look at these two modes in depth from architecture standpoint:</p>"},{"location":"design/architecture/#ovn-kubernetes-components-central-mode-deprecated","title":"OVN-Kubernetes Components - Central Mode (DEPRECATED!!)","text":"<p>The control plane has the <code>ovnkube-master</code> pod in the <code>ovn-kubernetes</code> namespace which are running only on the control plane nodes in your cluster:</p> <ul> <li>ovnkube-master pod<ul> <li>ovnkube-master container:<ul> <li>OVN-Kubernetes component</li> <li>Watches K8s API for objects - namespaces, pods, services, endpoints, network policies, CRs</li> <li>Translates K8s objects into OVN logical entities</li> <li>Stores OVN entities in NorthBound Database (NBDB)</li> <li>Manages pod subnet allocation to nodes (pod IPAM)</li> </ul> </li> <li>nbdb container:<ul> <li>Native OVN component</li> <li>Runs the OVN NBDB database</li> <li>Stores the logical elements created by ovnkube-master</li> <li>3 replicas across control plane nodes running using RAFT leadership algorithm in HA mode</li> </ul> </li> <li>northd container:<ul> <li>Native OVN component</li> <li>Converts the OVN logical elements from NBDB to OVN logical flows in SBDB</li> </ul> </li> <li>sbdb container:<ul> <li>Native OVN component</li> <li>Stores the logical flows created by northd</li> <li>3 replicas across control plane nodes running using RAFT leadership algorithm in HA mode</li> </ul> </li> </ul> </li> </ul> <p>The data plane includes the <code>ovnkube-node</code> and <code>ovs-node</code> pods in the <code>ovn-kubernetes</code> namespace which are running on all your nodes in the cluster.</p> <ul> <li>ovnkube-node pod<ul> <li>ovnkube-node container:<ul> <li>OVN-Kubernetes component</li> <li>Runs the CNI executable (CNI ADD/DEL)</li> <li>Digests the IPAM annotation set on pod by ovnkube-master</li> <li>Creates the veth pair for the pod</li> <li>Creates the ovs port on bridge</li> <li>Programs the necessary iptables and gateway service flows on a per-node basis.</li> </ul> </li> <li>ovn-controller container:<ul> <li>Native OVN component</li> <li>Connects to SBDB running in control plane using TLS</li> <li>Converts SBDB logical flows into openflows</li> <li>Write them to OVS</li> </ul> </li> </ul> </li> <li>ovs-node pod<ul> <li>ovs-daemons container:<ul> <li>OVS Native component</li> <li>OVS daemon and database running as a container</li> <li>virtual switch that pushes the network plumbing to the edge on the node</li> </ul> </li> </ul> </li> </ul>"},{"location":"design/architecture/#central-mode-architecture-deprecated","title":"Central Mode Architecture (DEPRECATED!!)","text":"<p>Now that we know the pods and components running in the central mode, let's tie up loose ends and show how these components run on a standard HA Kubernetes cluster.</p>"},{"location":"design/architecture/#control-plane-nodes","title":"Control Plane Nodes:","text":""},{"location":"design/architecture/#worker-nodes","title":"Worker Nodes:","text":""},{"location":"design/architecture/#ovn-kubernetes-components-interconnect-mode-default","title":"OVN-Kubernetes Components - Interconnect mode (DEFAULT)","text":"<p>The control plane has the <code>ovnkube-control-plane</code> pod in the <code>ovn-kubernetes</code> namespace which is super lightweight and running only on the control plane nodes in your cluster:</p> <ul> <li>ovnkube-control-plane pod<ul> <li>ovnkube-cluster-manager container:<ul> <li>OVN-Kubernetes component</li> <li>Watches K8s API for objects - nodes mainly</li> <li>Allocates pod subnet to each node</li> <li>Allocates join subnet IP to each node</li> <li>Allocates transit subnet IP to each node</li> <li>Consolidates zone statuses across all nodes for features like EgressFirewall and EgressQoS</li> </ul> </li> </ul> </li> </ul> <p>The data plane includes the <code>ovnkube-node</code> and <code>ovs-node</code> pods in the <code>ovn-kubernetes</code> namespace running on all your nodes in the cluster making this architecture localized and more distributed.</p> <ul> <li>ovnkube-node pod<ul> <li>ovnkube-controller container:<ul> <li>OVN-Kubernetes component</li> <li>Allocates podIP from the podSubnet to each pod in its zone (IPAM)</li> <li>Watches K8s API for objects - nodes, namespaces, pods, services, endpoints, network policies, CRs</li> <li>Translates K8s objects into OVN logical entities - stores them in OVN databases</li> <li>Stores OVN entities in NorthBound Database (NBDB)</li> <li>Manages pod subnet allocation to nodes (pod IPAM)</li> <li>Runs the CNI executable (CNI ADD/DEL)</li> <li>Digests the IPAM annotation set on pod</li> <li>Creates the veth pair for the pod</li> <li>Creates the ovs port on bridge</li> <li>Programs the necessary iptables and gateway service flows on a </li> </ul> </li> <li>nbdb container:<ul> <li>Native OVN component</li> <li>Runs the OVN NBDB database</li> <li>Stores the logical elements created by ovnkube-controller</li> <li>runs only 1 replica, contains information local to this node</li> </ul> </li> <li>northd container:<ul> <li>Native OVN component</li> <li>Converts the OVN logical elements from NBDB to OVN logical flows in SBDB</li> </ul> </li> <li>sbdb container:<ul> <li>Native OVN component</li> <li>Stores the logical flows created by northd</li> <li>runs only 1 replica, contains information local to this node</li> </ul> </li> <li>ovn-controller container:<ul> <li>Native OVN component</li> <li>Connects to SBDB running in control plane using TLS</li> <li>Converts SBDB logical flows into openflows</li> <li>Write them to OVS</li> </ul> </li> </ul> </li> <li>ovs-node pod<ul> <li>ovs-daemons container:<ul> <li>OVS Native component</li> <li>OVS daemon and database running as a container</li> <li>virtual switch that pushes the network plumbing to the edge on the node</li> </ul> </li> </ul> </li> </ul> <p>As we can see, the databases, northd and ovn-kubernetes controller components now run per zone rather than only on the control-plane.</p>"},{"location":"design/architecture/#interconnect-mode-architecture-default","title":"Interconnect Mode Architecture (DEFAULT)","text":""},{"location":"design/architecture/#what-is-interconnect","title":"What is Interconnect?","text":"<p>OVN Interconnection is a feature that allows connecting multiple OVN deployments with OVN managed GENEVE tunnels. Native ovn-ic feature allows for an <code>ovn-ic</code>, OVN interconnection controller, that is a centralized daemon which communicates with global interaction databases (IC_NB/IC_SB) to configure and exchange data with local NB/SB databases for interconnecting with other OVN deployments. See this for more details.</p>"},{"location":"design/architecture/#adopting-ovn-interconnect-into-ovn-kubernetes","title":"Adopting OVN-Interconnect into OVN-Kubernetes","text":"<p>In order to effectively adapt the capabilities of the interconnect feature in the kubernetes world, ovn-kubernetes components will replace <code>ovn-ic</code> daemon. Also note that the term <code>zone</code> which will be used heavily in these docs just refers to a single OVN deployment. Now that we know the pods and components running in the interconnect mode, let's tie up loose ends and show how these components run on a standard HA Kubernetes cluster. By default, each node in the cluster is a <code>zone</code>, so each <code>zone</code> contains 1 node. There is no more RAFT since each node has its own database.</p>"},{"location":"design/architecture/#control-plane-nodes_1","title":"Control Plane Nodes:","text":""},{"location":"design/architecture/#worker-nodes_1","title":"Worker Nodes:","text":""},{"location":"design/architecture/#why-do-we-need-interconnect-mode-in-ovn-kubernetes","title":"Why do we need Interconnect mode in OVN-Kubernetes?","text":"<p>This architecture brings about several improvements:</p> <ul> <li>Stability: The OVN Northbound and Southbound databases are local to each node. Since they are running in the standalone mode, that eliminates the need for RAFT, thus avoiding all the \u201csplit-brain\u201d issues. If one of the databases goes down, the impact is now isolated to only that node. This has led to improved stability of the OVN-Kubernetes stack and simpler customer escalation resolution.</li> <li>Scale: As seen in the above diagram, the ovn-controller container connects to the local Southbound database for logical flow information. On large clusters with N nodes, this means each Southbound database is handling only one connection from its own local ovn-controller. This has removed the scale bottlenecks that were present in the centralized model helping us to scale horizontally with node count.</li> <li>Performance: The OVN-Kubernetes brain is now local to each node in the cluster, and it is storing and processing changes to only those Kubernetes pods, services, endpoints objects that are relevant for that node (note: some features like NetworkPolicies need to process pods running on other nodes). This in turn means the OVN stack is also processing less data thus leading to improved operational latency. Another benefit is that the control plane stack is now lighter-weight.</li> <li>Security: Since the infrastructure network traffic between ovn-controller and OVN Southbound database is now contained within each node, overall cross-node and cross-cluster (HostedControlPlane, ManagedSaaS) chatter is decreased and traffic security can be increased.</li> </ul>"},{"location":"design/architecture/#central-mode-versus-interconnect-mode","title":"Central Mode versus Interconnect Mode","text":"<ul> <li>When you want your databases to stay centralized and do not require linear scaling with node count, choose Central Mode</li> <li>Note that there is no difference to OVS between the two deployment modes.</li> <li>FIXME: This section needs to be written well</li> </ul>"},{"location":"design/external-ip-and-loadbalancer-ingress/","title":"External IP and LoadBalancer Ingress","text":"<p>OVN-Kubernetes implements both External IPs and LoadBalancer Ingress IPs (<code>service.Status.LoadBalancer.Ingress</code>) in the form of OVN load balancers. These OVN load balancers live on all of the Kubernetes nodes and are thus highly available and ready for load sharing. It is the administrator's responsibility to route traffic to the Kubernetes nodes for both of these VIP types. </p> <p>In an environment where External IPs and LoadBalancer Ingress VIPs happen to be part of the nodes' subnets, administrators might expect that OVN-Kubernetes answer to ARP requests to these VIPs. However, this is not the case. The administrator is responsible for routing packets to the Kubernetes nodes and they cannot rely on the network plugin for this to happen.</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#external-ip","title":"External IP","text":"<p>The Kubernetes documentation states that External IPs are to be handled by the cluster administrator and are not the responsibility of the network plugin. See the following quote from the kubernetes documentation: <pre><code>External IPs\n\nIf there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those externalIPs. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port, will be routed to one of the Service endpoints. externalIPs are not managed by Kubernetes and are the responsibility of the cluster administrator.\n</code></pre></p> <p>Source: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#loadbalancer-ingress-servicestatusloadbalanceringress","title":"LoadBalancer Ingress (service.Status.LoadBalancer.Ingress)","text":"<p>For a service's Status LoadBalancer Ingress field <code>service.Status.LoadBalancer.Ingress</code>, the aforementioned statement applies in exactly the same manner. Both External IP and <code>service.Status.LoadBalancer.Ingress</code> should behave the same from the network plugin's behavior, and it is the administrator's responsibility to get traffic for the VIPs into the cluster. </p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#implementation-details","title":"Implementation details","text":"<p>OVN-Kubernetes exposes External IPs and <code>service.Status.LoadBalancer.Ingress</code> VIPs as OVN load balancers on every node in the cluster. However, OVN-Kubernetes will not answer to ARP requests to these VIP types, even if they reside on a node local subnet. This is because otherwise, every node in the cluster would answer with its own ARP reply to the same ARP request, leading to potential issues with stateful network flows that are tracked by conntrack. See the discussion in https://github.com/ovn-org/ovn-kubernetes/issues/2407 for further details.</p> <p>In fact, OVN-Kubernetes implements explicit bypass rules for ARP requests to these VIP types on the external bridge (<code>br-ex</code> or <code>breth0</code> in most deployments). Any ARP request to such an IP that comes in from the physical port will bypass the OVN dataplane and it will be sent to host's networking stack on purpose. If an ARP reponse to a VIP is expeced, make sure the VIP is added to the host's networking stack.</p> <p>For implementation details, see:  * https://github.com/ovn-org/ovn-kubernetes/blob/00925a6c64f57f03b2918eb48ff589c3417ddaa9/go-controller/pkg/node/gateway_shared_intf.go#L336 * https://github.com/ovn-org/ovn-kubernetes/blob/00925a6c64f57f03b2918eb48ff589c3417ddaa9/go-controller/pkg/node/gateway_shared_intf.go#L344 * https://github.com/ovn-org/ovn-kubernetes/pull/2540 * https://github.com/ovn-org/ovn-kubernetes/pull/2394</p>"},{"location":"design/external-ip-and-loadbalancer-ingress/#guidance-for-administrators","title":"Guidance for administrators","text":"<p>This absence of ARP replies from OVN-Kubernetes means that administrators must take extra actions to make External IPs and LoadBalancer Ingress VIPs work, even when these VIPs reside on one of the node local subnets.</p> <p>For External IPs, administrators can either assign the External IP to one of the nodes' Linux networking stacks if the External IP falls into one of the node's subnets. In this case, ARP requests to the External IP will be answered with ARP replies by the node that was assigned the External IP. For example, an admin could run <code>ip address add &lt;externalIP&gt;/32 dev lo</code> to make this work, assuming that <code>arp_ignore</code> is at its default setting of <code>0</code> and thus the Linux networking stack uses the default weak host model for ARP replies. An alternative could be to point one or multiple static routes for the External IP to one or several of the Kubernetes nodes. </p> <p>For LoadBalancer Ingress VIPs, an administrator will either use a tool such as MetalLB L2 mode. Or, they can configure ECMP load-sharing. ECMP load-sharing can be implemented via static routes which point to all Kubernetes nodes or via BGP route injection (e.g., MetalLB's BGP mode).</p>"},{"location":"design/gatway-accelerated-interface-configuration/","title":"Gateway Accelerated Interface Configuration","text":""},{"location":"design/gatway-accelerated-interface-configuration/#description","title":"Description","text":"<p>To provide hardware acceleration for traffic, both IN and OUT ports need to be a hardware  accelerated netdevice backed by the Network Interface Card hardware itself. In case of external traffic, when one such port is the external OVS bridge, which for example has the gateway IP,  such traffic (like host networking traffic) would not be accelerated. Using Switchdev VirtualFunction (VF) or SubFunction (SF) as a gateway interface allows to accelerate these too.</p>"},{"location":"design/gatway-accelerated-interface-configuration/#how-it-works","title":"How it works?","text":"<p>Instead of using the gateway interface as the external bridge itself, use a  switchdev VF or SF instead.  This is depicted as following:</p> <pre><code>                         +----------+\n                         |  br-ext  |\n                   +--------+       |\n                   | UPLINK |       |\n                   +--------+       |  patch  +----------+\n                         |          x---------x  br-int  |\n    +--------+     +--------+       |  port   +----------+\n    | NETDEV +-----+   REP  |       |\n    +--------+     +--------+       |\n                         +----------+\n</code></pre> <p>Where <code>UPLINK</code> is a port on an offloading capable network interface hardware, <code>NETDEV</code> is a switchdev function  of this port and <code>REP</code> is a representor netdevice of the switchdev function.  Node/Host IP assigned to <code>NETDEV</code> which make OVS to chose <code>REP</code> port for external flows instead of the bridge.</p>"},{"location":"design/gatway-accelerated-interface-configuration/#how-to-use","title":"How to use?","text":"<p>Gateway accelerated interface can be used in two steps:</p> <p>a) Creating and configuring the device.  See figure above. An <code>UPLINK</code> device is connected to the OVS external bridge.  An existing VF or SF <code>NETDEV</code> from the <code>UPLINK</code> is first selected as the the Gateway Interface. Its associated  representor <code>REP</code> is plugged into the OVS external bridge (br-ext). The gateway IP is assigned to this interface  instead of the OVS external bridge (br-ext). </p> <p>b) Specify <code>NETDEV</code> as a gateway interface explicitly via <code>OVN_GATEWAY_OPTS</code> environment variable for   ovnkube-node container. Example:</p> <pre><code>            - name: OVN_GATEWAY_OPTS\n              value: \"--gateway-accelerated-interface=&lt;&lt;NETDEV&gt;&gt;\"\n</code></pre> <p>Note that this is mutually exclusive to the <code>--gateway-interface</code> flag for GATEWAY_OPTIONS.</p> <p>c) Set the external-id on the bridge to detect the uplink device correctly. This is useful for instances where, the name of the bridge (eg: br-ext) does not use the uplink device (eg: p0) in its name. The uplink can also  be a bond device.  <pre><code>ovs-vsctl br-set-external-id br-ext bridge-uplink p0\n</code></pre> This gives more flexibility in detecting the uplink device in cases where the auto detection fails (like in case of  bonded uplinks etc.)</p>"},{"location":"design/gatway-accelerated-interface-configuration/#verification","title":"Verification","text":"<p>Openflow rules added to the external bridge will use this port as the IN/OUT port instead.</p> <p>Example flows when pf0vf1 is the netdev and pf0vf1_r is the representor <pre><code> cookie=0xdeff105, duration=505314.637s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=\"pf0vf1_r\",nw_dst=169.254.0.1 actions=ct(table=5,zone=64002,nat)\n cookie=0xdeff105, duration=505314.637s, table=0, n_packets=655, n_bytes=129843, priority=500,ip,in_port=\"pf0vf1_r\",nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.0.2))\n cookie=0xdeff105, duration=505314.637s, table=0, n_packets=359877855, n_bytes=531033264511, priority=205,udp,in_port=p0,dl_dst=42:0b:9a:f1:83:b2,tp_dst=6081 actions=output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=0, n_packets=6252796, n_bytes=775727815, priority=200,udp,in_port=\"pf0vf1_r\",tp_dst=6081 actions=output:p0\n cookie=0xdeff105, duration=505314.637s, table=0, n_packets=1867752, n_bytes=294547557, priority=100,ip,in_port=\"pf0vf1_r\" actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:p0\n cookie=0xdeff105, duration=505314.637s, table=0, n_packets=22, n_bytes=1320, priority=10,in_port=p0,dl_dst=42:0b:9a:f1:83:b2 actions=output:\"patch-brp0_c-23\",output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=1, n_packets=1313364, n_bytes=669490616, priority=100,ct_state=+est+trk,ct_mark=0x2,ip actions=output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=1, n_packets=0, n_bytes=0, priority=100,ct_state=+rel+trk,ct_mark=0x2,ip actions=output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=1, n_packets=0, n_bytes=0, priority=13,udp,in_port=p0,tp_dst=3784 actions=output:\"patch-brp0_c-23\",output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=1, n_packets=493602, n_bytes=48384748, priority=10,dl_dst=42:0b:9a:f1:83:b2 actions=output:\"pf0vf1_r\"\n cookie=0xdeff105, duration=505314.637s, table=3, n_packets=694, n_bytes=276779, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],mod_dl_dst:42:0b:9a:f1:83:b2,output:\"pf0vf1_r\"\n</code></pre></p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/","title":"Host -&gt; Services using OpenFlow with shared gateway bridge","text":""},{"location":"design/host-to-node-port-hairpin-trafficflow/#background","title":"Background","text":"<p>In order to allow host to Kubernetes service access packets originating from the host must go into OVN in order to hit load balancers and be routed to the appropriate destination. Typically when accessing a services backed by an OVN networked pod, a single interface can be used (ovn-k8s-mp0) in order to get into the local worker switch, hit the load balancer, and reach the pod endpoint. However, when a service is backed by host networked pods, the behavior becomes more complex. For example, if the host access a service, where the backing endpoint is the host itself, then the packet must hairpin back to the host after being load balanced. There are additional complexities to consider, such as when an host network endpoint is using a secondary IP on a NIC. To be able to solve all of the potential use cases for service traffic, OpenFlow is leveraged with OVN-Kubernetse programmed flows in order to steer service traffic.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#introduction","title":"Introduction","text":"<p>OpenFlow on the OVS gateway bridge to handle all host to service traffic and this methodology is used for both gateway modes (local and shared). However, the paths that local and shared take may be slightly different as local gateway mode requires that all service traffic uses host's routing stack as a next hop before leaving the node.</p> <p>Accessing services from the host via the shared gateway bridge means that traffic from the host enters the shared  gateway bridge and is then forwarded into the Gateway Router (GR). The complexity with this solution revolves around  the fact that the host and the GR both share the same node IP address. Due to this fact, the host and OVN can never know the other is using the same IP address and thus requires masquerading done inside of the shared gateway bridge.  Additionally, we want to avoid the shared gateway having to act like a router and managing control plane functions like ARP. This would add a bunch of overhead to managing the shared gateway bridge and greatly increase the cost of this implementation. </p> <p>In order to avoid ARP, both the OVN GR and the host think that the service CIDR is an externally routed network. In other words this both OVN and the host think that to reach the service CIDR they need to route their packet to some next hop external to the node. In the past OVN-Kubernetes has leveraged the default gateway as that next hop, routing all service traffic towards that default gateway. However, this behavior relied on the default gateway existing and being able to ARP for its MAC address. In the current implementation this dependency on the gateway has been removed, and a secondary network is configured on OVN and the host with a fake next hop in that secondary network. The default secondary networks for IPv4 and IPv6 are:  - <code>169.254.169.0/29</code>  - <code>fd69::/125</code></p> <p>OVN is assigned 169.254.169.1 and fd69::1, while the Host uses 169.254.169.2 and fd69::2. The next hop address used is 169.254.169.4 and fd69::4.</p> <p>This subnet is only used for services communication, and service access from the host will be routed via: <pre><code>10.96.0.0/16 via 169.254.169.4 dev breth0 mtu 1400\n</code></pre></p> <p>By doing this the destination MAC address will be the next hop MAC and can be manipulated to act as masquerade MAC to the host.  As the host goes to send traffic to the next hop, OpenFlow rules in br-ex hijack the packet, modify it, and redirect it  to the OVN GR. Similarly the reply packets from OVN GR are modified and masqueraded, before being sent back to the host.</p> <p>Since the next hop is not a real address it cannot perform ARP response, so static ARP entries are added to the host and OVN GR.</p> <p>For Host to pod access (and vice versa) the management port (ovn-k8s-mp0) is still used.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#new-load-balancer-behavior","title":"New Load Balancer Behavior","text":"<p>Load balancers with OVN are placed either on a router or worker switch. Previously, the \"node port\" load balancers (created on a per node basis) were applied to the GR and the worker switch, while singleton cluster wide load balancers for Cluster IP services were applied across all worker switches. With the new implementation, Cluster IP service traffic is destined for GR from the host and therefore the GR requires load balancers that can handle Cluster IP. In addition, the load balancer must not have the node's IP address as and endpoint. This is due to the fact that on a GR the node IP address is used. Therefore if a load balancer on the GR were to DNAT to its own node IP, the packet would be dropped.</p> <p>To solve this problem, an additional load balancer is added with this implementation. The purpose of this new load balancer is to accommodate host endpoints. If endpoints are added for a service that contain a host endpoint, that VIP is moved to the new load balancer. Additionally, if one of those endpoints contain this node's IP address, it is replaced with the host's special masqueraded IP (IPv4: 169.254.169.2, IPv6: fd69::2).</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#use-cases","title":"Use Cases","text":"<p>The following sections go over each potential traffic path originating from Host to Service. Note Host -&gt; node port or external IP services are DNAT'ed in iptables to the cluster IP address before being sent out. Therefore the behavior of any host to service is essentially the same, forwarded towards the Cluster IP via the shared gateway bridge.</p> <p>For all of the following use cases, follow this topology:</p> <pre><code>          host (ovn-worker, 172.18.0.3, 169.254.169.2) \n           |\neth0----|breth0| ------ 172.18.0.3   OVN GR 100.64.0.4 --- join switch --- ovn_cluster_router --- 10.244.1.3 pod\n                        169.254.169.1\n</code></pre> <p>The service used in the following use cases is: <pre><code>[trozet@trozet contrib]$ kubectl get svc web-service\nNAME          TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE\nweb-service   NodePort   10.96.146.87   &lt;none&gt;        80:30015/TCP,9999:32326/UDP   4m54s\n</code></pre></p> <p>OVN masquerade IPs are 169.254.169.1, and fd69::1</p> <p>Another worker node exists, ovn-worker2 at 172.18.0.4.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovn-masquerade-addresses","title":"OVN masquerade addresses","text":"<ul> <li>IPv4</li> <li>169.254.169.1: OVN masquerade address</li> <li>169.254.169.2: host masquerade address</li> <li>169.254.169.3: local ETP masquerade address (not used for shared GW, kept for parity)</li> <li>169.254.169.4: dummy next-hop masquerade address</li> <li>IPv6</li> <li>fd69::1: OVN masquerade address</li> <li>fd69::2: host masquerade address</li> <li>fd69::3: local ETP masquerade address (not used for shared GW, kept for parity)</li> <li>fd69::4: dummy next-hop masquerade address</li> </ul>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#configuration-details","title":"Configuration Details","text":""},{"location":"design/host-to-node-port-hairpin-trafficflow/#host","title":"Host","text":"<p>As previously mentioned the host is configured with the <code>169.254.169.2</code> and <code>fd69::2</code> addresses. These are configured on the shared gateway bridge interface (typically br-ex or breth0) in the kernel. Additionally a route for service traffic is added as well as a static ARP/IPv6 neighbor entry.</p> <p>The services will now be reached from the nodes via this dummy next hop masquerade IP (i.e. <code>169.254.169.4</code>).</p> <p>Additionally, to enable the hairpin scenario, we need to provision a route specifying traffic originating from the host (acting as an host networked endpoint reply) must be routed to the OVN masquerade address (i.e. <code>169.254.169.1</code>) using the source IP address of the real host IP. This route looks like: <code>169.254.169.1/32 dev breth0 mtu 1400 src &lt;node IP address&gt;</code></p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovn","title":"OVN","text":"<p>In OVN we do not explicitly configure the <code>169.254.169.1</code> and <code>fd69::1</code> addresses on the GR interface. OVN is only able to have a single primary IP on the interface. Instead, we simply add a MAC Binding (ARP entry equivalent) for the next hop address, as well as a route for the secondary subnets to the next hop. This is all that is required in order to allow OVN to route the packet towards the fake next hop.</p> <p>The SBDB MAC binding will look like this (one per node): <pre><code>_uuid               : f244faba-19ad-4b08-bffe-d0b52b270410\ndatapath            : 7cc30875-0c68-4ecc-8193-a9fe3abb6cd5\nip                  : \"169.254.169.4\"\nlogical_port        : rtoe-GR_ovn-worker\nmac                 : \"0a:58:a9:fe:a9:04\"\ntimestamp           : 0\n</code></pre></p> <p>Each OVN GR will then have a route table that looks like this: <pre><code>[root@ovn-control-plane ~]# ovn-nbctl lr-route-list GR_ovn-worker\nIPv4 Routes\nRoute Table &lt;main&gt;:\n         169.254.169.0/29             169.254.169.4 dst-ip rtoe-GR_ovn-worker\n            10.244.0.0/16                100.64.0.1 dst-ip\n                0.0.0.0/0                172.18.0.1 dst-ip rtoe-GR_ovn-worker\n</code></pre></p> <p>Notice 169.254.169.0 route via 169.254.169.4 works, even though OVN does not have an IP on that subnet: <pre><code>[root@ovn-control-plane ~]# ovn-nbctl show GR_ovn-worker\nrouter 7c1323d5-f388-449f-94cb-51216194c606 (GR_ovn-worker)\n    port rtoj-GR_ovn-worker\n        mac: \"0a:58:64:40:00:03\"\n        networks: [\"100.64.0.3/16\"]\n    port rtoe-GR_ovn-worker\n        mac: \"02:42:ac:12:00:02\"\n        networks: [\"172.18.0.2/16\"]\n    nat 98e32e9b-e8f1-413c-881d-cfdfd5a02d43\n        external ip: \"172.18.0.3\"\n        logical ip: \"10.244.0.0/16\"\n        type: \"snat\"\n</code></pre></p> <p>This works because the output port <code>rtoe_GR_ovn-worker</code> is configured on the route. OVN will simply lookup the MAC binding for 169.254.169.4, and then forward it out the rtoe_GR_ovn-worker interface, while SNAT'ing the source IP of the packet to its primary address of 1721.8.0.3.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#ovs","title":"OVS","text":"<p>The gateway bridge flows are managed by OVN-Kubernetes. Priority 500 flows are added which are specifically there to handle service traffic for this design. These flows handle masquerading between the host and OVN, while flows in later tables take care of rewriting MAC addresses.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#openflow-flows","title":"OpenFlow Flows","text":"<p>With the new implementation comes new OpenFlow rules in the shared gateway bridge. The following flows are added and used: <pre><code> cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=169.254.169.2 actions=ct(commit,table=4,zone=64001,nat(dst=172.18.0.3))\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n cookie=0xdeff105, duration=793.273s, table=0, n_packets=0, n_bytes=0, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n\n cookie=0xdeff105, duration=5.507s, table=2, n_packets=0, n_bytes=0, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n cookie=0xdeff105, duration=5.507s, table=3, n_packets=0, n_bytes=0, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n cookie=0xdeff105, duration=793.273s, table=4, n_packets=0, n_bytes=0, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n cookie=0xdeff105, duration=793.273s, table=5, n_packets=0, n_bytes=0, ip actions=ct(commit,table=2,zone=64001,nat)\n</code></pre></p> <p>How these flows are used will be explained in more detail in the following sections. The main thing to remember for now is that the shared gateway bridge will use Conntrack zones 64001 and 64002 to handle the new masquerading functionality.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-ovn-pod","title":"Host -&gt; Service -&gt; OVN Pod","text":"<p>This is the most simple case where a host wants to reach a service backed by an OVN Networked pod. For this example the service contains a single endpoint as an ovn networked pod on ovn-worker node: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   10.244.1.3:80,10.244.1.3:9999   6m13s\n</code></pre></p> <p>The general flow is:</p> <ol> <li>TCP Packet is sent via the host (ovn-worker) to an service: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=172.18.0.3 dst=10.96.146.87 sport=47108 dport=80 src=10.96.146.87 dst=172.18.0.3 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> <li>The host routes this packet towards the next hop's (169.254.169.4) MAC address via shared gateway bridge breth0.</li> <li>Flows in breth0 hijack the packet, SNAT to the host's masquerade IP (169.254.169.2) and send it to OF table 2: <pre><code>cookie=0xdeff105, duration=1136.261s, table=0, n_packets=12, n_bytes=884, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre></li> <li>In table 2, the destination MAC address is modified to be the MAC of the OVN GR. Note, although the source MAC is the host's, OVN does not care so this is left unmodified. <pre><code>cookie=0xdeff105, duration=1.486s, table=2, n_packets=12, n_bytes=884, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=172.18.0.3 dst=10.96.146.87 sport=47108 dport=80 src=10.96.146.87 dst=169.254.169.2 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64001 use=1\n</code></pre></li> <li>OVN GR receives the packet, DNAT's to pod endpoint IP: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=47108 dport=80 src=10.244.1.3 dst=169.254.169.2 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR SNAT's to the join switch IP and sends the packet towards the pod: CT Entry: <pre><code>tcp      6 114 TIME_WAIT src=169.254.169.2 dst=10.244.1.3 sport=47108 dport=80 src=10.244.1.3 dst=100.64.0.4 sport=80 dport=47108 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> </ol>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply","title":"Reply","text":"<p>The reply packet simply uses same reverse path and packets are unNAT'ed on their way back towards breth0 and eventually  the LOCAL host port. The OVN GR will think it is routing towards the next hop and set the dest MAC to be the MAC of 169.254.169.4. In OpenFlow, the return packet will hit this first flow in the shared gateway bridge: <pre><code>cookie=0xdeff105, duration=1136.261s, table=0, n_packets=11, n_bytes=1670, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n</code></pre> This flow will unDNAT the packet, and send to table 3: <pre><code>cookie=0xdeff105, duration=1.486s, table=3, n_packets=11, n_bytes=1670, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n</code></pre> This flow will move the dest MAC (next hop MAC) to be the source, and set the new dest MAC to be the MAC of the host. This ensures the Linux host thinks it is still talking to the external next hop. The packet is then delivered to the host.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-host-endpoint-on-different-node","title":"Host -&gt; Service -&gt; Host Endpoint on Different Node","text":"<p>This example becomes slightly more complex as the packet must be hairpinned by the GR and then sent out of the node. In this example the backing service endpoint will be a pod running on the other worker node (ovn-worker2) at 172.18.0.4: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   172.18.0.4:80,172.18.0.4:9999   32m\n</code></pre></p> <p>Steps 1 through 4 in this example are the same as the previous use case.</p> <ol> <li>OVN GR receives the packet, DNAT's to ovn-worker2's endpoint IP: CT Entry: <pre><code>tcp      6 116 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=55978 dport=80 src=172.18.0.4 dst=169.254.169.2 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR hairpins the packet back towards breth0 while SNAT'ing to the GR IP 172.18.0.3, and forwarding towards 172.18.0.4: <pre><code>tcp      6 116 TIME_WAIT src=172.18.0.3 dst=172.18.0.4 sport=55978 dport=80 src=172.18.0.4 dst=172.18.0.3 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64000 use=1\n</code></pre></li> <li>As this packet comes back into breth0 it is treated like any other normal packet going from OVN to the external network. The packet is Conntracked in zone 64000, marked with 1, and sent out of the eth0 interface: <pre><code>cookie=0xdeff105, duration=15820.270s, table=0, n_packets=0, n_bytes=0, priority=100,ip,in_port=\"patch-breth0_ov\" actions=ct(commit,zone=64000,exec(load:0x1-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> CT Entry: <pre><code>tcp      6 116 TIME_WAIT src=172.18.0.3 dst=172.18.0.4 sport=55978 dport=80 src=172.18.0.4 dst=172.18.0.3 sport=80 dport=55978 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64000 use=1\n</code></pre></li> </ol>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply_1","title":"Reply","text":"<p>When the reply packet comes back into ovn-worker via the eth0 interface, the packet will be forwarded back into OVN GR  due to a CT match in zone 64000 with a marking of 1: <pre><code>cookie=0xdeff105, duration=15820.270s, table=0, n_packets=5442, n_bytes=4579489, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\ncookie=0xdeff105, duration=15820.270s, table=1, n_packets=0, n_bytes=0, priority=100,ct_state=+est+trk,ct_mark=0x1,ip actions=output:\"patch-breth0_ov\"\ncookie=0xdeff105, duration=15820.270s, table=1, n_packets=0, n_bytes=0, priority=100,ct_state=+rel+trk,ct_mark=0x1,ip actions=output:\"patch-breth0_ov\"\n</code></pre></p> <p>OVN GR will then handle unSNAT, unDNAT and send the packet back towards breth0 where the packet will be handled the same way as it was in the previous example.</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#host-service-host-endpoint-on-same-node","title":"Host -&gt; Service -&gt; Host Endpoint on Same Node","text":"<p>This is the most complicated use case. Unlike the previous examples, multiple sessions have to be established and masqueraded between the host and OVN in order to trick the host into thinking it has two external connections that are not the same. This avoids issues with RPF as well as short-circuiting where the host would skip sending traffic back to OVN because it knows the traffic destination is local to itself. In this example a host network pod is used as the backend endpoint on ovn-worker: <pre><code>[trozet@trozet contrib]$ kubectl get ep web-service\nNAME          ENDPOINTS                       AGE\nweb-service   172.18.0.3:80,172.18.0.3:9999   42m\n</code></pre></p> <p>There is another manifestation of this case. Sometimes, an endpoint contains a secondary interface on a node. This can happen when endpoints are manually managed (especially in the case of api-server trickery). Thus, the endpoints look like <pre><code>$ kubectl get ep kubernetes\nNAME          ENDPOINTS                       AGE\nkubernetes    172.20.0.2:4443                  42m\n</code></pre></p> <p>where <code>172.20.0.2</code> is an \"extra\" address, maybe even on a different interface (e.g. <code>lo</code>).</p> <p>Like the previous example, Steps 1-4 are the same. Continuing with Step 5   :</p> <ol> <li>OVN GR receives the packet, DNAT's to ovn-worker's endpoint IP. However, if the endpoint IP is the node's physical IP, then it is replaced in the OVN load-balancer backends with the host masquerade IP (169.254.169.2): CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=169.254.169.2 dst=10.96.146.87 sport=33316 dport=80 src=169.254.169.2 dst=169.254.169.2 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=16 use=1\n</code></pre></li> <li>OVN GR hairpins the packet back towards breth0 while SNAT'ing to the GR IP 172.18.0.3, and forwarding towards 169.254.169.2: CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=169.254.169.2 dst=169.254.169.2 sport=33316 dport=80 src=169.254.169.2 dst=172.18.0.3 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 use=1\n</code></pre></li> <li>The packet is received in br-eth0, where it hits one of two flows, depending on if the destination is the masquerade IP or a \"secondary\" IP: <pre><code>cookie=0xdeff105, duration=2877.527s, table=0, n_packets=11, n_bytes=810, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=169.254.169.2 actions=ct(commit,table=4,zone=64001,nat(dst=172.18.0.3)) #primary case\ncookie=0xdeff105, duration=2877.527s, table=0, n_packets=11, n_bytes=810, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=172.18.0.3,nw_dst=127.20.0.2 actions=ct(commit,table=4,zone=64001) # secondary case\n</code></pre> This flow detects the packet is destined for the masqueraded host IP, DNATs it if necessary back to the host, and sends to table 4. CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=172.18.0.3 dst=169.254.169.2 sport=33316 dport=80 src=172.18.0.3 dst=172.18.0.3 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64001 use=1\n</code></pre></li> <li>In table 4, the packet hits the following flow: <pre><code>cookie=0xdeff105, duration=2877.527s, table=4, n_packets=11, n_bytes=810, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n</code></pre> Here, the packet is SNAT'ed to the special OVN masquerade IP (169.254.169.1) in order to obfuscate the node IP from the host as a source address. CT Entry: <pre><code>tcp      6 117 TIME_WAIT src=172.18.0.3 dst=172.18.0.3 sport=33316 dport=80 src=172.18.0.3 dst=169.254.169.1 sport=80 dport=33316 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=64002 use=1\n</code></pre></li> <li>The packet then enters table 3, where the MAC addresses are modified and sent to the LOCAL host.</li> </ol> <p>The trick here is that the host thinks it has two different connections, which are really part of the same single connection: 1. 172.18.0.3-&gt; 10.96.x.x 2. 169.254.169.1 -&gt; 172.18.0.3</p>"},{"location":"design/host-to-node-port-hairpin-trafficflow/#reply_2","title":"Reply","text":"<p>Reply traffic comes back into breth0 where via seeing a response to 169.254.169.1, we know this is a reply packet and thus hits an OpenFlow rule: <pre><code>cookie=0xdeff105, duration=2877.527s, table=0, n_packets=12, n_bytes=1736, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n</code></pre> This flow will unSNAT in zone 64002, and then send the packet to table 5. In table 5 it will hit this flow: <pre><code>cookie=0xdeff105, duration=2877.527s, table=5, n_packets=12, n_bytes=1736, ip actions=ct(commit,table=2,zone=64001,nat)\n</code></pre> Here the packet will be unDNAT'ed and sent towards table 2: <pre><code>cookie=0xdeff105, duration=5.520s, table=2, n_packets=47, n_bytes=4314, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> Table 2 will modify the MAC accordingly and sent it back into OVN GR. OVN GR will then unSNAT, unDNAT and send the packet back to breth0 in the same fashion as the previous example.</p>"},{"location":"design/service-traffic-policy/","title":"Kubernetes Service Traffic Policy Implementation","text":""},{"location":"design/service-traffic-policy/#external-traffic-policy","title":"External Traffic Policy","text":"<p>For Kubernetes Services of type Nodeport or Loadbalancer a user can set the <code>service.spec.externalTrafficPolicy</code> field to either <code>cluster</code> or <code>local</code> to denote whether or not external traffic is routed to cluster-wide or node-local endpoints. The default value for the <code>externalTrafficPolicy</code> field is <code>cluster</code>. In this configuration in ingress traffic is equally disributed across all backends and the original client IP address is lost due to SNAT. If set to <code>local</code> then the client source IP is preserved throughout the service flow and if service traffic arrives at nodes without local endpoints it gets dropped. See sources for more information on ETP=local.</p> <p>Setting an <code>ExternalTrafficPolicy</code> to <code>Local</code> is only allowed for Services of type <code>NodePort</code> or <code>LoadBalancer</code>. The APIServer enforces this requirement.</p>"},{"location":"design/service-traffic-policy/#implementing-externaltrafficpolicy-in-ovn-kubernetes","title":"Implementing <code>externalTrafficPolicy</code> In OVN-Kubernetes","text":"<p>To properly implement this feature for all relevant traffic flows, required changing how OVN, Iptables rules, and Physical OVS flows are updated and managed in OVN-Kubernetes</p>"},{"location":"design/service-traffic-policy/#externaltrafficpolicylocal","title":"ExternalTrafficPolicy=Local","text":""},{"location":"design/service-traffic-policy/#ovn-load_balancer-configuration","title":"OVN Load_Balancer configuration","text":"<p>Normally, each service in Kubernetes has a corresponding single Load_Balancer row created in OVN. This LB is attached to all node switches and gateway routers (GWRs). ExternalTrafficPolicy creates multiple LBs, however.</p> <p>Specifically, different load balancers are attached to switches versus routers. The node switch LBs handle traffic from pods, whereas the gateway router LBs handle external traffic.</p> <p>Thus, an additional LB is created with the <code>skip_snat=\"true\"</code> option and is applied to the GatewayRouters and Worker switches. It is needed to override the <code>lb_force_snat_ip=router_ip</code> option that is on all the Gateway Routers, which allows ingress traffic to arrive at OVN managed endpoints with the original client IP.</p> <p>All externally-accessible vips (NodePort, ExternalIPs, LoadBalancer Status IPs) for services with <code>externalTrafficPolicy:local</code> will reside on this loadbalancer. The loadbalancer backends may be empty, depending on whether there are pods local to that node.</p>"},{"location":"design/service-traffic-policy/#handling-flows-between-the-overlay-and-underlay","title":"Handling Flows between the overlay and underlay","text":"<p>In this section we will look at some relevant traffic flows when a service's <code>externalTrafficPolicy</code> is <code>local</code>.  For these examples we will be using a Nodeport service, but the flow is generally the same for ExternalIP and Loadbalancer type services.</p>"},{"location":"design/service-traffic-policy/#ingress-traffic","title":"Ingress Traffic","text":"<p>This section will cover the networking entities hit when traffic ingresses a cluster via a service to either host networked pods or cluster networked pods. If its host networked pods, then the traffic flow is the same on both gateway modes. If its cluster networked pods, they will be different for each mode.</p>"},{"location":"design/service-traffic-policy/#external-source-service-ovn-pod","title":"External Source -&gt; Service -&gt; OVN pod","text":""},{"location":"design/service-traffic-policy/#shared-gateway-mode","title":"Shared Gateway Mode","text":"<p>This case is the same as normal shared gateway traffic ingress, meaning the externally sourced traffic is routed into OVN via flows on breth0, except in this case the new local load balancer is hit on the GR, which ensures the ip of the client is preserved  by the time it gets to the destination Pod.</p> <pre><code>          host (ovn-worker, 172.18.0.3) \n           |\neth0---&gt;|breth0| -----&gt; 172.18.0.3 OVN GR 100.64.0.4 --&gt; join switch --&gt; ovn_cluster_router --&gt; 10.244.1.3 pod\n</code></pre>"},{"location":"design/service-traffic-policy/#local-gateway-mode","title":"Local Gateway Mode","text":"<p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway if the above path is used, response traffic would be assymmetric since the default route for pod egress traffic is via <code>ovn-k8s-mp0</code>.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host.</p> <pre><code>          host (ovn-worker, 172.18.0.3) ---- 172.18.0.3 LOCAL(host) -- iptables -- ovn-k8s-mp0 -- node-local-switch -- 10.244.1.3 pod\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to a masqueradeIP (169.254.169.3) used specially for this traffic flow.</li> </ol> <pre><code>[3:180] -A OVN-KUBE-ETP -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31746 -j DNAT --to-destination 169.254.169.3:31746\n</code></pre> <ol> <li>The special masquerade route in the host sends this packet into OVN via the management port.</li> </ol> <pre><code>169.254.169.3 via 10.244.0.1 dev ovn-k8s-mp0 \n</code></pre> <ol> <li>Since by default, all traffic into <code>ovn-k8s-mp0</code> gets SNAT-ed, we add an IPtable rule to <code>OVN-KUBE-SNAT-MGMTPORT</code> chain to ensure it doesn't get SNAT-ed to preserve its source-ip.</li> </ol> <pre><code>[3:180] -A OVN-KUBE-SNAT-MGMTPORT -p tcp -m tcp --dport 31746 -j RETURN\n</code></pre> <ol> <li>Traffic enters the node local switch on the worker node and hits the load-balancer where we add a new vip for this masqueradeIP to DNAT it correctly to the local backends. Note that this vip will translate only to the backends that are local to that worker node and hence traffic will be rejected if there is no local endpoint thus respecting ETP=local type traffic rules.</li> </ol> <p>The switch load-balancer on a node with local endpoints will look like this:</p> <pre><code>_uuid               : b3201caf-3089-4462-b96e-1406fd7c4256\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_node_switch_ovn-worker2\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"169.254.169.3:31746\"=\"10.244.1.3:8080\", \"172.18.0.3:31746\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre> <p>The switch load-balancer on a node without local endpoints will look like this: <pre><code>_uuid               : 42d75e10-5598-4197-a6f2-1a37094bee13\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_node_switch_ovn-worker\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"169.254.169.3:31746\"=\"\", \"172.18.0.4:31746\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre></p> <p>Response traffic will follow the same path (backend-&gt;node switch-&gt;mp0-&gt;host-&gt;breth0-&gt;eth0).</p> <ol> <li>Return traffic gets matched on default flow in <code>table0</code> and it sent out via default interface back to the external source.</li> </ol> <pre><code>cookie=0xdeff105, duration=12994.192s, table=0, n_packets=47706, n_bytes=3199460, idle_age=0, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:1\n</code></pre> <p>The conntrack state looks like this: <pre><code>    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 [UNREPLIED] src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366\n    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=169.254.169.3 sport=36366 dport=31746 [UNREPLIED] src=10.244.1.3 dst=172.18.0.1 sport=8080 dport=36366 zone=9\n    [NEW] tcp      6 120 SYN_SENT src=172.18.0.1 dst=10.244.1.3 sport=36366 dport=8080 [UNREPLIED] src=10.244.1.3 dst=172.18.0.1 sport=8080 dport=36366 zone=11\n [UPDATE] tcp      6 60 SYN_RECV src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366\n [UPDATE] tcp      6 432000 ESTABLISHED src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n    [NEW] tcp      6 300 ESTABLISHED src=172.18.0.3 dst=172.18.0.1 sport=31746 dport=36366 [UNREPLIED] src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 mark=2 zone=64000\n [UPDATE] tcp      6 120 FIN_WAIT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n [UPDATE] tcp      6 30 LAST_ACK src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n [UPDATE] tcp      6 120 TIME_WAIT src=172.18.0.1 dst=172.18.0.3 sport=36366 dport=31746 src=169.254.169.3 dst=172.18.0.1 sport=31746 dport=36366 [ASSURED]\n</code></pre></p>"},{"location":"design/service-traffic-policy/#allocateloadbalancernodeportsfalse","title":"AllocateLoadBalancerNodePorts=False","text":"<p>If AllocateLoadBalancerNodePorts=False then we cannot follow the same path as above since there won't be any node ports to DNAT to. Thus, for this special case which is only applicable to services of type LoadBalancer, we directly DNAT to the endpoints. Steps 1 &amp; 2 are same as above i.e packet arrives into the host via openflows on <code>breth0</code>.</p> <ol> <li>In the host, we introduce IPtable rules in the PREROUTING chain that DNATs this packet matched on externalIP or load balancer ingress VIP directly to the pod backend using random probability mode algorithm</li> </ol> <pre><code>[0:0] -A OVN-KUBE-ETP -d 172.18.0.10/32 -p tcp -m tcp --dport 80 -m statistic --mode random --probability 0.50000000000 -j DNAT --to-destination 10.244.0.3:8080\n[3:180] -A OVN-KUBE-ETP -d 172.18.0.10/32 -p tcp -m tcp --dport 80 -m statistic --mode random --probability 1.00000000000 -j DNAT --to-destination 10.244.0.4:8080\n</code></pre> <ol> <li>The pod subnet route in the host sends this packet into OVN via the management port.</li> </ol> <pre><code>10.244.0.0/24 dev ovn-k8s-mp0 proto kernel scope link src 10.244.0.2 \n</code></pre> <ol> <li>Since by default, all traffic into <code>ovn-k8s-mp0</code> gets SNAT-ed, we add an IPtable rule to <code>OVN-KUBE-SNAT-MGMTPORT</code> chain to ensure it doesn't get SNAT-ed to preserve its source-ip.</li> </ol> <pre><code>[0:0] -A OVN-KUBE-SNAT-MGMTPORT -d 10.244.0.3/32 -p tcp -m tcp --dport 8080 -j RETURN\n[3:180] -A OVN-KUBE-SNAT-MGMTPORT -d 10.244.0.4/32 -p tcp -m tcp --dport 8080 -j RETURN\n</code></pre> <ol> <li>Traffic hits the switch and enters the destination pod. OVN load balancers play no role in this traffic flow.</li> </ol> <p>Here are conntrack entries for this traffic: <pre><code>tcp,orig=(src=172.18.0.5,dst=10.244.0.4,sport=50134,dport=8080),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),zone=11,protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.5,dst=172.18.0.10,sport=50134,dport=80),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.10,dst=172.18.0.5,sport=80,dport=50134),reply=(src=172.18.0.5,dst=172.18.0.10,sport=50134,dport=80),zone=64000,mark=2,protoinfo=(state=TIME_WAIT)\ntcp,orig=(src=172.18.0.5,dst=10.244.0.4,sport=50134,dport=8080),reply=(src=10.244.0.4,dst=172.18.0.5,sport=8080,dport=50134),zone=10,protoinfo=(state=TIME_WAIT)\n</code></pre></p>"},{"location":"design/service-traffic-policy/#external-source-service-host-networked-pod","title":"External Source -&gt; Service -&gt; Host Networked pod","text":"<p>This Scenario is a bit different, specifically traffic now needs to be directed from an external source to service and then to the host itself (a host networked pod)</p> <p>In this flow, rather than going from breth0 into OVN we shortcircuit the path with physical flows on breth0. This is the same for both the gateway modes.</p> <pre><code>          host (ovn-worker, 172.18.0.3) \n           ^\n           ^\n           |\neth0---&gt;|breth0| ---- 172.18.0.3 OVN GR 100.64.0.4 -- join switch -- ovn_cluster_router -- 10.244.1.3 pod\n</code></pre> <ol> <li>Match on the incoming traffic via it's nodePort, DNAT directly to the host networked endpoint, and send to <code>table=6</code></li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=153.288s, table=0, n_packets=18, n_bytes=1468, idle_age=100, priority=100,tcp,in_port=1,tp_dst=&lt;nodePort&gt; actions=ct(commit,table=6,zone=64003,nat(dst=&lt;nodeIP&gt;:&lt;targetIP&gt;))\n</code></pre> <ol> <li>Send out the LOCAL ovs port on breth0, and traffic is delivered to host netwoked pod</li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=113.033s, table=6, n_packets=18, n_bytes=1468, priority=100 actions=LOCAL\n</code></pre> <ol> <li>Return traffic from the host networked pod to external source is matched in <code>table=7</code> based on the src_ip of the return    traffic being equal to <code>&lt;targetIP&gt;</code>, and un-Nat back to <code>&lt;nodeIP&gt;:&lt;NodePort&gt;</code></li> </ol> <pre><code> cookie=0x790ba3355d0c209b, duration=501.037s, table=0, n_packets=12, n_bytes=1259, idle_age=448, priority=100,tcp,in_port=LOCAL,tp_src=&lt;targetIP&gt; actions=ct(commit,table=7,zone=64003,nat)\n</code></pre> <ol> <li>Send the traffic back out breth0 back to the external source in <code>table=7</code></li> </ol> <pre><code>cookie=0x790ba3355d0c209b, duration=501.037s, table=7, n_packets=12, n_bytes=1259, idle_age=448, priority=100 actions=output:1\n</code></pre>"},{"location":"design/service-traffic-policy/#host-traffic","title":"Host Traffic","text":"<p>NOTE: Host-&gt; svc (NP/EIP/LB) is neither \"internal\" nor \"external\" traffic, hence it defaults to special case \"Cluster\" even if ETP=local. Only Host-&gt;differentNP traffic flow obeys ETP=local.</p>"},{"location":"design/service-traffic-policy/#externaltrafficpolicycluster","title":"ExternalTrafficPolicy=Cluster","text":""},{"location":"design/service-traffic-policy/#local-gateway-mode_1","title":"Local Gateway Mode","text":""},{"location":"design/service-traffic-policy/#external-source-service-host-networked-pod-non-hairpin-case","title":"External Source -&gt; Service -&gt; Host Networked pod (non-hairpin case)","text":"<p>NOTE: Same steps happen for <code>Host -&gt; Service -&gt; Host Networked Pods (non-hairpin case)</code>.</p> <p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway all service traffic is sent straight to host (instead of sending it to OVN) to allow users to apply custom routes according to their use cases.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host. Similarly rather than sending the DNAT-ed traffic from OVN to wire, we send it to host first.</p> <pre><code>          host (ovn-worker2, 172.19.0.3) ---- 172.19.0.3 LOCAL(host) -- iptables -- breth0 -- GR -- breth0 -- host -- breth0 -- eth0 (backend ovn-worker 172.19.0.4)\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <p>SYN flow:</p> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000,nat)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to its clusterIP:targetPort</li> </ol> <pre><code>[8:480] -A OVN-KUBE-NODEPORT -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31339 -j DNAT --to-destination 10.96.115.103:80\n</code></pre> <ol> <li>The service route in the host sends this packet back to breth0.</li> </ol> <pre><code>10.96.0.0/16 via 169.254.169.4 dev breth0 mtu 1400 \n</code></pre> <ol> <li>On breth0, we have priority 500 flows meant to handle hairpining, that will SNAT the srcIP to the special <code>169.254.169.2</code> masqueradeIP and send it to <code>table2</code></li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=11, n_bytes=814, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <ol> <li>In <code>table2</code> we have a flow that forwards this to patch port that takes the traffic in OVN:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=2, n_packets=11, n_bytes=814, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>Traffic enters the GR on the worker node and hits the load-balancer where we DNAT it correctly to the local backends.</li> </ol> <p>The GR load-balancer on a node with endpoints for the clusterIP will look like this:</p> <pre><code>_uuid               : 4e7ff1e3-a211-45d7-8243-54e087ca3965                                                                                                                   \nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-hello-world\"}                                                                \nhealth_check        : []                                                                                                                                                     \nip_port_mappings    : {}                                                                                                                                                     \nname                : \"Service_default/example-service-hello-world_TCP_node_router+switch_ovn-control-plane\"                                                                 \noptions             : {event=\"false\", hairpin_snat_ip=\"169.254.169.5 fd69::5\", reject=\"true\", skip_snat=\"false\"}                                                             \nprotocol            : tcp                                                                                                                                                    \nselection_fields    : []                                                                                                                                                     \nvips                : {\"10.96.115.103:80\"=\"172.19.0.3:8080\", \"172.19.0.3:31339\"=\"172.19.0.4:8080\"} \n</code></pre> <ol> <li>Traffic from OVN is sent back to host:</li> </ol> <pre><code>  cookie=0xdeff105, duration=839.789s, table=0, n_packets=6, n_bytes=484, priority=175,tcp,in_port=\"patch-breth0_ov\",nw_src=172.19.0.3 actions=ct(table=4,zone=64001)\n  cookie=0xdeff105, duration=2334.510s, table=4, n_packets=18, n_bytes=1452, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n  cookie=0xdeff105, duration=1.612s, table=3, n_packets=10, n_bytes=892, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:13:00:03-&gt;eth_dst,LOCAL\n</code></pre> <ol> <li>The routes in the host send this back to breth0:</li> </ol> <pre><code>169.254.169.1 dev breth0 src 172.19.0.3 mtu 1400 \n</code></pre> <ol> <li>Traffic leaves to primary interface from breth0:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=0, n_packets=7611, n_bytes=754388, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>Packet goes to other host via underlay.</p> <p>SYNACK flow:</p> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000,nat)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=1, n_packets=9466, n_bytes=4512265, priority=100,ct_state=+est+trk,ct_mark=0x2,ip actions=LOCAL\n</code></pre> <ol> <li>Before coming to host in breth0 using above flow it will get unSNATed back to .1 masqueradeIP in 64000 zone, then unDNATed back to clusterIP using iptables and sent to OVN:</li> </ol> <pre><code> cookie=0xdeff105, duration=2334.510s, table=0, n_packets=14, n_bytes=1356, priority=500,ip,in_port=LOCAL,nw_dst=169.254.169.1 actions=ct(table=5,zone=64002,nat)\n cookie=0xdeff105, duration=2334.510s, table=5, n_packets=14, n_bytes=1356, ip actions=ct(commit,table=2,zone=64001,nat)\n cookie=0xdeff105, duration=0.365s, table=2, n_packets=33, n_bytes=2882, actions=set_field:02:42:ac:13:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>From OVN it gets sent back to host and then back from host into breth0 and into the wire:</li> </ol> <pre><code>  cookie=0xdeff105, duration=2334.510s, table=0, n_packets=18, n_bytes=1452, priority=175,ip,in_port=\"patch-breth0_ov\",nw_src=172.19.0.3 actions=ct(table=4,zone=64001,nat)\n  cookie=0xdeff105, duration=2334.510s, table=4, n_packets=18, n_bytes=1452, ip actions=ct(commit,table=3,zone=64002,nat(src=169.254.169.1))\n  cookie=0xdeff105, duration=0.365s, table=3, n_packets=32, n_bytes=2808, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:13:00:03-&gt;eth_dst,LOCAL\n  cookie=0xdeff105, duration=2334.510s, table=0, n_packets=7611, n_bytes=754388, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>NOTE: We have added a masquerade rule to iptable rules to SNAT towards the netIP of the interface via which the packet leaves.</p> <pre><code>[12:720] -A POSTROUTING -s 169.254.169.0/29 -j MASQUERADE\n</code></pre> <p>tcpdump: <pre><code>SYN:\n13:38:52.988279 eth0  In  ifindex 19 02:42:df:4d:b6:d2 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 172.19.0.3.30950: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.988315 breth0 In  ifindex 6 02:42:df:4d:b6:d2 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 172.19.0.3.30950: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.988357 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.1.36363 &gt; 10.96.211.228.80: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.989240 breth0 In  ifindex 6 02:42:ac:13:00:03 ethertype IPv4 (0x0800), length 80: 169.254.169.1.36363 &gt; 172.19.0.4.8080: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\n13:38:52.989240 breth0 In  ifindex 6 02:42:ac:13:00:03 ethertype IPv4 (0x0800), length 80: 172.19.0.3.31991 &gt; 172.19.0.4.8080: Flags [S], seq 3548868802, win 64240, options [mss 1460,sackOK,TS val 1854443570 ecr 0,nop,wscale 7], length 0\nSYNACK:\n13:38:52.989515 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.4.8080 &gt; 172.19.0.3.31991: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989515 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.4.8080 &gt; 169.254.169.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989562 breth0 In  ifindex 6 0a:58:a9:fe:a9:04 ethertype IPv4 (0x0800), length 80: 10.96.211.228.80 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989571 breth0 Out ifindex 6 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.3.30950 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n13:38:52.989581 eth0  Out ifindex 19 02:42:ac:13:00:04 ethertype IPv4 (0x0800), length 80: 172.19.0.3.30950 &gt; 172.19.0.1.36363: Flags [S.], seq 3406651567, ack 3548868803, win 65160, options [mss 1460,sackOK,TS val 2294391439 ecr 1854443570,nop,wscale 7], length 0\n</code></pre></p>"},{"location":"design/service-traffic-policy/#external-source-service-ovn-pod_1","title":"External Source -&gt; Service -&gt; OVN pod","text":"<p>The implementation of this case differs for local gateway from that for shared gateway. In local gateway all service traffic is sent straight to host (instead of sending it to OVN) to allow users to apply custom routes according to their use cases.</p> <p>In local gateway mode, rather than sending the traffic from breth0 into OVN via gateway router, we use flows on breth0 to send it into the host.</p> <pre><code>          host (ovn-worker, 172.18.0.3) ---- 172.18.0.3 LOCAL(host) -- iptables -- breth0 -- GR -- 10.244.1.3 pod\n           ^\n           ^\n           |\neth0---&gt;|breth0|\n</code></pre> <ol> <li>Match on the incoming traffic via default flow on <code>table0</code>, send it to <code>table1</code>:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=99979, n_bytes=298029215, priority=50,ip,in_port=eth0 actions=ct(table=1,zone=64000)\n</code></pre> <ol> <li>Send it out to LOCAL ovs port on breth0 and traffic is delivered to the host:</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.787s, table=1, n_packets=108, n_bytes=23004, priority=0 actions=NORMAL\n</code></pre> <ol> <li>In the host, we have an IPtable rule in the PREROUTING chain that DNATs this packet matched on nodePort to its clusterIP:targetPort</li> </ol> <pre><code>[8:480] -A OVN-KUBE-NODEPORT -p tcp -m addrtype --dst-type LOCAL -m tcp --dport 31842 -j DNAT --to-destination 10.96.67.170:80\n</code></pre> <ol> <li>The service route in the host sends this packet back to breth0.</li> </ol> <pre><code>10.96.0.0/16 via 172.18.0.1 dev breth0 mtu 1400\n</code></pre> <ol> <li>On breth0, we have priority 500 flows meant to handle hairpining, that will SNAT the srcIP to the special <code>169.254.169.2</code> masqueradeIP and send it to <code>table2</code></li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=11, n_bytes=814, priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <ol> <li>In <code>table2</code> we have a flow that forwards this to patch port that takes the traffic in OVN:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=2, n_packets=11, n_bytes=814, actions=set_field:02:42:ac:12:00:03-&gt;eth_dst,output:\"patch-breth0_ov\"\n</code></pre> <ol> <li>Traffic enters the GR on the worker node and hits the load-balancer where we DNAT it correctly to the local backends.</li> </ol> <p>The GR load-balancer on a node with endpoints for the clusterIP will look like this:</p> <pre><code>_uuid               : b3201caf-3089-4462-b96e-1406fd7c4256\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"default/example-service-1\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_default/example-service-1_TCP_cluster\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.67.170:80\"=\"10.244.1.3:8080,10.244.2.3:8080\"}\n</code></pre> <p>Response traffic will follow the same path (backend-&gt;GR-&gt;breth0-&gt;host-&gt;breth0-&gt;eth0).</p> <ol> <li>Return traffic gets matched on the priority 500 flow in <code>table0</code> which sends it to <code>table3</code>.</li> </ol> <pre><code>cookie=0xdeff105, duration=3189.786s, table=0, n_packets=10, n_bytes=540, priority=500,ip,in_port=\"patch-breth0_ov\",nw_src=10.96.0.0/16,nw_dst=169.254.169.2 actions=ct(table=3,zone=64001,nat)\n</code></pre> <ol> <li>In <code>table3</code>, we send it to host:</li> </ol> <pre><code>cookie=0xdeff105, duration=6.308s, table=3, n_packets=10, n_bytes=540, actions=move:NXM_OF_ETH_DST[]-&gt;NXM_OF_ETH_SRC[],set_field:02:42:ac:12:00:03-&gt;eth_dst,LOCAL\n</code></pre> <ol> <li>From host we send it back to breth0 using:</li> </ol> <pre><code>cookie=0xdeff105, duration=5992.878s, table=0, n_packets=89312, n_bytes=6154654, idle_age=0, priority=100,ip,in_port=LOCAL actions=ct(commit,zone=64000,exec(load:0x2-&gt;NXM_NX_CT_MARK[])),output:eth0\n</code></pre> <p>where packet leaves the node and goes back to the external entity that initiated the connection.</p>"},{"location":"design/service-traffic-policy/#sources","title":"Sources","text":"<ul> <li>https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies</li> </ul>"},{"location":"design/service-traffic-policy/#internal-traffic-policy","title":"Internal Traffic Policy","text":"<p>Service Internal Traffic Policy (ITP) is a feature that can be enabled on a kubernetes service type object. This feature imposes restrictions on traffic that originates internally by routing it only to endpoints that are local to the node from where the traffic originated from. Here \"internal\" traffic means traffic originating from pods and nodes within the cluster. See https://kubernetes.io/docs/concepts/services-networking/service-traffic-policy/ for more details.</p> <pre><code>NOTE1: ITP is applicable only to service of type \"ClusterIP\" meaning it has no effect on services of type NodePorts, ExternalIPs or LoadBalancers. So if\nInternalTrafficPolicy=Local for services of types NodePorts, ExternalIPs or LoadBalancers, then the restirction of traffic policy will apply only to the\nclusterIP serviceVIP of these service types.\n\nNOTE2: Unlike ETP, it is not necessary that the srcIP be preserved in case of ITP.\n</code></pre> <p>By default ITP is of type <code>Cluster</code> on all services; meaning internal traffic will be routed to any of the endpoints for that service. When it is set to <code>Local</code>, only local endpoints are considered. The way this is implemented in OVN-K is by filtering out endpoints from the load balancer on the node switches for <code>ClusterIP</code> services .</p>"},{"location":"design/service-traffic-policy/#internaltrafficpolicylocal","title":"InternalTrafficPolicy=Local","text":"<p>This feature is implemented exactly in the same way for both the gateway modes.</p>"},{"location":"design/service-traffic-policy/#pod-traffic","title":"Pod Traffic","text":"<p>This section will cover the networking entities hit when traffic travels from an OVN pod via a service backed by either host networked pods or cluster networked pods.</p> <p>We have a service of type <code>NodePort</code> where <code>internalTrafficPolicy: Local</code>:</p> <pre><code>$ oc get svc\nNAMESPACE        NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE\ndefault          hello-world-2   NodePort    10.96.61.132   172.18.0.9    80:31358/TCP             108m\n$ oc get ep\nNAME            ENDPOINTS                         AGE\nhello-world-2   10.244.0.6:8080,10.244.1.3:8080   111m\n</code></pre> <p>This service is backed by two ovn pods: <pre><code>$ oc get pods -owide -n surya\nNAME                            READY   STATUS    RESTARTS   AGE    IP           NODE          NOMINATED NODE   READINESS GATES\nhello-world-2-5c87676b7-h6rm5   1/1     Running   0          111m   10.244.0.6   ovn-worker    &lt;none&gt;           &lt;none&gt;\nhello-world-2-5c87676b7-l82w8   1/1     Running   0          111m   10.244.1.3   ovn-worker2   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Traffic from pod hits the load balancer on the switch where the non-local endpoints for clusterIPs are filtered out. So traffic will be DNAT-ed to local endpoints if any. Here is how the load balancers will look like on the three KIND worker nodes for the above service.</p> <pre><code>a7865d9f-43d2-4cf9-a316-fc35e8c357a8    Service_surya/he    tcp        10.96.61.132:80        \n                                                            tcp        172.18.0.4:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\nd58c012a-7b1f-45ad-a817-a86845fde164    Service_surya/he    tcp        10.96.61.132:80        10.244.0.6:8080\n                                                            tcp        172.18.0.2:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\na993003a-a177-4673-8f7c-825e2c9bf205    Service_surya/he    tcp        10.96.61.132:80        10.244.1.3:8080\n                                                            tcp        172.18.0.3:31358       10.244.0.6:8080,10.244.1.3:8080\n                                                            tcp        172.18.0.9:80          10.244.0.6:8080,10.244.1.3:8080\n</code></pre> <p>Once the packet is DNAT-ed it is then redirected to the backend pod.</p> <p>NOTE: This traffic flow behaves exactly the same if the backend pods were host-networked.</p>"},{"location":"design/service-traffic-policy/#host-traffic_1","title":"Host Traffic","text":"<p>This section will cover the networking entities hit when traffic travels from a cluster host via a clusterIP service to either host networked pods or cluster networked pods. Note that host to clusterIP is considered \"internal\" traffic.</p>"},{"location":"design/service-traffic-policy/#host-service-clusterip-ovn-pod","title":"Host -&gt; Service (ClusterIP) -&gt; OVN Pod","text":"<ol> <li>Packet generated from the host towards clusterIP service <code>10.96.61.132:80</code> is marked for forwarding in the <code>mangle</code> table by an IP table rule called from the <code>OUTPUT</code> chain:</li> </ol> <pre><code>-A OUTPUT -j OVN-KUBE-ITP\n[1:60] -A OVN-KUBE-ITP -d 10.96.61.132/32 -p tcp -m tcp --dport 80 -j MARK --set-xmark 0x1745ec/0xffffffff\n</code></pre> <ol> <li>A routing policy (priority 30) is setup in the database to match on this mark and send it to custom routing table <code>number 7</code>:</li> </ol> <pre><code>root@ovn-worker:/# ip rule\n30: from all fwmark 0x1745ec lookup 7\n</code></pre> <ol> <li>In routing table <code>7</code> we have a route that steers this traffic into management port <code>ovn-k8s-mp0</code> instead of sending it via default service route towards <code>breth0</code>:</li> </ol> <pre><code>root@ovn-worker:/# ip r show table 7\n10.96.0.0/16 via 10.244.0.1 dev ovn-k8s-mp0 \n</code></pre> <ol> <li>Packet enters ovn via <code>k8s-nodename</code> port and hits the load balancer on the switch where the packet is DNAT-ed into the local endpoint if any, else rejected.</li> </ol> <p>Here is a sample load balancer on <code>ovn-worker</code> node which has an endpoint:</p> <pre><code>_uuid               : d58c012a-7b1f-45ad-a817-a86845fde164\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"surya/hello-world-2\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_surya/hello-world-2_TCP_node_switch_ovn-worker\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.61.132:80\"=\"10.244.0.6:8080\", \"172.18.0.2:31358\"=\"10.244.0.6:8080,10.244.1.3:8080\", \"172.18.0.9:80\"=\"10.244.0.6:8080,10.244.1.3:8080\"}\n</code></pre> <p>Note that only endpoints for <code>ClusterIP</code> are filtered, externalIPs/nodePorts are not.</p> <p>Here is a sample load balancer on <code>ovn-control-plane</code> node which does not have an endpoint:</p> <pre><code>_uuid               : a7865d9f-43d2-4cf9-a316-fc35e8c357a8\nexternal_ids        : {\"k8s.ovn.org/kind\"=Service, \"k8s.ovn.org/owner\"=\"surya/hello-world-2\"}\nhealth_check        : []\nip_port_mappings    : {}\nname                : \"Service_surya/hello-world-2_TCP_node_switch_ovn-control-plane\"\noptions             : {event=\"false\", reject=\"true\", skip_snat=\"false\"}\nprotocol            : tcp\nselection_fields    : []\nvips                : {\"10.96.61.132:80\"=\"\", \"172.18.0.4:31358\"=\"10.244.0.6:8080,10.244.1.3:8080\", \"172.18.0.9:80\"=\"10.244.0.6:8080,10.244.1.3:8080\"}\n</code></pre> <ol> <li>Packet is then delivered to backend pod.</li> </ol>"},{"location":"design/service-traffic-policy/#host-service-host-networked-pod","title":"Host -&gt; Service -&gt; Host Networked Pod","text":"<p>When the backend is a host networked pod we shortcircuit OVN to counter reverse path filtering issues and use iptables rules on the host to DNAT directly to the correct host endpoint.</p> <pre><code>[1:60] -A OVN-KUBE-ITP -d 10.96.48.132/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 8080\n</code></pre> <p>NOTE: If a service with ITP=local has both host-networked pods and ovn pods as local endpoints, traffic will always be delivered to the host-networked pod. This is acceptable since traffic policy claims unfair load balancing as a side effect of the feature.</p>"},{"location":"design/topology/","title":"OVN-Kubernetes Network Topology","text":"<p>Like we saw earlier in the architecture section there are two modes of deployment in OVN-Kubernetes:</p> <ul> <li>central mode (centralized control plane architecture) -- deprecated starting 1.2 release</li> <li>interconnect mode (distributed control plane architecture) -- default mode</li> </ul> <p>Based on the mode, there are subtle differences in network topology running on each node in the cluster</p>"},{"location":"design/topology/#ovn-kubernetes-network-topology-central-mode-deprecated","title":"OVN-Kubernetes Network Topology - Central Mode (DEPRECATED!!!)","text":"<p>The centralized architecture in OVN-K looks like this today:</p> <p>+</p> <p>On each node we have:</p> <ul> <li>node-local-switch: all the logical switch ports for the pods created on a node are bound to this switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>distributed-ovn-cluster-router: it's responsible for tunnelling overlay traffic between the nodes and also routing traffic between the node switches and gateway router's</li> <li>distributed-join-switch: connects the ovn-cluster-router to the gateway routers</li> <li>node-local-gateway-router: it's responsible for north-south traffic routing and connects the join switch to the external switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>node-local-external-switch: connects the gateway router to the external bridge</li> </ul>"},{"location":"design/topology/#ovn-kubernetes-network-topology-distributed-interconnect-default","title":"OVN-Kubernetes Network Topology - Distributed (Interconnect) (DEFAULT)","text":"<p>The interconnect architecture in OVN-K looks like this today (we assume each node is in a zone of their own):</p> <p>+</p> <p>On each node we have:</p> <ul> <li>node-local-switch: all the logical switch ports for the pods created on a node are bound to this switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>distributed-ovn-cluster-router: it's responsible for tunnelling overlay traffic between the nodes and also routing traffic between the node switches and gateway router's (note that if its one node per zone this behaves like a local router since there is no need for a distributed setup; if there are multiple nodes in the same zone, then it uses GENEVE tunnel for overlay traffic)</li> <li>distributed-join-switch: connects the ovn-cluster-router to the gateway routers (note that if its one node per zone this behaves like local switch since there is no need for a distributed setup; if there are multiple nodes in the same zone, then its distributed and connects cross more than one gateway router)</li> <li>node-local-gateway-router: it's responsible for north-south traffic routing and connects the join switch to the external switch and it also hosts load balancers that take care of DNAT-ing the service traffic</li> <li>node-local-external-switch: connects the gateway router to the external bridge</li> <li>transit-switch: This is the shiny new component coming in for IC. It is distributed across the nodes in the cluster and is responsible for routing traffic between the different zones.</li> </ul> <p>FIXME: This page is lazily written, there is so much more to do here.</p>"},{"location":"developer-guide/developer/","title":"Developer Documentation","text":"<p>This file aims to have information that is useful to the people contributing to this repo.</p>"},{"location":"developer-guide/developer/#generating-ovsdb-bindings-using-modelgen","title":"Generating ovsdb bindings using modelgen","text":"<p>In order to generate the latest NBDB and SBDB bindings, we have a tool called <code>modelgen</code> which lives in the libovsdb repo: https://github.com/ovn-kubernetes/libovsdb#modelgen. It is a code generator that uses <code>pkg/nbdb/gen.go</code> and <code>pkg/sbdb/gen.go</code> files to auto-generate the models and additional code like deep-copy methods.</p> <p>In order to use this tool do the following:</p> <pre><code>$ cd go-controller/\n$ make modelgen\ncurl -sSL https://raw.githubusercontent.com/ovn-org/ovn/${OVN_SCHEMA_VERSION}/ovn-nb.ovsschema -o pkg/nbdb/ovn-nb.ovsschema\ncurl -sSL https://raw.githubusercontent.com/ovn-org/ovn/${OVN_SCHEMA_VERSION}/ovn-sb.ovsschema -o pkg/sbdb/ovn-sb.ovsschema\nhack/update-modelgen.sh\n</code></pre> <p>If there are new bindings then you should see the changes being generated in the <code>pkg/nbdb</code> and <code>pkg/sbdb</code> parts of the repo. Include them and push a commit!</p> <p>NOTE1: You have to pay attention to the version of the commit hash used to download the modelgen client. While the client doesn't change too often it can also become outdated causing wrong generations. So keep in mind to re-install modelgen with latest commits and change the hash value in the <code>hack/update-modelgen.sh</code> file if you find it outdated.</p> <p>NOTE2: From time to time we always bump our fedora version of OVN used by KIND. But we oftentimes forget to update the <code>OVN_SCHEMA_VERSION</code> in our <code>Makefile</code> which is used to download the ovsdb schema. If that version seems to be outdated, probably best to update that as well and re-generate the schema bindings.</p>"},{"location":"developer-guide/developer/#generating-crd-yamls-using-codegen","title":"Generating CRD yamls using codegen","text":"<p>In order to generate the latest yaml files for a given CRD or to add a new CRD, once the <code>types.go</code> has been created according to sig-apimachinery docs, the developer can run <code>make codegen</code> to be able to generate all the clientgen, listers and informers for the new CRD along with the deep-copy methods and actual yaml files which get created in <code>_output/crd</code> folder and are copied over to <code>dist/templates</code> to then be used when creating a KIND cluster.</p>"},{"location":"developer-guide/developer/#level-driven-controllers","title":"Level-Driven Controllers","text":""},{"location":"developer-guide/developer/#background","title":"Background","text":"<p>OVN-Kubernetes has scale issues with network controllers today. We spin up 1 network controller per UDN, which incurs heavy cost when handling resource object events. The cost for example of unmarshaling a node annotation can be O(n) where n is the number of UDN controllers that are parsing the object.</p> <p>To fix this scale problem, the project has started to move to single controller instances that are aware of all UDNs. Therefore, handling and parsing of resource objects are done once. A few controllers have already moved in this direction, and the remaining components that are within a UDN controller (such as pod, node, namespace) will be migrated incrementally.</p> <p>Furthermore, as we move to per-resource, multi-network aware controllers, each controller type needs to be able to get network information. One obvious way to do this is to add a level-driven controller for NADs in each main controller. However, this too has a performance cost, because upon each NAD event, each controller will need to parse the NAD. Since we already have networkManager (NAD Controller), it is parsing and updating its cache with the NAD. In order to solve this problem, NetworkManager has been extended with a \"RegisterNADReconciler\" function, which is a callback that controllers can register with NAD Controller to be informed when a NAD event happens. Controllers can then query NAD Controller to access its cache as the source of truth.</p> <p>For example there is commonly used GetActiveNetworkForNamespace, and a new API, GetPrimaryNADForNamespace is added in #5623, as well as GetNetInfoForNADKey.</p> <p>Controllers should be using the pkg/controller Reconciler framework to implement their level-driven controllers, which are fed keys externally from NAD Controller.</p>"},{"location":"developer-guide/developer/#guidelines","title":"Guidelines","text":"<p>We prefer level-driven controllers built with <code>pkg/controller/controller.go</code>. When adding a new controller:</p> <ul> <li>Use the shared controller framework rather than bespoke loops or per-network controllers.</li> <li>Design controllers to be User Defined Network (UDN) aware: a single controller instance should reconcile objects across all networks instead of spinning up one instance per network.</li> <li>If the controller is network-aware, do not create a separate NAD Controller, use a Reconciler.</li> <li>The network manager is the source of truth for NADs; register a lightweight, non-blocking handler via <code>RegisterNADReconciler</code> that will queue keys to the Reconciler.</li> <li>RegisterNADReconciler  before starting controller workers to avoid missing events during startup.</li> <li>For a concrete example, see <code>go-controller/pkg/ovn/controller/egressfirewall/egressfirewall.go</code>.</li> </ul>"},{"location":"developer-guide/documentation/","title":"Documentation Guide","text":"<p>Documentation is an important step in the SDLC process. Unless there are proper docs to tell end users about the code you contributed; there will be less visibility and understanding of your code and feature. It will also become hard to maintain such code.</p>"},{"location":"developer-guide/documentation/#writing-documentation","title":"Writing Documentation","text":"<p>All OVN-Kubernetes docs are kept under the <code>docs/</code> folder in the main path. There are specific folders such as <code>features</code> and <code>developer-guide</code> where you can add your commit against. These simple <code>.md</code> files are referred from the navigation tile in <code>mkdocs.yaml</code> file which is what is used to build our website.</p> <p>As a developer and contributor to this project; it is recommended that you include a docs commit in your PR whenever possible and relevant. Some examples where we mandate docs include:</p> <ul> <li> <p>Enhancement Proposals: If you are planning to do a new feature, then start with an enhancement proposal a.k.a OKEP so that maintainers and other reviewers can get an understanding of what your goals are. See here for more details and open a commit adding it to our <code>docs/okeps</code> folder.</p> </li> <li> <p>Architecture and Topology docs: Did you change the architecture or topology? If so please write docs reflecting your design changes and update any existing docs so that they remain relevant. Open a commit adding it to our <code>docs/design</code> folder.</p> </li> <li> <p>Feature Docs: If your enhancement proposal has merged; next step is to implement that feature. As part of the main implementation PR we mandate adding a feature documentation commit. See here for how a feature documentation should be done. Open a commit adding it to our <code>docs/features</code> folder.</p> </li> <li> <p>Contributor Docs: If your changes include code cleanup or refactoring that is not user facing but internal; example: \"Adding DBIndexes to AddressSets\" write up a developer guide for contributors asking them to follow that new code structure process while adding new AddressSets. Open a commit adding it to our <code>docs/developer-guide</code> folder.</p> </li> <li> <p>Bug Fixes: If there was a difficult or critical bug fix that warrants a doc; please feel free to write it. If you are unsure where this should be placed; reach out to the maintainers.</p> </li> <li> <p>Blog Posts: Are you an end-user of OVN-Kubernetes? Is there something you wish to share with the community about your awesome use cases and how you used our CNI to solve your problems? We welcome blog post contributions from all! See here for details. Open a commit adding it to our <code>docs/blog</code> folder.</p> </li> <li> <p>Performance Enhancements: We love performance enhancements! Did you write a cool patch to reduce the time it takes for iptables to sync up on startup? Think about writing a good blog post around this! Open a commit adding it to our <code>docs/blog</code> folder.</p> </li> <li> <p>API Reference: Did you introduce a new CRD? OR Did you add a watcher a new CRD? Include API Reference documentation changes to <code>docs/api-reference</code> folder. See here for more details.</p> </li> </ul>"},{"location":"developer-guide/documentation/#website-guide","title":"Website Guide","text":"<p>We are utilizing GitHub Pages to host the ovn-kubernetes.io website. The website's content is composed in Markdown and stored within the 'docs' directory at the root of our repository. For static site generation, we employ the MkDocs framework, managed by the 'mkdocs.yml' configuration file located next to the 'docs' folder. Additionally, we use the Material framework on top of  MkDocs, which enhances our site with advanced features like customizable navigation, color schemes, and site search capabilities</p> <p>Any changes to the files in the doc folder or the mkdocs.yml file is picked up by a GitHub Action workflow that will automatically publish the changes to the website.</p>"},{"location":"developer-guide/documentation/#gemini-ai-summarizer-side-popup","title":"Gemini AI summarizer (side popup)","text":"<p>Next to each h1 and h2, a Gemini Sparkle icon appears. Clicking it sends the text under that heading to the Google Gemini API and shows a 3-bullet summary in a side drawer (400px, glassmorphism, from the right) with Copy and Close (\u00d7) buttons.</p>"},{"location":"developer-guide/documentation/#step-by-step-get-the-api-key-and-where-to-paste-it","title":"Step-by-step: Get the API key and where to paste it","text":"<ol> <li>Get a Gemini API key</li> <li>Open Google AI Studio in your browser.</li> <li>Sign in with your Google account.</li> <li>Click \"Get API key\" (or \"Create API key\").</li> <li>If asked, create a new project or pick an existing Google Cloud project.</li> <li> <p>Copy the API key that is shown (it looks like <code>AIzaSy...</code>).</p> </li> <li> <p>Paste the key in the docs</p> </li> <li>In this repo, open the file: <code>docs/javascripts/gemini-api-key.js</code>.</li> <li>Find the line: <code>window.OVN_GEMINI_API_KEY = \"\";</code></li> <li>Paste your key between the quotes, for example:      <pre><code>window.OVN_GEMINI_API_KEY = \"AIzaSy...your-key-here\";\n</code></pre></li> <li> <p>Save the file.</p> </li> <li> <p>Use the summarizer</p> </li> <li>Rebuild the docs (<code>mkdocs build</code>) or refresh if you use <code>mkdocs serve</code>.</li> <li>On any doc page, click the sparkle icon next to an h1 or h2 heading.</li> <li>The summary appears in the side drawer; use Copy or \u00d7 to close.</li> </ol> <p>Security: Do not commit a real API key to a public repo. Add <code>docs/javascripts/gemini-api-key.js</code> to <code>.gitignore</code> if the file contains a secret, or use a placeholder in the repo and set the key only in your local copy.</p> <p>The prompt sent to Gemini is: \"Summarize this technical OVN-Kubernetes documentation into 3 bullet points for a senior engineer.\"</p>"},{"location":"developer-guide/documentation/#smart-headings-data-summary","title":"Smart headings (data-summary)","text":"<p>To provide a custom summary for a heading (e.g. for tooltips or future use), add the <code>data-summary</code> attribute using the <code>attr_list</code> extension:</p> <pre><code>## Architecture {: data-summary=\"Overview of centralized vs distributed control plane and data plane components.\" }\n</code></pre> <p>You can also use emojis in headings (e.g. <code>## \ud83c\udf10 What is OVN-Kubernetes?</code>) for visual scanning; the theme supports this via the existing emoji extension.</p>"},{"location":"developer-guide/documentation/#how-to-test-your-documentation-changes","title":"How to test your documentation changes?","text":""},{"location":"developer-guide/documentation/#option-1-build-and-view-docs-with-a-pr","title":"Option 1) Build and view docs with a PR","text":"<p>Pushing docs changes to the ovn-kubernetes/ovn-kubernetes project as a pull request will run the job name \"Test and Deploy static content to Pages\" which has a step to save the docs artifacts. You can download those as a .zip file, extract and view them locally.</p>"},{"location":"developer-guide/documentation/#option-2-build-and-serve-docs-locally","title":"Option 2) Build and serve docs locally","text":"<p>In order to test changes locally to either mkdocs.yml or to files under docs/ folder, please follow the instructions below.</p>"},{"location":"developer-guide/documentation/#clone-the-repository","title":"Clone the repository","text":"<pre><code># git clone https://github.com/ovn-org/ovn-kubernetes\nCloning into 'ovn-kubernetes'...\nremote: Enumerating objects: 84258, done.\nremote: Counting objects: 100% (1546/1546), done.\nremote: Compressing objects: 100% (686/686), done.\nremote: Total 84258 (delta 809), reused 1208 (delta 663), pack-reused 82712\nReceiving objects: 100% (84258/84258), 56.39 MiB | 28.74 MiB/s, done.\nResolving deltas: 100% (55993/55993), done.\n\n# cd ovn-kubernetes\n</code></pre>"},{"location":"developer-guide/documentation/#create-a-python-virtual-environment","title":"Create a Python virtual environment","text":"<pre><code>#  python -m venv venv\n</code></pre>"},{"location":"developer-guide/documentation/#activate-the-virtual-environment","title":"Activate the virtual environment","text":"<pre><code># source venv/bin/activate\n</code></pre>"},{"location":"developer-guide/documentation/#install-all-the-required-python-packages-to-render-the-website","title":"Install all the required python packages to render the website","text":"<pre><code>(venv) # pip install -r requirements.txt \nCollecting Click\n  Using cached click-8.1.7-py3-none-any.whl (97 kB)\nCollecting htmlmin\n\n&lt;output snipped for brevity&gt;\n</code></pre>"},{"location":"developer-guide/documentation/#run-the-website-locally-using","title":"Run the website locally using","text":"<p><pre><code>(venv) # mkdocs serve\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\n\n&lt;output snipped for brevity&gt;\n\nINFO    -  Documentation built in 1.52 seconds\nINFO    -  [17:05:21] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [17:05:21] Serving on http://127.0.0.1:8000/ovn-kubernetes/\n</code></pre> Now you can browse the website on your browser using the above URL.</p> <p>As you make changes and save the files, the mkdocs server notices that and re-builds the website. It also spews out any WARNINGS or ERRORS with respect to the changes that you have just made. If the changes look good, then go ahead and submit the PR.</p>"},{"location":"developer-guide/image-build/","title":"OVN-Kubernetes Container Images","text":"<p>This file covers the container images available for OVN-Kubernetes and how to build them.</p>"},{"location":"developer-guide/image-build/#images-packages","title":"Images / Packages","text":"<p>There are Ubuntu and Fedora-based images available in GitHub's Registry. They are automatically generated upon merges via a workflow. Prior to release-1.0, they were called ovn-kube-f (for the Fedora-based image) and ovnkube-u (for the Ubuntu-based image). From release 1.0 and beyond, these have been renamed to ovn-kube-fedora and ovn-kube-ubuntu, respectively.</p> <p>Therefore, use the following images and tags to obtain these images:</p> <ul> <li>ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-fedora:master</li> <li> <p>ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-fedora:release-1.0</p> </li> <li> <p>ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu:master</p> </li> <li>ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu:release-1.0</li> </ul>"},{"location":"developer-guide/image-build/#building-images","title":"Building Images","text":"<p>To build images locally, use the following Makefile and their respective Dockerfiles from the dist/images folder in this repository.</p> <pre><code>$ cd dist/images\n$ make fedora-image\n$ make ubuntu-image\n</code></pre> <p>The build will create an image called ovn-kube-fedora:latest or ovn-kube-ubuntu:latest, which can be re-tagged. For example: <code>${OCI_BIN} tag ovn-kube-fedora:latest ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-fedora:master</code></p>"},{"location":"developer-guide/local_testing_guide/","title":"Running CI Locally","text":"<p>This section describes how to run CI tests on a local machine. This may be useful for expanding the CI test coverage or testing a private fix before creating a pull request.</p>"},{"location":"developer-guide/local_testing_guide/#download-and-build-kubernetes-components","title":"Download and Build Kubernetes Components","text":""},{"location":"developer-guide/local_testing_guide/#go-version","title":"Go Version","text":"<p>Older versions of Kubernetes do not build with newer versions of Go, specifically Kubernetes v1.16.4 doesn't build with Go version 1.13.x. If this is a version of Kubernetes that needs to be tested with, as a workaround, Go version 1.12.1 can be downloaded to a local directory and the $PATH variable updated only where kubernetes is being built.</p> <pre><code>$ go version\ngo version go1.13.8 linux/amd64\n\n$ mkdir -p /home/$USER/src/golang/go1-12-1/; cd /home/$USER/src/golang/go1-12-1/\n$ wget https://dl.google.com/go/go1.12.1.linux-amd64.tar.gz\n$ tar -xzf go1.12.1.linux-amd64.tar.gz\n$ PATH=/home/$USER/src/golang/go1-12-1/go/bin:$GOPATH/src/k8s.io/kubernetes/_output/local/bin/linux/amd64:$PATH\n</code></pre>"},{"location":"developer-guide/local_testing_guide/#download-and-build-kubernetes-components-e2e-tests-ginkgo-kubectl","title":"Download and Build Kubernetes Components (E2E Tests, ginkgo, kubectl):","text":"<p>Determine which version of Kubernetes is currently used in CI (See ovn-kubernetes/.github/workflows/test.yml) and set the environmental variable <code>K8S_VERSION</code> to the same value. Also make sure to export a GOPATH which points to your go directory with <code>export GOPATH=(...)</code>.</p> <pre><code>K8S_VERSION=v1.34.0\ngit clone --single-branch --branch $K8S_VERSION https://github.com/kubernetes/kubernetes.git $GOPATH/src/k8s.io/kubernetes/\npushd $GOPATH/src/k8s.io/kubernetes/\nmake WHAT=\"test/e2e/e2e.test vendor/github.com/onsi/ginkgo/ginkgo cmd/kubectl\"\nrm -rf .git\n\nsudo cp _output/local/go/bin/e2e.test /usr/local/bin/.\nsudo cp _output/local/go/bin/kubectl /usr/local/bin/kubectl-$K8S_VERSION\nsudo ln -s /usr/local/bin/kubectl-$K8S_VERSION /usr/local/bin/kubectl\ncp _output/local/go/bin/ginkgo /usr/local/bin/.\npopd\n</code></pre> <p>If you have any failures during the build, verify $PATH has been updated to point to correct GO version. Also may need to change settings on some of the generated binaries. For example:</p> <pre><code>chmod +x $GOPATH/src/k8s.io/kubernetes/_output/bin/deepcopy-gen\n</code></pre>"},{"location":"developer-guide/local_testing_guide/#export-environment-variables","title":"Export environment variables","text":"<p>Before setting up KIND and before running the actual tests, export essential environment variables.</p> <p>The environment variables and their values depend on the actual test scenario that you want to run.</p> <p>Look at the <code>e2e</code> action (search for <code>name: e2e</code>) in ovn-kubernetes/.github/workflows/test.yml. Prior to installing kind, set the following environment variables according to your needs: <pre><code>export KIND_CLUSTER_NAME=ovn\nexport KIND_INSTALL_INGRESS=[true|false]\nexport KIND_ALLOW_SYSTEM_WRITES=[true|false]\nexport PARALLEL=[true|false]\nexport JOB_NAME=(... job name ...)\nexport OVN_HYBRID_OVERLAY_ENABLE=[true|false]\nexport OVN_MULTICAST_ENABLE=[true|false]\nexport OVN_EMPTY_LB_EVENTS=[true|false]\nexport OVN_HA=[true|false]\nexport OVN_DISABLE_SNAT_MULTIPLE_GWS=[true|false]\nexport OVN_GATEWAY_MODE=[\"local\"|\"shared\"]\nexport PLATFORM_IPV4_SUPPORT=[true|false]\nexport PLATFORM_IPV6_SUPPORT=[true|false]\n# not required for the OVN Kind installation script, but export this already for later\nexport OVN_SECOND_BRIDGE=[true|false]\n</code></pre></p> <p>You can refer to a recent CI run from any pull request in https://github.com/ovn-org/ovn-kubernetes/actions to get a valid set of settings.</p> <p>As an example for the <code>control-plane-noHA-local-ipv4-snatGW-1br</code> job, the settings are at time of this writing: <pre><code>export KIND_CLUSTER_NAME=ovn\nexport KIND_INSTALL_INGRESS=true\nexport KIND_ALLOW_SYSTEM_WRITES=true\nexport PARALLEL=true\nexport JOB_NAME=control-plane-noHA-local-ipv4-snatGW-1br\nexport OVN_HYBRID_OVERLAY_ENABLE=true\nexport OVN_MULTICAST_ENABLE=true\nexport OVN_EMPTY_LB_EVENTS=true\nexport OVN_HA=false\nexport OVN_DISABLE_SNAT_MULTIPLE_GWS=false\nexport OVN_GATEWAY_MODE=\"local\"\nexport PLATFORM_IPV4_SUPPORT=true\nexport PLATFORM_IPV6_SUPPORT=false\n# not required for the OVN Kind installation script, but export this already for later\nexport OVN_SECOND_BRIDGE=false\n</code></pre></p>"},{"location":"developer-guide/local_testing_guide/#kind","title":"KIND","text":"<p>Kubernetes in Docker (KIND) is used to deploy Kubernetes locally where a docker (or podman) container is created per Kubernetes node. The CI tests run on this Kubernetes deployment. Therefore, KIND will need to be installed locally.</p> <p>Generic instructions for installing and running OVN-Kubernetes with KIND can be found at: OVN-Kubernetes KIND Setup</p> <p>Make sure to set the required environment variables first (see section above). Then, deploy kind: <pre><code>$ pushd contrib\n$ ./kind.sh\n$ popd\n</code></pre></p>"},{"location":"developer-guide/local_testing_guide/#run-tests","title":"Run Tests","text":"<p>To run the tests locally, run a KIND deployment as described above. The E2E tests look for the kube config file in a special location, so make a copy:</p> <pre><code>cp ~/ovn.conf ~/.kube/kind-config-kind\n</code></pre> <p>To run the desired shard, first make sure that the necessary environment variables are exported (see section above). Then, go to the location of your local copy of the <code>ovn-kubernetes</code> repository: <pre><code>$ REPO=$GOPATH/src/github.com/ovn-org/ovn-kubernetes\n$ cd $REPO\n</code></pre></p>"},{"location":"developer-guide/local_testing_guide/#running-a-suite-of-shards-or-control-plane-tests","title":"Running a suite of shards or control-plane tests","text":"<p>Finally, run the the shard that you want to test against (each shard can take 30+ minutes to complete) <pre><code>$ pushd test\n# run either\n$ make shard-network\n# or\n$ make shard-conformance\n# or\n$ GITHUB_WORKSPACE=\"$REPO\" make control-plane\n# or\n$ make conformance\n$ popd\n</code></pre></p>"},{"location":"developer-guide/local_testing_guide/#running-a-single-e2e-test","title":"Running a single E2E test","text":"<p>To run a single E2E test instead, target the shard-test action, as follows:</p> <pre><code>$ cd $REPO\n$ pushd test\n$ make shard-test WHAT=\"should enforce egress policy allowing traffic to a server in a different namespace based on PodSelector and NamespaceSelector\"\n$ popd\n</code></pre> <p>As a reminder, shards use the E2E framework. The value of <code>WHAT=</code> will be used to modify the <code>--focus</code> parameter. Individual tests can be retrieved from https://github.com/kubernetes/kubernetes/tree/master/test/e2e. For network tests, one could run: <pre><code>grep ginkgo.It $GOPATH/src/k8s.io/kubernetes/test/e2e/network/ -Ri\n</code></pre></p> <p>For example: <pre><code># grep ginkgo.It $GOPATH/src/k8s.io/kubernetes/test/e2e/network/ -Ri | head -1\n/root/go/src/k8s.io/kubernetes/test/e2e/network/conntrack.go:   ginkgo.It(\"should be able to preserve UDP traffic when server pod cycles for a NodePort service\", func() {\n# make shard-test WHAT=\"should enforce policy to allow traffic from pods within server namespace based on PodSelector\"\n(...)\n+ case \"$SHARD\" in\n++ echo should be able to preserve UDP traffic when server pod cycles for a NodePort service\n++ sed 's/ /\\\\s/g'\n+ FOCUS='should\\sbe\\sable\\sto\\spreserve\\sUDP\\straffic\\swhen\\sserver\\spod\\scycles\\sfor\\sa\\sNodePort\\sservice'\n+ export KUBERNETES_CONFORMANCE_TEST=y\n+ KUBERNETES_CONFORMANCE_TEST=y\n+ export KUBE_CONTAINER_RUNTIME=remote\n+ KUBE_CONTAINER_RUNTIME=remote\n+ export KUBE_CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\n+ KUBE_CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock\n+ export KUBE_CONTAINER_RUNTIME_NAME=containerd\n+ KUBE_CONTAINER_RUNTIME_NAME=containerd\n+ export FLAKE_ATTEMPTS=5\n+ FLAKE_ATTEMPTS=5\n+ export NUM_NODES=20\n+ NUM_NODES=20\n+ export NUM_WORKER_NODES=3\n+ NUM_WORKER_NODES=3\n+ ginkgo --nodes=20 '--focus=should\\sbe\\sable\\sto\\spreserve\\sUDP\\straffic\\swhen\\sserver\\spod\\scycles\\sfor\\sa\\sNodePort\\sservice' '--skip=Networking\\sIPerf\\sIPv[46]|\\[Feature:PerformanceDNS\\]|Disruptive|DisruptionController|\\[sig-apps\\]\\sCronJob|\\[sig-storage\\]|\\[Feature:Federation\\]|should\\shave\\sipv4\\sand\\sipv6\\sinternal\\snode\\sip|should\\shave\\sipv4\\sand\\sipv6\\snode\\spodCIDRs|kube-proxy|should\\sset\\sTCP\\sCLOSE_WAIT\\stimeout|should\\shave\\ssession\\saffinity\\stimeout\\swork|named\\sport.+\\[Feature:NetworkPolicy\\]|\\[Feature:SCTP\\]|service.kubernetes.io/headless|should\\sresolve\\sconnection\\sreset\\sissue\\s#74839|sig-api-machinery|\\[Feature:NoSNAT\\]|Services.+(ESIPP|cleanup\\sfinalizer)|configMap\\snameserver|ClusterDns\\s\\[Feature:Example\\]|should\\sset\\sdefault\\svalue\\son\\snew\\sIngressClass|should\\sprevent\\sIngress\\screation\\sif\\smore\\sthan\\s1\\sIngressClass\\smarked\\sas\\sdefault|\\[Feature:Networking-IPv6\\]|\\[Feature:.*DualStack.*\\]' --flake-attempts=5 /usr/local/bin/e2e.test -- --kubeconfig=/root/ovn.conf --provider=local --dump-logs-on-failure=false --report-dir=/root/ovn-kubernetes/test/_artifacts --disable-log-dump=true --num-nodes=3\nRunning Suite: Kubernetes e2e suite\n(...)\n\u2022 [SLOW TEST:28.091 seconds]\n[sig-network] Conntrack\n/root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23\n  should be able to preserve UDP traffic when server pod cycles for a NodePort service\n  /root/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/conntrack.go:128\n------------------------------\n{\"msg\":\"PASSED [sig-network] Conntrack should be able to preserve UDP traffic when server pod cycles for a NodePort service\",\"total\":-1,\"completed\":1,\"skipped\":229,\"failed\":0}\nAug 17 14:46:42.842: INFO: Running AfterSuite actions on all nodes\n\n\nAug 17 14:46:15.264: INFO: Running AfterSuite actions on all nodes\nAug 17 14:46:42.885: INFO: Running AfterSuite actions on node 1\nAug 17 14:46:42.885: INFO: Skipping dumping logs from cluster\n\n\nRan 1 of 5667 Specs in 30.921 seconds\nSUCCESS! -- 1 Passed | 0 Failed | 0 Flaked | 0 Pending | 5666 Skipped\n\n\nGinkgo ran 1 suite in 38.489055861s\nTest Suite Passed\n</code></pre></p>"},{"location":"developer-guide/local_testing_guide/#running-a-control-plane-test","title":"Running a control-plane test","text":"<p>All local tests are defined as <code>control-plane</code> tests. To run a single <code>control-plane</code> test, target the <code>control-plane</code> action and append the <code>WHAT=&lt;test name&gt;</code> parameter, as follows:</p> <pre><code>$ cd $REPO\n$ pushd test\n$ make control-plane WHAT=\"should be able to send multicast UDP traffic between nodes\"\n$ popd\n</code></pre> <p>The value of <code>WHAT=</code> will be used to modify the <code>-ginkgo.focus</code> parameter. Individual tests can be retrieved from this repository under test/e2e. To see a list of individual tests, one could run: <pre><code>grep -R ginkgo.It test/\n</code></pre></p> <p>For example: <pre><code># grep -R ginkgo.It . | head -1\n./e2e/multicast.go: ginkgo.It(\"should be able to send multicast UDP traffic between nodes\", func() {\n# make control-plane WHAT=\"should be able to send multicast UDP traffic between nodes\"\n(...)\n+ go test -timeout=0 -v . -ginkgo.v -ginkgo.focus 'should\\sbe\\sable\\sto\\ssend\\smulticast\\sUDP\\straffic\\sbetween\\snodes' -ginkgo.flakeAttempts 2 '-ginkgo.skip=recovering from deleting db files while maintain connectivity|Should validate connectivity before and after deleting all the db-pods at once in HA mode|Should be allowed to node local cluster-networked endpoints by nodeport services with externalTrafficPolicy=local|e2e ingress to host-networked pods traffic validation|host to host-networked pods traffic validation' -provider skeleton -kubeconfig /root/ovn.conf --num-nodes=2 --report-dir=/root/ovn-kubernetes/test/_artifacts --report-prefix=control-plane_\nI0817 15:26:21.762483 1197731 test_context.go:457] Tolerating taints \"node-role.kubernetes.io/control-plane\" when considering if nodes are ready\n=== RUN   TestE2e\nI0817 15:26:21.762635 1197731 e2e_suite_test.go:67] Saving reports to /root/ovn-kubernetes/test/_artifacts\nRunning Suite: E2e Suite\n(...)\n\u2022 [SLOW TEST:12.332 seconds]\nMulticast\n/root/ovn-kubernetes/test/e2e/multicast.go:25\n  should be able to send multicast UDP traffic between nodes\n  /root/ovn-kubernetes/test/e2e/multicast.go:75\n------------------------------\nSSSSSSSSSSSS\nJUnit report was created: /root/ovn-kubernetes/test/_artifacts/junit_control-plane_01.xml\n\nRan 1 of 60 Specs in 12.333 seconds\nSUCCESS! -- 1 Passed | 0 Failed | 0 Flaked | 0 Pending | 59 Skipped\n--- PASS: TestE2e (12.34s)\nPASS\nok      github.com/ovn-org/ovn-kubernetes/test/e2e  12.371s\n+ popd\n~/ovn-kubernetes/test\n</code></pre></p>"},{"location":"developer-guide/mocks-ut-faq/","title":"Mocks ut faq","text":""},{"location":"developer-guide/mocks-ut-faq/#how-are-the-mock-files-for-unit-tests-organized","title":"How are the mock files for unit tests organized?","text":"<ul> <li> <p>The name of the mock file generated will be the same as the name of the <code>interface</code> definition.</p> </li> <li> <p>Mock files for <code>interfaces</code> defined in the <code>go-controller/vendor</code> directories are located in the  <code>go-controller/pkg/testing/mocks</code> directory. The directory structure in the <code>go-controller/pkg/testing/mocks/</code> closely  mimic the directory structure of  <code>go-controller/vendor/</code>.</p> <p>e.g; a) The <code>Cmd</code> interface defined in the <code>go-controller/vendor/k8s.io/utils/exec.go</code> file has its mock generated  in <code>go-controller/pkg/testing/mocks/k8s.io/utils/exec/Cmd.go</code> file</p> <p>e.g; b) The <code>Link</code> interface defined in the <code>go-controller/vendor/github.com/vishvananda/netlink/link.go</code> file has  its mock generated in <code>go-controller/pkg/testing/mocks/vishvananda/netlink/Link.go</code> file</p> </li> <li> <p>Mock files for <code>interfaces</code> defined in the non vendor directories of the project are located in the <code>mocks</code> directory  of the same package as where the interface is defined.</p> <p>e.g; a) The <code>ExecRunner</code> interface defined in <code>go-controller/pkg/util/ovs.go</code> file has the its mock generated in  <code>go-controller/pkg/util/mocks/ExecRunner.go</code> file.</p> <p>e.g; b) The <code>SriovNetLibOps</code> interface defined in <code>go-controller/pkg/cni/helper_linux.go</code> file has its mock  generated in <code>go-controller/pkg/cni/mocks/SriovNetLibOps.go</code> file.</p> </li> </ul>"},{"location":"developer-guide/mocks-ut-faq/#how-are-the-mocks-for-interfaces-to-be-consumed-by-unit-tests-currently-generated","title":"How are the mocks for interfaces to be consumed by unit tests currently generated?","text":"<ul> <li> <p>The vektra/mockery package from https://github.com/vektra/mockery is leveraged to auto generate mocks for defined interfaces.</p> </li> <li> <p>Mocks for interfaces can be generated using vektra/mockery in one of two ways:</p> <ul> <li> <p>Using the binaries at https://github.com/vektra/mockery/releases</p> </li> <li> <p>Using the docker image</p> </li> </ul> </li> <li> <p>Sample commands to generate mocks when using the <code>binary</code> installed on a linux host.</p> <ul> <li> <p>Mock for interface <code>SriovNetLibOps</code> defined in the <code>go-controller/pkg/cni/helper_linux.go</code> file when executing the <code>mockery</code> command from dir <code>go-controller/</code></p> <p><code>mockery --name SriovnetLibOps --dir pkg/cni/ --output pkg/cni/mocks</code></p> </li> <li> <p>Mock for all interfaces defined in the vendor folder <code>go-controller/vendor/k8s.io/utils/exec</code> when executing the <code>mockery</code> command from dir <code>go-controller/</code></p> <p><code>mockery --all --dir vendor/k8s.io/utils/exec --output pkg/testing/mocks/k8s.io/utils/exec</code></p> </li> </ul> </li> <li> <p>Sample command to generate mocks when using the <code>docker</code> image</p> <ul> <li> <p>Mock for interface <code>SriovNetLibOps</code> defined in the <code>go-controller/pkg/cni/helper_linux.go</code> file when running the <code>docker</code> container from dir <code>go-controller/</code></p> <p><code>docker run -v $PWD:/src -w /src vektra/mockery --name SriovNetLibOps --dir pkg/cni/ --output pkg/cni/mocks</code></p> </li> <li> <p>Mock for all interfaces defined in the vendor folder <code>go-controller/vendor/k8s.io/utils/exec</code> when running the <code>docker</code> container from dir <code>go-controller/</code></p> <p><code>docker run -v $PWD:/src -w /src vektra/mockery --all --dir vendor/k8s.io/utils/exec --output pkg/testing/mocks/k8s.io/utils/exec</code></p> </li> </ul> </li> </ul>"},{"location":"developer-guide/mocks-ut-faq/#how-to-regenerate-all-existing-mocks-when-interfaces-locally-defined-or-in-vendor-libraries-are-updated","title":"How to regenerate all existing mocks when interfaces (locally defined or in vendor libraries) are updated?","text":"<pre><code>- Execute the ```make mocksgen``` in situations where all existing mocks have to be regenerated.\nNOTE: It would take a while(approx 20+ minutes) for all mocks to be regenerated.\n</code></pre>"},{"location":"developer-guide/mocks-ut-faq/#reference-links-that-explain-how-to-use-mocks-with-testify","title":"Reference links that explain how to use mocks with testify","text":"<ul> <li> <p>https://tutorialedge.net/golang/improving-your-tests-with-testify-go/ </p> </li> <li> <p>https://techblog.fexcofts.com/2019/09/23/go-and-test-mocking/ </p> </li> <li> <p>https://gowalker.org/github.com/stretchr/testify/mock </p> </li> <li> <p>https://ncona.com/2020/02/using-testify-for-golang-tests/ </p> </li> </ul>"},{"location":"developer-guide/release/","title":"Release Documentation","text":""},{"location":"developer-guide/release/#overview","title":"Overview","text":"<p>Each new release of OVN-Kubernetes is defined with a \"version\" that represents the Git tag of a release, such as v1.0.0. This contains the following:</p> <ul> <li>Source Code</li> <li>Binaries<ul> <li><code>ovnkube</code>: which is our main single all-in-one binary executable used to launch the ovnkube control plane and data plane pods in a kubernetes deployment</li> <li><code>ovn-k8s-cni-overlay</code>: is the cni executable to be placed in /opt/cni/bin (or another directory in which kubernetes will look for the plugin) so that it can be invoked for each pod event by kubernetes</li> <li><code>hybrid-overlay-node</code></li> <li><code>ovn-kube-util</code>: contains the Utils for ovn-kubernetes</li> <li><code>ovndbchecker</code></li> <li><code>ovnkube-trace</code>: is the binary that contains ovnkube-trace which is an abstraction used to invoke OVN/OVS packet tracing utils</li> <li><code>ovnkube-identity</code>: is the executable that is invoked to run ovn-kubernetes identity manager, which includes the admission webhook and the CertificateSigningRequest approver</li> </ul> </li> <li><code>ovnkube</code> API configuration </li> <li>scripts used to deploy OVN-Kubernetes including helm charts</li> <li>Images for fedora and ubuntu</li> </ul>"},{"location":"developer-guide/release/#release-planning","title":"Release Planning","text":"<ul> <li>OVN-Kubernetes projects uses milestones to track our release planning</li> <li>All PRs and Issues must be tagged with the correct milestone so that it get's included in the release planning</li> <li>Please check our roadmap for more details on our release tracking process</li> </ul>"},{"location":"developer-guide/release/#release-cadence","title":"Release Cadence","text":"<ul> <li>We will do three major releases each year. Ex: 1.x.0 in April 2026 and 1.y.0 in August 2026 and 1.z.0 in December 2026</li> <li>The release timings have been fixed roughly to be 3 or so months after a major Kubernetes release (allowing time for that to stabilize and for us to consume that kube release before we cut our own release)</li> <li>At a given time we will maintain only two active major releases   (So when 1.2.0 is released we will stop maintaining and backporting fixes into 1.0.0)</li> <li>For a supported major release we will continue to do backports for backfixes and offer   support. A minor patch release will be done at a cadence of 4 weeks for every major release.   Ex. Once 1.0.0 is release, we will do 1.0.1, 1.0.2, 1.0.3 etc. This also depends on a case-by-case   basis based on demands from end-users and backport statuses.</li> </ul>"},{"location":"developer-guide/release/#release-process","title":"Release Process","text":"<ul> <li>You can find our current releases here.</li> <li>Every major release cut will be preceded by an alpha prerelease and beta prerelease.</li> <li>See sample release PR which   will become the head commit for a given release.</li> <li>Branch will be cut on the day of release once the release PR merges.</li> <li>CI-CD for the release branch will be added to the GitHub Workflow.</li> </ul>"},{"location":"developer-guide/release/#backport-request","title":"BackPort Request","text":"<ul> <li>If a PR needs to be backported to an older release that should be requested   by adding the <code>needs-backport</code> label.</li> <li>Reach out to the maintainers on slack or by tagging them directly on the PR or come to the community meetings to discuss this.</li> </ul>"},{"location":"developer-guide/release/#information-on-past-releases","title":"Information on Past Releases","text":"<ul> <li>v1.0.0 - release-notes - not maintained anymore.</li> <li>v1.1.0 - release-notes - actively maintained</li> <li>v1.2.0 - release-notes - actively maintained</li> </ul>"},{"location":"features/hybrid-overlay/","title":"Hybrid Overlay","text":""},{"location":"features/hybrid-overlay/#introduction","title":"Introduction","text":"<p>The hybrid overlay feature creates VXLAN tunnels to nodes in the cluster that have been excluded from the ovn-kubernetes overlay using the <code>no-hostsubnet-nodes</code> config option.</p> <p>These tunnels allow pods on ovn-kubernetes nodes to communicate directly with other pods on nodes that do not run ovn-kubernetes.</p>"},{"location":"features/hybrid-overlay/#requirements","title":"Requirements","text":"<p>The feature can be enabled at runtime, but requires that the VXLAN UDP port (4789) be accessible on every node in the cluster. The cluster administrator is responsible for ensuring this port is open on all nodes.</p> <p>Hybrid overlay uses the third IP address on every node's logical switch as the gateway for traffic to hybrid overlay nodes. If the feature is enabled after the cluster has been installed, and a pod on the node is using the address, that pod will no longer work correctly until it has been killed and restarted. This is not handled automatically.</p> <p>It is recommended the hybrid overlay feature be enabled at cluster install time.</p>"},{"location":"features/live-migration/","title":"Live Migration","text":""},{"location":"features/live-migration/#introduction","title":"Introduction","text":"<p>The Live Migration feature allows kubevirt virtual machines to be live migrated while keeping the established TCP connections alive, and preserving the VM IP configuration. These two requirements provide seamless live-migration of a KubeVirt VM using OVN-Kubernetes cluster default network.</p>"},{"location":"features/live-migration/#motivation","title":"Motivation","text":"<p>kubevirt integration requires the ability to have live-migration functionality across the network with OVN-Kubernetes implemented at the pod's default network.</p>"},{"location":"features/live-migration/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>I, a kubevirt user, need to migrate one virtual machine with bridge binding without affecting different ones and have minimal disruption.</p>"},{"location":"features/live-migration/#non-goals","title":"Non-Goals","text":"<p>This feature is not implemented at secondary networks, only primary networks will be live migratable.</p>"},{"location":"features/live-migration/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>This feature is always enabled, it get triggered when a VM is annotated with  <code>kubevirt.io/allow-pod-bridge-network-live-migration: \"\"</code> and use bridge binding. </p>"},{"location":"features/live-migration/#workflow-description","title":"Workflow Description","text":""},{"location":"features/live-migration/#requirements","title":"Requirements","text":"<ul> <li>KubeVirt &gt;= v1.0.0</li> <li>DHCP aware guest image (fedora family is well tested, https://quay.io/organization/containerdisks)</li> </ul>"},{"location":"features/live-migration/#example-live-migrating-a-fedora-guest-image","title":"Example: live migrating a fedora guest image","text":"<p>Install KubeVirt following the guide here</p> <p>Create a fedora virtual machine with the annotations <code>kubevirt.io/allow-pod-bridge-network-live-migration</code>: <pre><code>cat &lt;&lt;'EOF' | kubectl create -f -\napiVersion: kubevirt.io/v1\nkind: VirtualMachine\nmetadata:\n  name: fedora\nspec:\n  runStrategy: Always\n  template:\n    metadata:\n      annotations:\n        # Allow KubeVirt VMs with bridge binding to be migratable\n        # also ovn-k will not configure network at pod, delegate it to DHCP\n        kubevirt.io/allow-pod-bridge-network-live-migration: \"\"\n    spec:\n      domain:\n        devices:\n          disks:\n          - disk:\n              bus: virtio\n            name: containerdisk\n          - disk:\n              bus: virtio\n            name: cloudinit\n          rng: {}\n        features:\n          acpi: {}\n          smm:\n            enabled: true\n        firmware:\n          bootloader:\n            efi:\n              secureBoot: true\n        resources:\n          requests:\n            memory: 1Gi\n      terminationGracePeriodSeconds: 180\n      volumes:\n      - containerDisk:\n          image: quay.io/containerdisks/fedora:38\n        name: containerdisk\n      - cloudInitNoCloud:\n          networkData: |\n            version: 2\n            ethernets:\n              eth0:\n                dhcp4: true\n          userData: |-\n            #cloud-config\n            # The default username is: fedora\n            password: fedora\n            chpasswd: { expire: False }\n        name: cloudinit\nEOF\n</code></pre></p> <p>After waiting for the VM to be ready - <code>kubectl wait vmi fedora --for=condition=Ready --timeout=5m</code> - the VM status should be as shown below - i.e. <code>kubectl get vmi fedora</code>:</p> <pre><code>kubectl get vmi fedora\nNAME     AGE     PHASE     IP            NODENAME      READY\nfedora   9m42s   Running   10.244.2.26   ovn-worker3   True\n</code></pre> <p>Login and check that the VM has receive a proper address <code>virtctl console fedora</code> <pre><code>[fedora@fedora ~]ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000\n    link/ether 0a:58:0a:f4:02:1a brd ff:ff:ff:ff:ff:ff\n    altname enp1s0\n    inet 10.244.2.26/24 brd 10.244.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 3412sec preferred_lft 3412sec\n    inet6 fe80::32d2:10d4:f5ed:3064/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n</code></pre></p> <p>Also we can check the neighbours cache to verify it later on <pre><code>[fedora@fedora ~]arp -a\n_gateway (10.244.2.1) at 0a:58:a9:fe:01:01 [ether] on eth0\n</code></pre></p> <p>The default gateway is the pod network subnet gateway IP. The ARP proxy feature ensures the gateway MAC address remains consistent across live migrations.</p> <pre><code>[fedora@fedora ~]ip route\ndefault via 10.244.2.1 dev eth0 proto dhcp src 10.244.2.26 metric 100\n10.244.2.0/24 dev eth0 proto kernel scope link src 10.244.2.26 metric 100\n</code></pre> <p>Then a live migration can be initialized with <code>virtctl migrate fedora</code> and wait at the vmim resource <pre><code>virtctl migrate fedora\nVM fedora was scheduled to migrate\nkubectl get vmim -A -o yaml\n  status:\n    migrationState:\n      completed: true\n</code></pre></p> <p>After migration, the network configuration is the same - including the GW neighbor cache. <pre><code>oc get vmi -A\nNAMESPACE   NAME     AGE   PHASE     IP            NODENAME     READY\ndefault     fedora   16m   Running   10.244.2.26   ovn-worker   True\n[fedora@fedora ~]ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc fq_codel state UP group default qlen 1000\n    link/ether 0a:58:0a:f4:02:1a brd ff:ff:ff:ff:ff:ff\n    altname enp1s0\n    inet 10.244.2.26/24 brd 10.244.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 2397sec preferred_lft 2397sec\n    inet6 fe80::32d2:10d4:f5ed:3064/64 scope link noprefixroute\n       valid_lft forever preferred_lft forever\n[fedora@fedora ~]arp -a\n_gateway (10.244.2.1) at 0a:58:a9:fe:01:01 [ether] on eth0\n</code></pre></p>"},{"location":"features/live-migration/#configuring-dns-server","title":"Configuring dns server","text":"<p>By default the DHCP server at ovn-kuberntes will configure the kubernetes default dns service <code>kube-system/kube-dns</code> as the name server. This can be overriden with the following command line options: - dns-service-namespace - dns-service-name</p>"},{"location":"features/live-migration/#configuring-dual-stack-guest-images","title":"Configuring dual stack guest images","text":"<p>For dual stack, ovn-kubernetes is configuring the IPv6 address to guest VMs using DHCPv6, but the IPv6 default gateway has to be configured manually. Since this address is the same - <code>fe80::1</code> - the virtual machine configuration is stable. </p> <p>Both the ipv4 and ipv6 configurations have to be activated.</p> <p>NOTE: The IPv6 autoconf/SLAAC is not supported at ovn-k live-migration</p> <p>For fedora cloud-init can be used to activate dual stack: <pre><code> - cloudInitNoCloud:\n     networkData: |\n       version: 2\n       ethernets:\n         eth0:\n           dhcp4: true\n           dhcp6: true\n     userData: |-\n       #cloud-config\n       # The default username is: fedora\n       password: fedora\n       chpasswd: { expire: False }\n       runcmd:\n         - nmcli c m \"cloud-init eth0\" ipv6.method dhcp\n         - nmcli c m \"cloud-init eth0\" +ipv6.routes \"fe80::1\"\n         - nmcli c m \"cloud-init eth0\" +ipv6.routes \"::/0 fe80::1\"\n         - nmcli c reload \"cloud-init eth0\"\n</code></pre></p> <p>For fedora coreos this can be configured with using the following ignition yaml:</p> <pre><code>variant: fcos\nversion: 1.4.0\nstorage:\n  files:\n    - path: /etc/nmstate/001-dual-stack-dhcp.yml\n      contents:\n        inline: |\n          interfaces:\n          - name: enp1s0\n            type: ethernet\n            state: up\n            ipv4:\n              enabled: true\n              dhcp: true\n            ipv6:\n              enabled: true\n              dhcp: true\n              autoconf: false\n    - path: /etc/nmstate/002-dual-sack-ipv6-gw.yml\n      contents:\n        inline: |\n          routes:\n            config:\n            - destination: ::/0\n              next-hop-interface: enp1s0\n              next-hop-address: fe80::1\n</code></pre>"},{"location":"features/live-migration/#implementation-details","title":"Implementation Details","text":""},{"location":"features/live-migration/#kubevirt-introduction","title":"Kubevirt Introduction","text":"<p>At kubevirt every VM is executed inside a \"virt-launcher\" pod; how the \"virt-launcher\" pod net interface is \"bound\" to the VM is specified by the network binding at the VM spec, ovn-kubernetes support live migration of the KubeVirt network bridge binding. </p> <p>The KubeVirt bridge binding adds  a bridge and a tap device and connects the existing pod interface as a port of the aforementioned bridge. When IPAM is configured on the pod interface, a DHCP server is started to advertise the pod's IP to DHCP aware VMs.</p> <p>KubeVirt doesn't replace anything. It adds a bridge and a tap device, and connects the existing pod interface as a port of the aforementioned bridge.</p> <p>Benefit of the bridge binding is that is able to expose the pod IP to the VM as expected by most users also pod network bridge binding live migration is implemented by ovn-kubernetes by doing the following: - Do IP assignemnt with ovn-k but skip the CNI part that configure it at pod's netns veth. - Do not expect the IP to be on the pod netns. - Add the ability at ovn-k controllers to migrate pod's IP from node to node.</p>"},{"location":"features/live-migration/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p>To implement live migration ovn-kubernetes do the following: - Send DHCP replies advertising the allocated IP address and subnet gateway to the guest VM (via OVN-Kubernetes DHCP options configured for the logical switch ports). - A point to point routing is used so one node's subnet IP can be routed from different node - The VM's gateway IP (subnet gateway) and MAC are kept consistent across nodes using ARP proxy</p> <p>Point to point routing:</p> <p>When the VM is running at a node that does not \"own\" it's IP address - e.g. after a live migration -  after a live migration, do point to point routing with a policy for non  interconnect and a cluster wide static route for interconnected zones, it will route outbound traffic and a static route to route inboud. By doing this, the VM can live migrate to different node and keep previous  addresses (IP / MAC), thus preserving n/s and e/w communication, and ensuring traffic goes over the node where the VM is running. The latter reduces inter node communication.</p> <p>If the VM is going back to the node that \"owns\" the ip, those static routes and  policies should be reconciled (deleted).</p> <pre><code>       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 static route (ingress all)\u2502\u2500\u2500\u2500\u2502 policy  (egress n/s)       \u2502\n\u2502 prefix: 10.244.0.8        \u2502   \u2502 match: ip4.src==10.244.0.8 \u2502\n\u2502 nexthop: 10.244.0.8       \u2502   \u2502 action: reroute            \u2502\n\u2502 output-port: rtos-node1   \u2502   \u2502 nexthop: 10.64.0.2         \u2502\n\u2502 policy: dst-ip            \u2502   \u2502                            \u2502  \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n- static route: ingress traffic to VM's IP via VM's node port\n- policy: egress n/s traffic from VM's IP via gateway router ip\n</code></pre> <p>When ovn-kubernetes is deployed with multiple ovn zones interconected  an extra static route is added to the local zone (zone where vm is running) to route egress n/s traffic to the gw router. Note that for interconnect the previous policy is not needed, at remote zones a static route is address to  enroute VM egress traffic over the transit switch port address.</p> <p>If the transit switch port address is 10.64.0.3 and gw router is 10.64.0.2,  those below are the static route added to the topology when running with  multiple zones:</p> <pre><code>Local zone:\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 static route (egress n/s) \u2502\u2500\u2500\u2500\u2502 static route (ingress all)\u2502\n\u2502 prefix: 10.244.0.0/16     \u2502   \u2502 prefix: 10.244.0.8        \u2502\n\u2502 nexthop: 10.64.0.2        \u2502   \u2502 nexthop: 10.244.0.8       \u2502\n\u2502 policy: src-ip            \u2502   \u2502 output-port: rtos-node1   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502 policy: dst-ip            \u2502\n                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nRemote zone:\n       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502     ovn_cluster_router    \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502 static route (ingress all)\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 prefix: 10.244.0.8        \u2502\n\u2502 nexthop: 10.64.0.3        \u2502\n\u2502 policy: dst-ip            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Nodes logical switch ports:</p> <p>To have a consistent gateway at VMs (keep ip and mac after live migration) the \"arp_proxy\" feature is used and it need to be activated at the logical switch port of type router connects node's logical switch to ovn_cluster_router logical router.</p> <p>The \"arp_proxy\" LSP option will include the MAC to answer ARPs with, and the link local ipv4 and ipv6 addresses, as well as the cluster wide pod CIDR. This allows the proxy to answer ARP requests for the subnet gateway IP when the node switch does not have the live migrated IP. The flows from arp_proxy have less priority than the ones from the node logical switch so ARP flows are not overriden.</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502logical switch node1\u2502   \u2502logical switch node2\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 lsp stor-node1     \u2502\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2502 lsp stor-node2      \u2502\n\u2502 options:           \u2502            \u2502 options:            \u2502\n\u2502  arp_proxy:        \u2502            \u2502   arp_proxy:        \u2502\n\u2502   0a:58:0a:f3:00:00\u2502            \u2502    0a:58:0a:f3:00:00\u2502\n\u2502   169.254.1.1      \u2502            \u2502    169.254.1.1      \u2502\n\u2502   10.244.0.0/16    \u2502            \u2502    10.244.0.0/16    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>VMs logical switch ports:</p> <p>The logical switch port for new VMs will use ovn-k ipam to reserve an IP address, and when live-migrating the VM, the LSP address will be re-used.</p> <p>CNI must avoid setting the IP address on the migration destination pod, but ovn-k controllers should  preserve the IP allocation, that is done when ovn-k detects that the pod is live migratable.</p> <p>Also the DHCP options will be configured to deliver the address to the VMs</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502logical switch node1  \u2502   \u2502logical switch node2\u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \n\u2502 lsp ns-virt-launcher1\u2502\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2502 lsp ns-virt-launcher2\u2502     \n\u2502 dhcpv4_options: 1234 \u2502            \u2502 dhcpv4_options: 1234 \u2502\n\u2502 address:             \u2502            \u2502 address:             \u2502    \n\u2502  0a:58:0a:f4:00:01   \u2502            \u2502  0a:58:0a:f4:00:01   \u2502\n\u2502  10.244.0.8          \u2502            \u2502  10.244.0.8          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 dhcp-options 1234               \u2502\n\u2502   lease_time: 3500              \u2502\n\u2502   router: 10.244.0.1            \u2502\n\u2502   dns_server: [kubedns]         \u2502\n\u2502   server_id: 169.254.1.1        \u2502\n\u2502   server_mac: c0:ff:ee:00:00:01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Virt-launcher pod address: The CNI will not set an address at virt-launcher pod netns, that address is assigned to the VM with the DHCP options from the LSP, this allows to use  kubevirt bridge binding with pod networking and still do live migration.</p>"},{"location":"features/live-migration/#ipam","title":"IPAM","text":"<p>The point to point routing feature allows an address to be running at a node different from the one \"owning\" the subnet the address is coming from. This will happen after VM live migration.</p> <p>One scenario for live migration is to shut down the node the VMs were migrated from, this means that the IPAM node subnet should go back to the pool but since the migrated VMs contains IPs from it those IPs should reserved in case the  subnet is assigned to a new node.</p> <p>On that case before assigning the subnet to the node the VMs ips need to be  reserved so they don't get assigned to new pods</p> <p>Another scenario is ovn-kubernetes pods restarting after live migration, on  that case ovnkube-master should discover to what IP pool the VM belongs.</p>"},{"location":"features/live-migration/#detecting-migratable-vms-and-changing-point-to-point-routes","title":"Detecting migratable VMs and changing point to point routes","text":"<p>To detect that a pod is migratable KubevirtVm the annotation <code>kubevirt.io/allow-pod-bridge-network-live-migration</code> has to be present and also the label <code>vm.kubevirt.io/name=vm1</code>, on that case the point to point routes will be created after live migration and also a  DHCPOptions will be configured to serve the ip configuration to the VM</p> <p>During live migration there are two virt-launcher pods with different names for the same VM (source and target). The ovn-k pod controller uses <code>CreationTimestamp</code> and <code>kubevirt.io/vm=vm1</code> to differentiate between them; it then watches the target pod <code>kubevirt.io/nodeName</code> and <code>kubevirt.io/migration-target-start-timestamp</code> annotation and label, with the following intent: - The <code>kubevirt.io/nodeName</code> is set after the VM finishes live migrating or when it becomes ready. - The <code>kubevirt.io/migration-target-start-timestamp</code> is set when live migration has not finished but migration-target pod is ready to receive traffic (this happens at post-copy live migration, where migration is taking too long).</p> <p>The point to point routing cleanup (remove of static routes and policies) will be done at this cases: - VM is deleted, all the routing related to the VM is removed at all the ovn zones. - VM is live migrated back to the node that owns its IP, all the routing related to the VM is removed at all the ovn zones. - ovn-kubernetes controllers are restarted, stale routing is removed.</p>"},{"location":"features/live-migration/#future-items","title":"Future Items","text":"<ul> <li>Implement single stack IPv6</li> </ul>"},{"location":"features/live-migration/#known-limitations","title":"Known Limitations","text":"<ul> <li>Only KubeVirt VMs with bridge binding pod network are supported</li> <li>Single stack IPv6 is not supported</li> <li>DualSack does not configure routes for IPv6 over DHCP/autoconf</li> <li>SRIOV is not supported</li> </ul>"},{"location":"features/live-migration/#references","title":"References","text":"<ul> <li>hypershift kubevirt live migration enhancement</li> </ul>"},{"location":"features/multicast/","title":"Multicast","text":""},{"location":"features/multicast/#introduction","title":"Introduction","text":"<p>IP multicast enables data to be delivered to multiple IP addresses simultaneously. Multicast can distribute data one-to-many or many-to-many. For this to happen, the 'receivers' join a multicast group, and the sender(s) send data to it. In other words, multicast filtering is achieved by dynamic group control management.</p> <p>The multicast group membership is implemented with IGMP. For details, check RFCs 1112 and 2236.</p>"},{"location":"features/multicast/#configuring-multicast-on-the-cluster","title":"Configuring multicast on the cluster","text":"<p>The feature is gated by config flag. In order to create a KIND cluster with multicast feature enabled, use the <code>--multicast-enabled</code> option with KIND.</p>"},{"location":"features/multicast/#enabling-multicast-per-namespace","title":"Enabling multicast per namespace","text":"<p>The multicast traffic between pods in the cluster is blocked by default; it can be enabled per namespace - but it cannot be enabled cluster wide.</p> <p>To enable multicast support on a given namespace, you need to annotate the namespace:</p> <pre><code>$ kubectl annotate namespace &lt;namespace name&gt; \\\n    k8s.ovn.org/multicast-enabled=true\n</code></pre>"},{"location":"features/multicast/#changes-in-ovn-northbound-database","title":"Changes in OVN northbound database","text":"<p>In this section we will be seeing plenty of OVN north entities; all of it consists of an example with a single pod:</p> <pre><code># only list the pod name + IPs of the pod (in the default namespace)\n$ kubectl get pods -o=custom-columns=Name:.metadata.name,IP:.status.podIPs\nName                                 IP\nvirt-launcher-vmi-masquerade-thr9j   [map[ip:10.244.1.8]]\n</code></pre> <p>The implementation of IPv4 multicast for ovn-kubernetes relies on: - 2 ACLs (ingress/egress) dropping all multicast traffic - on all switches (via clusterPortGroup) - 2 ACLs (ingress/egress) allowing all multicast traffic - on clusterRouterPortGroup  (that allows multicast between pods that reside on different nodes, see  https://github.com/ovn-org/ovn-kubernetes/commit/3864f2b6463392ae2d80c18d06bd46ec44e639f9 for more details)</p> <p>These ACLs Matches look like:</p> <pre><code># deny all multicast match\n\"(ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n\n# allow clusterPortGroup match ingress\n\"outport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n# allow clusterPortGroup match egress\n\"inport == @clusterRtrPortGroup &amp;&amp; (ip4.mcast || mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))\"\n</code></pre> <p>Then, for each annotated(<code>k8s.ovn.org/multicast-enabled=true</code>) namespace, two ACLs with higher priority are provisioned; in the following example we show ACLs that apply to the <code>default</code> namespace.</p> <pre><code># egress direction\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; ip4.mcast\"\n\n# ingress direction\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; (igmp || (ip4.src == $a5154718082306775057 &amp;&amp; ip4.mcast))\"\n</code></pre> <p>As can be seen in the match condition of the ACLs above, the former ACL allows egress traffic for all multicast traffic whose originating ports belong to the namespace, whereas the latter allows ingress multicast traffic for ports belonging to the namespace. This last match also assures that traffic originating by pods in the same namespace are allowed.</p> <p>Both these ACLs require a port group to keep track of all ports within the namespace - <code>@a16982411286042166782</code> - while the <code>ingress</code> ACL also requires the namespace's <code>address set</code> to be up to date. Both these tables can be seen below:</p> <pre><code># port group encoding the `default` namespace\n_uuid               : dde5bbb0-0b1d-4dab-b100-0b710f46fc28\nacls                : [b930b6ea-5b16-4eb1-b962-6b3e9273d0a0, f086c9b7-fa61-4a91-b545-f228f6cf954b]\nexternal_ids        : {name=default}\nname                : a16982411286042166782\nports               : [5fefc0c6-e651-48a9-aa0d-f86197b93267]\n\n# address set encoding the `default` namespace\n_uuid               : 1349957f-68e5-4f5a-af26-01ec58d96f6b\naddresses           : [\"10.244.1.8\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n</code></pre> <p>Note: notice the IP address on the address set encoding the default namespace matches the IP address of the pod listed on our example.</p> <p>For completeness, let's also take a look at the port referenced in the port group:</p> <p><pre><code>_uuid               : 5fefc0c6-e651-48a9-aa0d-f86197b93267\naddresses           : [\"0a:58:0a:f4:01:08 10.244.1.8\"]\ndhcpv4_options      : []\ndhcpv6_options      : []\ndynamic_addresses   : []\nenabled             : []\nexternal_ids        : {namespace=default, pod=\"true\"}\nha_chassis_group    : []\nname                : default_virt-launcher-vmi-masquerade-thr9j\noptions             : {requested-chassis=ovn-worker2}\nparent_name         : []\nport_security       : [\"0a:58:0a:f4:01:08 10.244.1.8\"]\ntag                 : []\ntag_request         : []\ntype                : \"\"\nup                  : true\n</code></pre> As you can see, this is the OVN logical switch port assigned to the pod running on the namespace we have annotated.</p>"},{"location":"features/multicast/#multicast-group-membership","title":"Multicast group membership","text":"<p>As indicated in the introduction secion, multicast filtering is achieved by dynamic group control management. The manager of these filtering lists is the switch that is configured to act as an IGMP querier - on OVN-K's case, the node's logical switches.</p> <p>For these membership requests to be allowed in the network, each node's logical switch's must declare themselves \"multicast queriers\", by using the <code>other_config:mcast_querier=true</code> option.</p> <p>The <code>other_config:mcast_snoop=true</code> is also used so the logical switches can passively snoop IGMP query / report / leave packets transferred between the multicast hosts and switches to compute group membership.</p> <p>Without these two options, multicast traffic would be treated as broadcast traffic, which forwards packets to all ports on the network.</p> <p>Please refer to the following snippet featuring the node's logical swithes of a cluster with one control plane node, and two workers, to see these options in use: <pre><code># control plane node\n_uuid               : 09d7a498-8885-4b66-9c50-da2286579382\nacls                : [ef11235e-54f7-4f7b-a19a-d6d935c836a2]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [78e40b60-458c-43b7-b1d8-344ca85c8b08, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, bafdde0a-\nf303-41e2-98eb-4e149bc75c91, ce697ebf-2059-48fa-a47c-20b901a18395, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-control-plane\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:00:01\", mcast_ip4_src=\"10.244.0.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.0.0/24\"}\nports               : [08273d87-50f9-461f-9421-4df0360a624b, 3ab00f31-a2dc-4257-8882-5e73913c55d3, 3d4ce3c5-407f-4fa2-bbb6-fe9107d9ddc2]\nqos_rules           : []\n\n# ovn-worker\n_uuid               : cb85ea2b-309b-43fa-8fc8-db97353e872c\nacls                : [8d9f3900-1cc2-433b-8a1b-c9536eac8575]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [17cb0b5e-840c-4aad-abc5-d0dd9d67a1e7, 2a84133f-0675-4f40-bc47-78608886b848, 38455c9c-5c03-4777-8f35-3d71b3af0487, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-worker\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:02:01\", mcast_ip4_src=\"10.244.2.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.2.0/24\"}\nports               : [199d3b11-3eba-41b6-947b-22db7f9ee529, 4202a225-83b4-46ff-ba87-624af78d2b16, 84cf40d2-7dc2-477c-aff8-bdf2e0fc60d7, b7415c11-2840-4abb-8965-3abf073aa26b, c716aa8a-7178-4d52-9b7f-174c22084e4e, c7d9ef31-4ec6-450e-8299-cc4d715ce9ea, dfa10e32-5613-4297-b635-75f4db26f4e7, ef13e922-a097-460f-94a2-e1f08b16fbeb, fd8e6b71-ea0d-4a44-a18b-824aa5ff6bc0]\nqos_rules           : []\n\n# ovn-worker 2\n_uuid               : b9255452-3e7a-4b84-ab64-11154432eb08\nacls                : [208907e0-55b1-4b6d-a1d8-985148ba6b29]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [32091c8e-394d-46a8-a65e-10b0f1d4aecc, 36f5827e-e027-4740-a908-d6e277b5ad55, 535c0e56-9762-4890-931b-50c4062aa673, 84b6c4a7-3ec4-46d4-8611-23267a2e1d83, 9ab56c1a-00f1-4d3d-bea0-e3a2679bce11, ebbb52b8-ba28-4550-b03f-0865e6c7162b]\nname                : ovn-worker2\nother_config        : {mcast_eth_src=\"0a:58:0a:f4:01:01\", mcast_ip4_src=\"10.244.1.1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.1.0/24\"}\nports               : [20e826fb-0298-4f45-8621-5af6fe7c3bd1, 49938955-f3b8-4363-ab40-acd826cd977c, 5fefc0c6-e651-48a9-aa0d-f86197b93267, 7323dbf3-f8a8-4858-8a00-5b7987e97648, 79463ad7-a50d-4a05-8e15-d695713f6bb3, dddf772b-6925-4fa7-9ea5-1e4fd1267cb3, f584c3b2-5370-45c6-912b-56be00168c98]\nqos_rules           : []\n</code></pre></p> <p>Note: it is important to note that the source IP / MAC address of the multicast queries sent by the logical switch are the addresses of the logical router ports connecting each node's switch to the cluster's router, as can be seen below.</p> <pre><code># ovn-control-plane logical router port\n_uuid               : e3e21af2-0ec5-4993-bb0c-e052b3f3eeb7\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:00:01\"\nname                : rtos-ovn-control-plane\nnetworks            : [\"10.244.0.1/24\"]\noptions             : {}\npeer                : []\n\n# ovn-worker logical router port\n_uuid               : 28be35a4-26cf-4daf-b922-c6aa5cecf58b\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:02:01\"\nname                : rtos-ovn-worker\nnetworks            : [\"10.244.2.1/24\"]\noptions             : {}\npeer                : []\n\n# ovn-worker2 logical router port\n_uuid               : 6bcbca4e-572f-4109-a71e-862292f463b2\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:0a:f4:01:01\"\nname                : rtos-ovn-worker2\nnetworks            : [\"10.244.1.1/24\"]\noptions             : {}\npeer                : []\n</code></pre> <p>Finally, to enable the multicast group to span across multiple OVN nodes, we need to enable multicast relay on the cluster router. That is done via the <code>option:mcast_relay=true</code> option. Please check below for a real OVN-K deployment example:</p> <pre><code>_uuid               : acb0f1ab-2b40-463b-b5ce-fbdd183f0f44\nenabled             : []\nexternal_ids        : {k8s-cluster-router=yes, k8s-ovn-topo-version=\"4\"}\nload_balancer       : []\nname                : ovn_cluster_router\nnat                 : [8002667a-f4a1-4cbb-8830-9286f3636791, a74f337f-6a8d-4cd6-a32d-df49560aa224, c38d2424-0e13-40a8-a67f-3317b9bdbbdd]\noptions             : {mcast_relay=\"true\"}\npolicies            : [1ace3e52-8be3-49fa-86ae-e910ffbe4dd3, 1afd7fa9-c32d-4240-84dd-da48115e95c8, 2274b1f0-e8c3-495d-9c9d-a5f14f517920, 24f00c8a-7df5-4041-986b-5be79a605807, 2d7df894-ea13-4819-9614-04c814f34c94, 3a20641e-29f3-4577-aaa5-12a5ae8a56fa, 521edd9a-bc5d-4ca7-bf13-7435c5f1fbce, 8d5ee2c5-65b3-478a-a034-a6624231b6ec, b024d223-56eb-462e-9e27-1b8b4ffcec08, b4999288-7af0-4b1f-bcd7-94974562e5b0, b4ac21ed-530f-45ac-bff4-fc09fcfedd25, d4c434df-9ddd-47ab-8016-062f42d3102b, ec342a8c-a39a-4ea5-ae13-b39d17760a4e]\nports               : [28be35a4-26cf-4daf-b922-c6aa5cecf58b, 3f5b669e-6c6c-46b0-a029-c6198d47706d, 6bcbca4e-572f-4109-a71e-862292f463b2, e3e21af2-0ec5-4993-bb0c-e052b3f3eeb7, f39f5210-8ec6-4d0b-89ef-8397599cc8cf]\nstatic_routes       : [3d9a8a37-368a-43ca-9c62-80cdae843b77, 53cfa8f0-a10e-45aa-9a9f-8e9b4910315b, 6f992b50-5c52-4caf-a146-ba5ca45d7d6a, ae5f8b78-3253-47b1-818d-13f07f42dd48, b65fcc82-1015-40dd-99f4-5b98e7514fe0, de705ce6-3a28-42ac-b3bb-fdba55b020a5]\n</code></pre>"},{"location":"features/multicast/#ipv6-considerations","title":"IPv6 considerations","text":"<p>There are some changes when the cluster is configured to also assign IPv6 addresses to the pods, starting with the <code>allow</code> ACLs, which now also account for the IPv6 addresses:</p> <pre><code>_uuid               : 67ed1d4d-c81e-4553-a232-f0448798462e\naction              : allow\ndirection           : to-lport\nexternal_ids        : {default-deny-policy-type=Ingress}\nlog                 : false\nmatch               : \"outport == @a16982411286042166782 &amp;&amp; ((igmp || (ip4.src == $a5154718082306775057 &amp;&amp; ip4.mcast)) || (mldv1 || mldv2 || (ip6.src == $a5154715883283518635 &amp;&amp; (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1))))\"\nmeter               : acl-logging\nname                : default_MulticastAllowIngress\npriority            : 1012\nseverity            : info\n\n_uuid               : bda7c475-613e-4dcf-8e88-024ef70b030d\naction              : allow\ndirection           : from-lport\nexternal_ids        : {default-deny-policy-type=Egress}\nlog                 : false\nmatch               : \"inport == @a16982411286042166782 &amp;&amp; (ip4.mcast || (mldv1 || mldv2 || (ip6.dst[120..127] == 0xff &amp;&amp; ip6.dst[116] == 1)))\"\nmeter               : acl-logging\nname                : default_MulticastAllowEgress\npriority            : 1012\nseverity            : info\n</code></pre> <p>Please note that in fact there is an address set per IP family per namespace ; this means the <code>default</code>  namespaces has two distinct address sets: one for IPv4, another for IPv6:</p> <pre><code>_uuid               : 2b48e408-5058-470b-8c0f-59d839d6a80f\naddresses           : [\"10.244.2.9\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 37076cff-9560-47c2-bbaa-312ff5e1a114\naddresses           : [\"fd00:10:244:3::9\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre> <p>Finally, it is also important to refer we need to specify the <code>mcast_ip6_src</code> option on each node's logical switch, also using the IP address of each node's logical router port:</p> <pre><code>_uuid               : e1fd9e60-831a-4ca9-ad4c-6a5bdfc018f8\nacls                : [0403fbb5-6633-46f1-8481-b329c1ccd916, f49c2de7-5f9d-42bf-a2dc-5f10b53a8347]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [142ed4bf-1422-4e9a-a69f-9d07aa67abb8, 6df77979-f829-465f-9617-abc479945261, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265, f05994f7-3f21-457e-aa2b-896a86da319e, fa55f0a8-e3af-4467-8444-a54acfffef6d]\nname                : ovn-control-plane\nother_config        : {ipv6_prefix=\"fd00:10:244:1::\", mcast_eth_src=\"0a:58:0a:f4:00:01\", mcast_ip4_src=\"10.244.0.1\", mcast_ip6_src=\"fe80::858:aff:fef4:1\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.0.0/24\"}\nports               : [abea774f-d989-402a-a2af-07c066b130db, c6c6468f-6459-4072-8c98-e02bf08fb377, ecceb049-8afd-40ea-bd6d-77eab32c3302]\nqos_rules           : []\n\n_uuid               : 6b7de40a-618a-4d5a-b8b8-58e1f964ee71\nacls                : [42018b5b-31e2-4733-bcde-c6ea470836f2, d980fb54-3178-468a-981b-b43e5efa1d48]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [109b7d66-2a59-4945-b067-1e51548a6f30, 6727e44b-6256-47b7-8146-0dc9561ce270, 6df77979-f829-465f-9617-abc479945261, 7e59e9b1-a2db-4005-97f3-c1b99131126f, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265]\nname                : ovn-worker\nother_config        : {ipv6_prefix=\"fd00:10:244:2::\", mcast_eth_src=\"0a:58:0a:f4:01:01\", mcast_ip4_src=\"10.244.1.1\", mcast_ip6_src=\"fe80::858:aff:fef4:101\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.1.0/24\"}\nports               : [0a9f14e7-94f0-49f2-b5df-acb123ace867, 53251da6-1c21-4e81-97f6-a9b41c64d71a, 6b5deecb-7536-4eeb-a865-2b8ca17be1fc, 8d56ad80-e45c-43b7-9f20-91d15bfc8836, a3252b54-7c2d-48e0-b2b2-36d121ca7292, cc0c791d-c6ee-4381-b8a4-91462039bfd1, f261f485-b974-4448-9751-62e883d677f2, fe77f44c-51ee-45f7-a7a2-4e03862803de]\nqos_rules           : []\n\n_uuid               : 3b4e3a69-8439-4124-a028-cb2851d80da6\nacls                : [607abd66-a12a-4387-8b3f-66c4ba4cb9a8, 9963f94c-cde0-41a1-b0cc-c3e6f5800e67]\ndns_records         : []\nexternal_ids        : {}\nforwarding_groups   : []\nload_balancer       : [26a7f6a7-9a5c-4023-ac55-10a860237a6d, 2fa35fce-a63a-4185-ae49-b9c09709fdea, 6df77979-f829-465f-9617-abc479945261, a9395376-9e96-4b1e-ac2d-8b03947db6cc, bd8a8f3a-4872-49d2-8d14-cd83c48ca265, c3de2c11-8cd3-4d6f-9373-7a1b54e1366a]\nname                : ovn-worker2\nother_config        : {ipv6_prefix=\"fd00:10:244:3::\", mcast_eth_src=\"0a:58:0a:f4:02:01\", mcast_ip4_src=\"10.244.2.1\", mcast_ip6_src=\"fe80::858:aff:fef4:201\", mcast_querier=\"true\", mcast_snoop=\"true\", subnet=\"10.244.2.0/24\"}\nports               : [1b40d04a-26d2-4881-ac55-6c2a5fa679a9, 35316f83-79c3-4a23-9fdd-04cefd963f54, 39c240ef-3303-4fa4-8b5c-b860d69d7c11, 44ddb1ab-56c3-4665-8f1e-02505553a950, 728221da-fe1f-41cc-90ff-9e2a8eb8fb6d, ac22f113-7e40-4657-ad7e-4444a39bfd45, bd115b86-bdc1-4427-9cc7-0ece2d9269c6, f2a89d6a-b6c8-491c-b2b1-398d37c5aa4c]\nqos_rules           : []\n</code></pre>"},{"location":"features/multicast/#sources","title":"Sources","text":"<ul> <li>PR introducing multicast into OVN-K</li> <li>PR introducing IPv6 multicast support into OVN-K</li> <li>Dumitru Ceara's presentation about IGMP snooping / relay</li> </ul>"},{"location":"features/network-qos-guide/","title":"Guide to Using Network QoS","text":""},{"location":"features/network-qos-guide/#contents","title":"Contents","text":"<ol> <li>Overview </li> <li>Create a Secondary Network (NAD) </li> <li>Define a NetworkQoS Policy </li> <li>Create Sample Pods and Verify the Configuration</li> <li>Explain the NetworkQoS Object</li> </ol>"},{"location":"features/network-qos-guide/#1-overview","title":"1 Overview","text":"<p>Differentiated Services Code Point (DSCP) marking and egress bandwidth metering let you prioritize or police specific traffic flows. The new NetworkQoS Custom Resource Definition (CRD) in ovn-kubernetes makes both features available to Kubernetes users on all pod interfaces\u2014primary or secondary\u2014without touching pod manifests.</p> <p>This guide provides a step-by-step example of how to use this feature. Before you begin, ensure that you have a Kubernetes cluster configured with the ovn-kubernetes CNI. Since the examples use network attachments, you must run the cluster with multiple network support enabled. In a kind cluster, you would use the following flags:</p> <pre><code>cd contrib\n./kind-helm.sh -nqe -mne ;  #  --enable-network-qos --enable-multi-network\n</code></pre>"},{"location":"features/network-qos-guide/#2-create-a-secondary-network","title":"2 Create a Secondary Network","text":"<p>File: nad.yaml</p> <p><pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ovn-stream\n  namespace: default\n  labels:                   # label needed for NetworkQoS selector\n    nad-type: ovn-kubernetes-nqos\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"1.0.0\",\n            \"name\": \"ovn-stream\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\": \"layer3\",\n            \"subnets\": \"10.245.0.0/16/24\",\n            \"mtu\": 1300,\n            \"master\": \"eth1\",\n            \"netAttachDefName\": \"default/ovn-stream\"\n    }\n</code></pre> Why the label? <code>NetworkQoS</code> uses a label selector to find matching NADs. Without at least one label, the selector cannot match.</p>"},{"location":"features/network-qos-guide/#3-define-a-networkqos-policy","title":"3 Define a NetworkQoS Policy","text":"<p>File: nqos.yaml</p> <p><pre><code>apiVersion: k8s.ovn.org/v1alpha1\nkind: NetworkQoS\nmetadata:\n  name: qos-external\n  namespace: default\nspec:\n  networkSelectors:\n  - networkSelectionType: NetworkAttachmentDefinitions\n    networkAttachmentDefinitionSelector:\n      namespaceSelector: {}  # any namespace\n      networkSelector:\n        matchLabels:\n          nad-type: ovn-kubernetes-nqos\n  podSelector:\n    matchLabels:\n      nqos-app: bw-limited\n  priority: 10              # higher value wins in a tie-break\n  egress:\n  - dscp: 20               \n    bandwidth:\n      burst: 100            # kilobits\n      rate: 20000           # kbps\n    classifier:\n      to:\n      - ipBlock:\n          cidr: 0.0.0.0/0\n          except:\n          - 10.11.12.13/32\n          - 172.16.0.0/12\n          - 192.168.0.0/16\n</code></pre> A full CRD template lives here.</p> <p>The <code>egress</code> field is a list, allowing you to define multiple markings and bandwidth limits based on different classifiers.</p> <p>Note that this configuration will apply to the NAD of pods based on the network selector, and only on pods that have the label <code>nqos-app: bw-limited</code>.</p> <p><pre><code>$ kubectl create -f nad.yaml &amp;&amp; \\\n  kubectl create -f nqos.yaml\n\nnetworkattachmentdefinition.k8s.cni.cncf.io/ovn-stream created\nnetworkqos.k8s.ovn.org/qos-external created\n</code></pre> At this point, the output from <code>kubectl get networkqoses</code> will look like this:</p> <pre><code>$ kubectl api-resources -owide | head -1 ; \\\n  kubectl api-resources -owide | grep NetworkQoS\nNAME                                SHORTNAMES         APIVERSION                          NAMESPACED   KIND                               VERBS                                                        CATEGORIES\nnetworkqoses                                           k8s.ovn.org/v1alpha1                true         NetworkQoS                         delete,deletecollection,get,list,patch,create,update,watch\n\n$ kubectl get networkqoses qos-external -n default -owide\nNAME           STATUS\nqos-external   NetworkQoS Destinations applied\n</code></pre>"},{"location":"features/network-qos-guide/#4-create-sample-pods-and-verify-the-configuration","title":"4 Create Sample Pods and Verify the Configuration","text":""},{"location":"features/network-qos-guide/#41-launch-test-pods","title":"4.1  Launch Test Pods","text":"<p>To test this, let's create a pod using a helper function that allows us to add labels to it.</p> <p>File: create_pod.source</p> <pre><code>create_pod() {\n    local pod_name=${1:-pod0}\n    local node_name=${2:-ovn-worker}\n    local extra_labels=${3:-}\n\n    NAMESPACE=$(kubectl config view --minify --output 'jsonpath={..namespace}')\n    NAMESPACE=${NAMESPACE:-default}\n\n    if ! kubectl get pod \"$pod_name\" -n \"$NAMESPACE\" &amp;&gt;/dev/null; then\n        echo \"Creating pod $pod_name in namespace $NAMESPACE...\"\n\n        # Prepare labels block\n        labels_block=\"    name: $pod_name\"\n        if [[ -n \"$extra_labels\" ]]; then\n            # Convert JSON string to YAML-compatible lines\n            while IFS=\"=\" read -r k v; do\n                labels_block+=\"\n    $k: $v\"\n            done &lt; &lt;(echo \"$extra_labels\" | jq -r 'to_entries|map(\"\\(.key)=\\(.value)\")|.[]')\n        fi\n\n        # Generate the manifest\n        cat &lt;&lt;EOF | kubectl apply -n \"$NAMESPACE\" -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: $pod_name\n  labels:\n$labels_block\n  annotations:\n    k8s.v1.cni.cncf.io/networks: ovn-stream@eth1\nspec:\n  nodeSelector:\n    kubernetes.io/hostname: $node_name\n  containers:\n  - name: $pod_name\n    image: ghcr.io/nicolaka/netshoot:v0.13\n    command: [\"/bin/ash\", \"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]\nEOF\n    else\n        echo \"Pod $pod_name already exists.\"\n    fi\n}\n</code></pre> <pre><code>$ create_pod pod0 &amp;&amp; \\\n  create_pod pod1 ovn-worker '{\"nqos-app\":\"bw-limited\"}' &amp;&amp; \\\n  create_pod pod2 ovn-worker2 '{\"foo\":\"bar\",\"nqos-app\":\"bw-limited\"}' &amp;&amp; \\\n  echo pods created\n\nextract_pod_ip_from_annotation() {\n    local pod_name=\"$1\"\n    local namespace=\"${2:-default}\"\n    local interface=\"${3:-eth1}\"\n\n    kubectl get pod \"$pod_name\" -n \"$namespace\" -o json |\n        jq -r '.metadata.annotations[\"k8s.v1.cni.cncf.io/network-status\"]' |\n        jq -r --arg iface \"$interface\" '.[] | select(.interface == $iface) | .ips[0]'\n}\n</code></pre> <pre><code>NAMESPACE=$(kubectl config view --minify --output 'jsonpath={..namespace}') ; NAMESPACE=${NAMESPACE:-default}\nDST_IP_POD0=$(extract_pod_ip_from_annotation pod0 $NAMESPACE eth1)\nDST_IP_POD1=$(extract_pod_ip_from_annotation pod1 $NAMESPACE eth1)\nDST_IP_POD2=$(extract_pod_ip_from_annotation pod2 $NAMESPACE eth1)\n\n# Let's see the NAD IP addresses of the pods created\n$ echo pod0 has ip $DST_IP_POD0 ; \\\n  echo pod1 has ip $DST_IP_POD1 ; \\\n  echo pod2 has ip $DST_IP_POD2\n\npod0 has ip 10.245.4.4\npod1 has ip 10.245.4.3\npod2 has ip 10.245.2.3\n</code></pre>"},{"location":"features/network-qos-guide/#42-checking-bandwidth","title":"4.2  Checking Bandwidth","text":"<p><code>qos-external</code> limits only traffic on pods that carry <code>nqos-app=bw-limited</code>. That means:</p> <ul> <li>pod1 \u2192 pod0: unlimited (no matching label)</li> <li>pod1 \u2192 pod2: rate-limited to \u2248 20 Mbit/s</li> </ul> <p>Follow these steps to verify it with <code>iperf3</code>.</p> <pre><code># 1) Start an iperf server inside pod0 and pod2 (runs forever in background)\nkubectl -n default exec pod0 -- iperf3 -s -p 5201 &amp;\nkubectl -n default exec pod2 -- iperf3 -s -p 5201 &amp;\n\n# 2) From pod1 \u2192 pod0  (EXPECTED \u2248 line rate)\nkubectl -n default exec pod1 -- iperf3 -c \"$DST_IP_POD0\"   -p 5201 -R -t 10\n\n# 3) From pod1 \u2192 pod2  (EXPECTED \u2248 20 Mbit/s)\nkubectl -n default exec pod1 -- iperf3 -c \"$DST_IP_POD2\"   -p 5201 -R -t 10\n</code></pre> <p>Sample output:</p> <pre><code># to pod0 (unlimited)\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  37.2 GBytes  31.9 Gbits/sec  607             sender\n[  5]   0.00-10.00  sec  37.2 GBytes  31.9 Gbits/sec                  receiver\n\n# to pod1 (rate-limited)\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec  20.8 MBytes  17.4 Mbits/sec  4056             sender\n[  5]   0.00-10.00  sec  20.8 MBytes  17.4 Mbits/sec                  receiver\n</code></pre> <p>The sharp drop confirms that <code>NetworkQoS</code> is enforcing the 20 Mbit/s rate limit only for pods matching the selector.</p>"},{"location":"features/network-qos-guide/#43-packet-capture","title":"4.3  Packet Capture","text":"<p>Generate ICMP traffic and observe DSCP markings in Geneve outer headers using <code>tcpdump -envvi eth0 geneve</code> inside the worker node's network namespace. Only flows involving label-matched pods (those with <code>nqos-app=bw-limited</code>) will show <code>tos 0x50</code> (DSCP 20).</p> <pre><code># Run ping commands in the background, so we can look at packets they generate\n\n# pod0 to pod2\nnohup kubectl exec -i pod0 -- ping -c 3600 -q $DST_IP_POD2 &gt;/dev/null 2&gt;&amp;1 &amp;\n# pod1 to pod2\nnohup kubectl exec -i pod1 -- ping -c 3600 -q $DST_IP_POD2 &gt;/dev/null 2&gt;&amp;1 &amp;\n\nsudo dnf install -y --quiet tcpdump ; # Install tcpdump, if needed\n\nIPNS=$(docker inspect --format '{{ .State.Pid }}' ovn-worker)\nsudo nsenter -t ${IPNS} -n tcpdump -envvi eth0 geneve\n</code></pre> <pre><code>tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n\n**Pod0 to Pod2**: Notice that since pod0 does not have the label to match against NetworkQoS, its TOS is 0. However, pod2's response is DSCP marked (tos 0x50), since pod2 matches the NetworkQoS criteria with the label `nqos-app: bw-limited`.\n\n12:46:30.755551 02:42:ac:12:00:06 &gt; 02:42:ac:12:00:05, ethertype IPv4 (0x0800), length 156: (tos 0x0, ttl 64, id 26896, offset 0, flags [DF], proto UDP (17), length 142)\n    172.18.0.6.38210 &gt; 172.18.0.5.geneve: [bad udp cksum 0x58bb -&gt; 0xc87d!] Geneve, Flags [C], vni 0x12, proto TEB (0x6558), options [class Open Virtual Networking (OVN) (0x102) type 0x80(C) len 8 data 00090006]\n        0a:58:0a:f5:02:01 &gt; 0a:58:0a:f5:02:03, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 61037, offset 0, flags [DF], proto ICMP (1), length 84)\n    10.245.4.4 &gt; 10.245.2.3: ICMP echo request, id 14, seq 44, length 64\n\n\u2014\n\n12:46:30.755694 02:42:ac:12:00:05 &gt; 02:42:ac:12:00:06, ethertype IPv4 (0x0800), length 156: (tos 0x50, ttl 64, id 46220, offset 0, flags [DF], proto UDP (17), length 142)\n    172.18.0.5.38210 &gt; 172.18.0.6.geneve: [bad udp cksum 0x58bb -&gt; 0xc47d!] Geneve, Flags [C], vni 0x12, proto TEB (0x6558), options [class Open Virtual Networking (OVN) (0x102) type 0x80(C) len 8 data 0004000a]\n        0a:58:0a:f5:04:01 &gt; 0a:58:0a:f5:04:04, ethertype IPv4 (0x0800), length 98: (tos 0x50, ttl 63, id 45002, offset 0, flags [none], proto ICMP (1), length 84)\n    10.245.2.3 &gt; 10.245.4.4: ICMP echo reply, id 14, seq 44, length 64\n\n\u2014---------\n\n**Pod1 to Pod2**: Traffic is marked both ways (both pods have the matching label)\n\n12:46:30.497289 02:42:ac:12:00:06 &gt; 02:42:ac:12:00:05, ethertype IPv4 (0x0800), length 156: (tos 0x50, ttl 64, id 26752, offset 0, flags [DF], proto UDP (17), length 142)\n    172.18.0.6.7856 &gt; 172.18.0.5.geneve: [bad udp cksum 0x58bb -&gt; 0x3f10!] Geneve, Flags [C], vni 0x12, proto TEB (0x6558), options [class Open Virtual Networking (OVN) (0x102) type 0x80(C) len 8 data 00090006]\n        0a:58:0a:f5:02:01 &gt; 0a:58:0a:f5:02:03, ethertype IPv4 (0x0800), length 98: (tos 0x50, ttl 63, id 21760, offset 0, flags [DF], proto ICMP (1), length 84)\n    10.245.4.3 &gt; 10.245.2.3: ICMP echo request, id 14, seq 56, length 64\n\n\u2014\n\n12:46:30.497381 02:42:ac:12:00:05 &gt; 02:42:ac:12:00:06, ethertype IPv4 (0x0800), length 156: (tos 0x50, ttl 64, id 46019, offset 0, flags [DF], proto UDP (17), length 142)\n    172.18.0.5.7856 &gt; 172.18.0.6.geneve: [bad udp cksum 0x58bb -&gt; 0x3b11!] Geneve, Flags [C], vni 0x12, proto TEB (0x6558), options [class Open Virtual Networking (OVN) (0x102) type 0x80(C) len 8 data 0004000a]\n        0a:58:0a:f5:04:01 &gt; 0a:58:0a:f5:04:03, ethertype IPv4 (0x0800), length 98: (tos 0x50, ttl 63, id 3850, offset 0, flags [none], proto ICMP (1), length 84)\n    10.245.2.3 &gt; 10.245.4.3: ICMP echo reply, id 14, seq 56, length 64\n</code></pre>"},{"location":"features/network-qos-guide/#5-explain-the-networkqos-object","title":"5 Explain the NetworkQoS Object","text":"<p>Below is an abbreviated map of the CRD schema returned by <code>kubectl explain networkqos --recursive</code> (v1alpha1). Use this as a quick reference. For the definitive specification, always consult the <code>kubectl explain</code> output or the CRD YAML in the ovn-kubernetes repository.</p>"},{"location":"features/network-qos-guide/#51-toplevel-spec-keys","title":"5.1  Top\u2011level <code>spec</code> keys","text":"Field Type Required Purpose podSelector <code>LabelSelector</code> No Selects pods whose traffic will be evaluated by the QoS rules. If empty, all pods in the namespace are selected. networkSelectors[] list <code>NetworkSelector</code> No Restricts the rule to traffic on specific networks. If absent, the rule matches any interface. (See \u00a75.2) priority <code>int</code> Yes Higher number \u2192 chosen first when multiple <code>NetworkQoS</code> objects match the same packet. egress[] list <code>EgressRule</code> Yes One or more marking / policing rules. Evaluated in the order listed. (See \u00a75.3) <p>Note the square-bracket notation (<code>[]</code>) for both <code>egress</code> and <code>networkSelectors</code>\u2014each is an array in the CRD.</p>"},{"location":"features/network-qos-guide/#52-inside-a-networkselectors-entry","title":"5.2  Inside a <code>networkSelectors[]</code> entry","text":"<p>Each list element tells the controller where the pods' egress traffic must flow in order to apply the rule. Exactly one selector type must be set.</p> Key Required Description <code>networkSelectionType</code> Yes Enum that declares which selector below is populated. Common values: <code>NetworkAttachmentDefinitions</code>, <code>DefaultNetwork</code>, <code>SecondaryUserDefinedNetworks</code>, \u2026 <code>networkAttachmentDefinitionSelector</code> conditional When <code>networkSelectionType=NetworkAttachmentDefinitions</code>. Selects NADs by namespaceSelector (required) and networkSelector (required). Both are ordinary <code>LabelSelectors</code>. <code>secondaryUserDefinedNetworkSelector</code> conditional Used when <code>networkSelectionType=SecondaryUserDefinedNetworks</code>. Similar structure: required namespaceSelector &amp; networkSelector. <code>clusterUserDefinedNetworkSelector</code>, <code>primaryUserDefinedNetworkSelector</code> conditional Additional selector styles, each with required sub\u2011selectors as per the CRD. <p>Typical usage \u2013 <code>networkSelectionType: NetworkAttachmentDefinitions</code> + <code>networkAttachmentDefinitionSelector</code>.</p>"},{"location":"features/network-qos-guide/#53-inside-an-egress-rule","title":"5.3  Inside an <code>egress[]</code> rule","text":"Field Type Required Description <code>dscp</code> <code>int</code> (0 \u2013 63) Yes DSCP value to stamp on the inner IP header. This value determines the traffic priority. <code>bandwidth.rate</code> <code>int</code> (kbps) No Sustained rate for the token-bucket policer (in kilobits per second). <code>bandwidth.burst</code> <code>int</code> (kilobits) No Maximum burst size that can accrue (in kilobits). <code>classifier.to</code> / <code>classifier.from</code> list <code>TrafficSelector</code> No CIDRs the packet destination (or source) must match. Each entry is an <code>ipBlock</code> supporting an <code>except</code> list. <code>classifier.ports[]</code> list No List of <code>{protocol, port}</code> tuples the packet must match; protocol is <code>TCP</code>, <code>UDP</code>, or <code>SCTP</code>. <p>If all specified classifier conditions match, the packet gets the DSCP mark and/or bandwidth policer defined above. This allows for fine-grained control over which traffic flows receive QoS treatment.</p>"},{"location":"features/network-qos/","title":"Network QoS","text":""},{"location":"features/network-qos/#introduction","title":"Introduction","text":"<p>To enable NetworkQoS, we will use Differentiated Services Code Point (DSCP) which allows us to classify packets by setting a 6-bit field in the IP header, effectively marking the priority of a given packet relative to other packets as \"Critical\", \"High Priority\", \"Best Effort\" and so on.</p>"},{"location":"features/network-qos/#problem-statement","title":"Problem Statement","text":"<p>The workloads running in Kubernetes using OVN-Kubernetes as a networking backend might have different requirements in handling network traffic. For example video streaming application needs low latency and jitter whereas storage application can tolerate with packet loss. Hence NetworkQoS is essential in meeting these SLAs to provide better service quality.</p> <p>The workload taffic can be either east west (pod to pod traffic) or north south traffic (pod to external traffic) types in a Kubernetes cluster which is limited by finite bandwidth. So NetworkQoS must ensure high priority applications get the necessary NetworkQoS marking so that it can prevent network conjestion.</p>"},{"location":"features/network-qos/#proposed-solution","title":"Proposed Solution","text":"<p>By introducing a new CRD <code>NetworkQoS</code>, users could specify a DSCP value for packets originating from pods on a given namespace heading to a specified Namespace Selector, Pod Selector, CIDR, Protocol and Port. This also supports metering for the packets by specifying bandwidth parameters <code>rate</code> and/or <code>burst</code>. The CRD will be Namespaced, with multiple resources allowed per namespace. The resources will be watched by ovn-k, which in turn will configure OVN's QoS Table. The <code>NetworkQoS</code> also has <code>status</code> field which is populated by ovn-k which helps users to identify whether NetworkQoS rules are configured correctly in OVN or not.</p>"},{"location":"features/network-qos/#sources","title":"Sources","text":"<ul> <li>OKEP-4380: Network QoS Support</li> </ul>"},{"location":"features/template/","title":"Name of the Feature","text":"<ul> <li>Add diagrams wherever possible to explain things better</li> <li>Keep the end user, new community contributors in mind as target audience when you write this documentation; not an ovn expert</li> <li>Remove sections if they are not relevant for your feature</li> <li>Add new sections if your feature needs them</li> <li>This document should empower end users to start playing around with this feature and understand implementation details</li> <li>If you are looking for inspiration see admin network policies feature docs for a good example.</li> </ul>"},{"location":"features/template/#introduction","title":"Introduction","text":"<p>This section is important for producing high quality user-focused documentation. Goal here is to highlight what this feature is all about. This paragraph can be used to construct release notes when we do releases.</p> <p>Your Introduction should be one paragraph long. More detail should go into the following sections.</p>"},{"location":"features/template/#motivation","title":"Motivation","text":"<p>This section is for explicitly listing the motivation, goals and non-goals of this feature. Describe why the change is important and the benefits to users. Goal here is to highlight why is this feature needed. If there was an enhancement proposal written please link that in this section so that readers can go through the user stories there.</p>"},{"location":"features/template/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>NOTE: If you had an enhancement proposal written for this feature then copy the user stories over from there OR provide a link to that enhancement proposal.</p> <p>(What new user-stories/use-cases does this feature introduce?)</p> <p>A user story should typically have a summary structured this way:</p> <ol> <li>As a [user concerned by the story]</li> <li>I want [goal of the story]</li> <li>so that [reason for the story]</li> </ol> <p>The \u201cso that\u201d part is optional if more details are provided in the description. A story can also be supplemented with examples, diagrams, or additional notes.</p> <p>e.g</p> <p>Story 1: Deny traffic at a cluster level</p> <p>As a cluster admin, I want to apply non-overridable deny rules to certain pod(s) and(or) Namespace(s) that isolate the selected resources from all other cluster internal traffic.</p> <p>For Example: The admin wishes to protect a sensitive namespace by applying an AdminNetworkPolicy which denies ingress from all other in-cluster resources for all ports and protocols.</p>"},{"location":"features/template/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>Did you write new configs or knobs that need to be turned on for this feature? If so educate end users on those details. How can users disable this feature if they don't want it?</p>"},{"location":"features/template/#workflow-description","title":"Workflow Description","text":"<p>Explain how the user will use the feature. Be detailed and explicit. Describe all of the actors, their roles, and the APIs or interfaces involved. Define a starting state and then list the steps that the user would need to go through to trigger the feature described in the enhancement. A detailed architectural diagram is a must to showcase how this feature works in our CNI. Remember that a picture can speak a thousand words.</p>"},{"location":"features/template/#implementation-details","title":"Implementation Details","text":"<p>Use the following sub-sections to explain more implementation details in a top down fashion</p>"},{"location":"features/template/#user-facing-api-changes","title":"User facing API Changes","text":"<p>If any API changes were done, they must be outlined here and look at the developer docs on how to generate API reference documentation for this feature. Provide a link to that API reference web page from this spot. Also add details on where the CRD lives; example is it in kubernetes or kubernetes-sigs or network-plumbing-working-group?</p> <p>Provide a sample CRD/API.</p>"},{"location":"features/template/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p>Keep in mind goal is to educate users how this feature was implemented in OVN-Kubernetes in this section.</p> <p>What were the changes made to ovn-kubernetes control plane and data plane to make this happen? Note differences if any for local gateway versus shared gateway and default mode versus interconnect mode. A detailed OVN-Kubernetes networking topology diagram is a must to showcase how this feature works in our CNI. Remember that a picture can speak a thousand words. Give details on how the above API is watched and converted into OVN objects by OVN-Kubernetes. If there are changes done on host-networking level like adding new routes they must be highlighted.</p>"},{"location":"features/template/#ovn-constructs-created-in-the-databases","title":"OVN Constructs created in the databases","text":"<p>What were the OVN Objects used? Give snippets and details on the various database objects that this feature creates in the OVN NBDB and SBDB. Keep in mind goal is to educate users how this feature was implemented using OVN in this section. You can go as deep as you want like mentioning how the pipelines get executed for your feature.</p> <p>Provide sample nbctl commands to view the constructs and show the output.</p>"},{"location":"features/template/#ovs-flows-generated","title":"OVS Flows generated","text":"<p>Provide details on specific changes done to br-ex, br-int or how the relevant openflows generated by this feature look like. Keep in mind goal is to educate users how this feature was implemented using OVS in this section.</p> <p>Provide sample ofctl commands to view the flows and show the output.</p>"},{"location":"features/template/#troubleshooting","title":"Troubleshooting","text":"<p>Include details on how the end user can know if the feature is correctly configured here? What are the different ways to troubleshoot this feature?</p> <ul> <li>What metrics, alerts were added to warn users something has gone wrong?</li> <li>Debugging notes</li> </ul>"},{"location":"features/template/#best-practices","title":"Best Practices","text":"<ul> <li>Write best practices to be used/recommended for smooth or better functioning of the feature if any; example \"using namespaceSelectors\" will scale better than using \"podSelectors\" because flows generated will be less for former than latter etc.</li> </ul>"},{"location":"features/template/#future-items","title":"Future Items","text":"<p>Add a bullet list of future plans if any</p>"},{"location":"features/template/#known-limitations","title":"Known Limitations","text":"<p>Add a bullet list of known limitations if any</p>"},{"location":"features/template/#references","title":"References","text":"<p>Provide links or other relevant details outside of this documentation that you think end users should read. If there were useful github discussions, public google docs enhancement proposals etc they must be mentioned in this section</p>"},{"location":"features/bgp-integration/route-advertisements/","title":"Route Advertisements","text":""},{"location":"features/bgp-integration/route-advertisements/#introduction","title":"Introduction","text":"<p>The Route Advertisements feature introduces BGP as a supported routing protocol with OVN-Kubernetes enabling the integration into different BGP user environments. The extent of the Route Advertisements feature and corresponding API allows importing routes from BGP peers on the provider network into OVN pod networks as well as exporting pod network and egress IP routes to BGP peers on the provider network. Both default pod network as well as primary Layer 3 and Layer 2 cluster-user-defined networks (CUDNs) are supported.</p> <p>[!NOTE] For purposes of this documentation, the external, physical network of the cluster which a user administers will be called the \u201cprovider network\u201d.</p>"},{"location":"features/bgp-integration/route-advertisements/#prerequisites","title":"Prerequisites","text":"<ul> <li>FRR-k8s</li> </ul>"},{"location":"features/bgp-integration/route-advertisements/#motivation","title":"Motivation","text":"<p>There are multiple driving factors which necessitate integrating BGP into OVN-Kubernetes:</p> <ul> <li> <p>Importing Routes from the Provider Network: Today there is no API for a user to be able to configure routes into OVN. In order for a user to change how egress traffic is routed, the user leverages local gateway mode. This mode forces traffic to hop through the Linux networking stack, and there a user can configure routes inside the host to control egress routing. This manual configuration would need to be performed and maintained across nodes and VRFs within each node.</p> </li> <li> <p>Exporting Routes into the Provider Network: There exists a need for provider networks to learn routes directly to pods today in Kubernetes. One such use case is integration with 3rd party load balancers, where they terminate a load balancer and then send packets directly to cluster nodes with the destination IP address being the pod IP itself. Today these load balancers rely on custom operators to detect which node a pod is scheduled to and then add routes into its load balancer to send the packet to the right node. By integrating BGP and advertising the pod subnets/addresses directly on the provider network, load balancers and other entities on the network would be able to reach the pod IPs directly.</p> </li> </ul> <p>Additionally, integrating BGP support paves the way for other BGP based features that might be implemented in the future, like:</p> <ul> <li>EVPN support to extend pod network isolation outside the cluster.</li> <li>No overlay mode to avoid the Geneve overhead.</li> </ul>"},{"location":"features/bgp-integration/route-advertisements/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<ul> <li>As a user, I want to be able to leverage my existing BGP network to dynamically   learn routes to pods in my Kubernetes cluster.</li> <li>As a user, rather than having to maintain routes manually in each Kubernetes   node, as well as being constrained to using local gateway mode for respecting   user-defined routes; I want to use BGP so that I can dynamically advertise   egress routes for the Kubernetes pod traffic in either gateway mode.</li> <li>As an egress IP user, I want to use a pure routing implementation to handle   advertising egress IP movement across nodes.</li> <li>As a user, I want to extend CUDN isolation to the provider network over a   VRF-Lite type of VPN where I can restrict traffic of the CUDN to an interface   attached to the VRF associated with the CUDN.</li> </ul> <p>[!NOTE] The isolation between different pod networks is unaffected by this feature.</p>"},{"location":"features/bgp-integration/route-advertisements/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>The <code>route-advertisements</code> feature must be enabled in the OVN-Kubernetes configuration. Please use the <code>Feature Config</code> option <code>enable-route-advertisements</code> under <code>OVNKubernetesFeatureConfig</code> config to enable it.</p>"},{"location":"features/bgp-integration/route-advertisements/#user-facing-api-changes","title":"User-facing API Changes","text":"<p>A new OVN-Kubernetes API is introduced for this feature: <code>RouteAdvertisements</code>.</p>"},{"location":"features/bgp-integration/route-advertisements/#workflow-description","title":"Workflow Description","text":"<p>OVN-Kubernetes integrates with FRR-k8s to provide BGP support and it must be deployed before enabling the <code>route-advertisements</code> feature.</p> <p>Once deployed, an initial FRR-k8s configuration must be done using its <code>FRRConfiguration</code> API which serves, among others, three purposes:</p> <ul> <li>Configure BGP peering.</li> <li>Configure route import.</li> <li>Serve as a template to the <code>FRRConfiguration</code> instances that OVN-Kubernetes   generates.</li> </ul> <p>Finally, route export is configured through <code>RouteAdvertisements</code> instances. Each <code>RouteAdvertisements</code> instance allows to select which pod networks to export routes for. It also allows to select which <code>FRRConfiguration</code> instances to use as template, and as a consequence, provides the flexibility to export routes in a different number of ways including: which BGP peers to export to, the use of iBGP or eBGP, etc.</p>"},{"location":"features/bgp-integration/route-advertisements/#import-routes-into-the-default-pod-network","title":"Import routes into the default pod network","text":"<p>The following example represents an initial FRR-k8s configuration that configures FRR-k8s to have all the nodes establish a BGP peering session and receive routes in the <code>172.20.0.0/16</code> subnet:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  labels:\n    use-for-advertisements: default\n  name: receive-filtered\n  namespace: frr-k8s-system\nspec:\n  nodeSelector: {}\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      - address: 192.168.111.3\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: filtered\n            prefixes:\n            - prefix: 172.20.0.0/16\n</code></pre> <p>This will result in the routes being installed in the main (default VRF) routing table on the nodes and used by the pod egress traffic in local gateway mode. As long as the <code>route-advertisements</code> feature is enabled, OVN-Kubernetes will synchronize the BGP routes from the default VRF to the default OVN pod network gateway router and hence used for the egress traffic of the pods on that network in shared gateway mode.</p> <p>[!NOTE] For two BGP routers to establish a peering session and exchange routes, their configurations must be mutually aligned: the <code>neighbor</code> configuration in the previous example must correspond to the remote BGP router's configuration (router ID, AS number, accept routes, etc...), and vice versa.</p>"},{"location":"features/bgp-integration/route-advertisements/#import-routes-from-the-default-vrf-into-a-cudn","title":"Import routes from the default VRF into a CUDN","text":"<p>Assuming we have a CUDN:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: extranet\n  labels:\n    advertise: \"true\"\nspec:\n  namespaceSelector:\n    matchLabels:\n      network: extranet\n  network:\n    topology: Layer3\n    layer3:\n      role: Primary\n      subnets:\n      - cidr: \"22.100.0.0/16\"\n        hostSubnet: 24\n</code></pre> <p>After routes have been imported to the default VRF as in the previous example, a typical scenario is to import those routes from the default VRF to a CUDN as well. This can be achieved with:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  labels:\n    use-for-advertisements: default\n  name: import-extranet\n  namespace: frr-k8s-system\nspec:\n  nodeSelector: {}\n  bgp:\n    routers:\n    - asn: 64512\n      imports:\n      - vrf: default\n      vrf: extranet\n</code></pre> <p>This will result in the routes being installed in the extranet VRF associated to the CUDN of the same name. If <code>route-advertisements</code> feature is enabled, OVN-Kubernetes will synchronize the BGP routes installed on a VRF to the OVN gateway router of the associated CUDN and hence will be used for the egress traffic of the pods on that network.</p> <p>[!NOTE] As long as the name of the CUDN is less than 16 characters, the corresponding VRF name for the network will have the same name. Otherwise the name will be pseudo-randomly generated and not easy to predict. Future enhancements will allow for the VRF name to be configurable.</p> <p>[!NOTE] If you export routes for a CUDN over the default VRF as detailed on the next sections, installed BGP routes in the default VRF are imported to the CUDN automatically and this configuration is not necessary.</p>"},{"location":"features/bgp-integration/route-advertisements/#export-routes-to-the-default-pod-network","title":"Export routes to the default pod network","text":"<p>Assuming the <code>FRRConfiguration</code> examples that have been used previously, this example would advertise routes to the default pod network and its egress IPs:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default\nspec:\n  targetVRF: default\n  advertisements:\n  - PodNetwork\n  - EgressIP\n  nodeSelector: {}\n  frrConfigurationSelector:\n    matchLabels:\n      use-for-advertisements: default\n  networkSelectors:\n  - networkSelectionType: DefaultNetwork\n</code></pre> <p>This would advertise routes for the pod network to the BGP peers as defined on the selected <code>FRRConfiguration</code> instances; and make the necessary changes to correctly handle N/S traffic directly addressing IPs of that network.</p> <p>Currently, when the <code>advertisements</code> field includes <code>PodNetwork</code>, you must select all nodes with <code>nodeSelector</code>. However, if you are only advertising egress IPs, you can limit advertisements to egress IPs assigned to the selected nodes:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default-egressip\nspec:\n  advertisements:\n  - EgressIP\n  nodeSelector: \n    matchLabels:\n      egress-nodes: bgp\n  frrConfigurationSelector:\n    matchLabels:\n      use-for-advertisements: default\n  networkSelectors:\n  - networkSelectionType: DefaultNetwork\n</code></pre> <p>[!NOTE] Egress IPs will be advertised over the selected BGP sessions regardless of whether they are assigned to the same interface those sessions are established over or not, probably making the advertisements ineffective if they are not the same.</p>"},{"location":"features/bgp-integration/route-advertisements/#export-routes-to-a-cudn-over-the-default-vrf","title":"Export routes to a CUDN over the default VRF","text":"<p>Similarly, routes to pods on a CUDN can be advertised over the default VRF:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default-cudn\nspec:\n  targetVRF: default\n  advertisements:\n  - PodNetwork\n  - EgressIP\n  nodeSelector: {}\n  frrConfigurationSelector:\n    matchLabels:\n      use-for-advertisements: default\n  networkSelectors:\n  - networkSelectionType: ClusterUserDefinedNetworks\n    clusterUserDefinedNetworkSelector:\n      networkSelector:\n        matchLabels:\n          advertise: true\n</code></pre> <p>Note that this configuration also results in the BGP installed routes of the default VRF to be imported to the CUDN VRF.</p> <p>Multiple types of network selectors can be specified making it possible to merge the previous two examples into one:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default-all\nspec:\n  targetVRF: default\n  advertisements:\n  - PodNetwork\n  - EgressIP\n  nodeSelector: {}\n  frrConfigurationSelector:\n    matchLabels:\n      use-for-advertisements: default\n  networkSelectors:\n  - networkSelectionType: DefaultNetwork\n  - networkSelectionType: ClusterUserDefinedNetworks\n    clusterUserDefinedNetworkSelector:\n      networkSelector:\n        matchLabels:\n          advertise: true\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#import-and-export-routes-to-a-cudn-over-the-network-vrf-vrf-lite","title":"Import and export routes to a CUDN over the network VRF (VRF-Lite)","text":"<p>It is also possible to import and export routes to a CUDN over a BGP session established over that network's VRF without involving the default VRF at all.</p> <p>To import, we define the proper <code>FRRConfiguration</code> first. This example is similar to how routes are imported for the default pod network with the exception that the BGP peering session is configured to happen over the CUDN VRF <code>extranet</code>:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  labels:\n    use-for-advertisements: extranet\n  name: receive-filtered-extranet\n  namespace: frr-k8s-system\nspec:\n  nodeSelector: {}\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      - address: 192.168.221.3\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: filtered\n            prefixes:\n            - prefix: 172.20.0.0/16\n      vrf: extranet\n</code></pre> <p>Then we define the <code>RouteAdvertisements</code> to export:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: extranet\nspec:\n  targetVRF: auto\n  advertisements:\n  - PodNetwork\n  nodeSelector: {}\n  frrConfigurationSelector:\n    matchLabels:\n      use-for-advertisements: extranet\n  networkSelectors:\n  - networkSelectionType: ClusterUserDefinedNetworks\n    clusterUserDefinedNetworkSelector:\n      networkSelector:\n        matchLabels:\n          advertise: true\n</code></pre> <p><code>targetVRF</code> value <code>auto</code> is a magic helper value that tells OVN-Kubernetes to advertise each network over that network's VRF.</p> <p>When a CUDN is advertised only over its own VRF, OVN-Kubernetes interprets this as an explicit intention to isolate the network to that VRF and takes additional measures to ensure that no network traffic is leaked externally over the default VRF. This configuration is referred to as <code>VRF-Lite</code>. An external provider edge BGP router could map this isolated traffic to an EVPN achieving a similar use case as if EVPN were to be supported directly.</p> <p>[!NOTE] For the BGP session to be actually established over that network's VRF, at least one interface with proper IP configuration needs to be attached to the network's VRF. The CUDN egress traffic matching the learned routes will be routed through that interface. OVN-Kubernetes does not manage this interface nor its attachment to the network's VRF.</p> <p>[!NOTE] This configuration is only supported in local gateway mode. Additionally, this configuration does not support the advertisement of egress IPs.</p>"},{"location":"features/bgp-integration/route-advertisements/#cudn-isolation","title":"CUDN isolation","text":"<p>User defined networks are isolated by default. In other words, users on CUDN A cannot access pods on CUDN B via their internal pod or service addresses. When advertising CUDNs via BGP on the same VRF (typically the default VRF), the behavior of inter-CUDN isolation is preserved: from the perspective of a CUDN, traffic addressing the subnet of a different CUDN will be considered N/S traffic and will egress the cluster towards the provider network; and if the provider network is able to route it back to the cluster by virtue of learned BGP routes, the traffic will still be dropped to upkeep the CUDN isolation promise.</p> <p>OVN-Kubernetes relaxes the default advertised UDN isolation behavior when the configuration flag <code>advertised-udn-isolation-mode</code> is set to <code>loose</code>. In this configuration, traffic addressing the subnet of a different CUDN will egress the cluster towards the provider network as before but, if routed back towards the cluster, connectivity will be allowed in this case.</p>"},{"location":"features/bgp-integration/route-advertisements/#implementation-details","title":"Implementation Details","text":""},{"location":"features/bgp-integration/route-advertisements/#overview","title":"Overview","text":"<pre><code>flowchart TD\n    S@{shape: sm-circ}\n\n    S--&gt;|User configures|T0\n\n    subgraph T0\n    J0@{shape: f-circ}\n    F0(FRRConfiguration)\n    R0(RouteAdvertisements)\n    C0(CUDNs)\n    J0--&gt;|to configure BGP peering and route import|F0\n    J0--&gt;|to export routes|R0\n    J0--&gt;|to add networks|C0\n    end\n\n    R0--&gt;|ovn-kubernetes configures|J1\n\n    subgraph T1\n    J1@{shape: f-circ}\n    F1(FRRConfiguration)\n    O1(OVN Networks)\n    H1(Host Networks)\n    J1--&gt;F1\n    J1--&gt;O1\n    J1--&gt;H1\n    end\n\n    F0--&gt;J2\n    F1--&gt;J2\n\n    subgraph T2\n    J2@{shape: f-circ}\n    F2{/etc/frr.conf}\n    J2 --&gt;|FRR-k8s configures|F2\n    end\n\n    F2--&gt;T3\n\n    subgraph T3\n    J3@{shape: f-circ}\n    E3@{shape: framed-circle}\n    F31(FRR advertises exported routes)\n    F32(FRR installs imported routes in host)\n    J3--&gt;F31--&gt;E3\n    J3--&gt;F32--&gt;E3\n    end\n\n    T3--&gt;T4\n\n    subgraph T4\n    J4@{shape: f-circ}\n    E4@{shape: framed-circle}\n    O4(ovn-kubernetes copies installed routes to OVN)\n    J4--&gt;O4--&gt;E4\n    end\n</code></pre> <p>The flowchart above gives an idea on what happens on different convergence timelines:</p> <ul> <li>T0: Initially a user configures CUDN networks, sets up BGP peering and route   import with <code>FRRConfiguration</code> instances and route export with   <code>RouteAdvertisements</code> instances.</li> <li>T1: OVN-Kubernetes reacts to the configured <code>RouteAdvertisements</code> and   generates the appropriate <code>FRRConfiguration</code> instances to export the selected   networks. OVN-Kubernetes then reconfigures those networks in both OVN and the   host stack so that they operate correctly when advertised.</li> <li>T2: FRR-k8s merges all the <code>FRRConfiguration</code> instances and configures its   internal FRR daemons.</li> <li>T3: FRR daemons export, import and install routes accordingly.</li> <li>T4: OVN-Kubernetes copies installed routes to the appropriate OVN networks.</li> </ul>"},{"location":"features/bgp-integration/route-advertisements/#routeadvertisements-controller","title":"RouteAdvertisements controller","text":"<p>The <code>RouteAdvertisements</code> controller reacts to <code>RouteAdvertisements</code> instances and generates the corresponding <code>FRRConfiguration</code> instances to export routes for the selected networks. It also annotates the NetworkAttachmentDefinition instances for the selected networks to instruct the OVN and host network controllers on each node to reconfigure the network.</p>"},{"location":"features/bgp-integration/route-advertisements/#frrconfiguration-instances-generated-by-ovn-kubernetes","title":"FRRConfiguration instances generated by OVN-Kubernetes","text":"<p>When <code>RouteAdvertisements</code> instances are configured, OVN-Kubernetes generates additional <code>FRRConfiguration</code> instances in order for the selected network prefixes to be advertised, using the following logic:</p> <ul> <li>For each pair combination of selected network and selected node; and for each   selected <code>FRRConfiguration</code> to be used as template:<ul> <li>If the <code>FRRConfiguration</code> does not apply to the node, it is discarded.</li> <li>If a router defined in that <code>FRRConfiguration</code> does not apply to the   target VRF, it is discarded.</li> <li>An <code>FRRConfiguration</code> instance is generated that contains all routers that   were not discarded with the following modifications:<ul> <li>If advertising pod network:<ul> <li>Router <code>prefixes</code> and neighbors <code>toAdvertise</code> <code>prefixes</code> set to:<ul> <li>the network host subnet for default network or layer 3   topologies.</li> <li>the network subnet for layer 2 topologies.</li> </ul> </li> <li>Neighbors \u201ctoReceive\u201d cleared defaulting to <code>filtered</code> mode with   no prefixes.</li> <li>If <code>targetVRF</code> and network VRF are different and <code>targetVRF</code> is   not \u201cauto\u201d, routes are imported reciprocally across both VRFs:<ul> <li>An import from the network VRF.</li> <li>An additional router on network VRF to import from target VRF.</li> </ul> </li> </ul> </li> <li>If advertising egress IPs: for each egress IP, if the egress IP   selects a namespace served by the selected network and it is assigned   to the selected node, the egress IP is added to \u201cprefixes\u201d and   neighbors \u201ctoAdvertise\u201d.</li> </ul> </li> </ul> </li> </ul> <p>This is an example of an <code>FRRConfiguration</code> instance generated for a node from previous <code>RouteAdvertisements</code> examples when a CUDN is advertised over the default VRF:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  annotations:\n    k8s.ovn.org/route-advertisements: extranet/receive-filtered/master-1.ostest.test.metalkube.org\n  labels:\n    k8s.ovn.org/route-advertisements: extranet\n  name: ovnk-generated-vl8gk\n  namespace: frr-k8s-system\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      imports:\n      - vrf: extranet\n      neighbors:\n      - address: 192.168.111.3\n        asn: 64512\n        disableMP: true\n        toAdvertise:\n          allowed:\n            mode: filtered\n            prefixes:\n            - 22.100.2.0/24\n        toReceive:\n          allowed:\n            mode: filtered\n      prefixes:\n      - 22.100.2.0/24\n    - asn: 64512\n      imports:\n      - vrf: default\n      vrf: extranet\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/hostname: master-1.ostest.test.metalkube.org\n</code></pre> <p>This example <code>FRRConfiguration</code> instance applies to one of the nodes but you would see similar <code>FRRConfiguration</code> instances for the other selected nodes. In summary, the instance is instructing FRR-k8s to advertise the <code>22.100.2.0/24</code> prefix, which is the one assigned to pods hosted on that node for that network, over the session established towards the BGP peer <code>192.168.111.3</code> as instructed by the selected <code>FRRConfiguration</code> instances used as a template to generate this one.</p> <p>From this example, it is relevant to highlight a couple of things:</p> <ul> <li>When a CUDN is advertised over the default VRF, received routes on the default   VRF will also be imported to the VRF associated with the CUDN and become   available for use to that CUDN.</li> <li>A previously mentioned, this generated configuration only deals with the   advertisement of routes. Route reception must be configured manually as   detailed in previous sections. Particularly, cluster advertised routes are not   configured to be received by other cluster nodes as that would be problematic   for the intra-cluster connectivity.</li> </ul> <p>[!NOTE] <code>FRRConfiguration</code> instances generated in this manner by OVN-Kubernetes can't become selected by <code>RouteAdvertisements</code>.</p>"},{"location":"features/bgp-integration/route-advertisements/#ovn-network-controllers-impacts-in-ovn-configuration","title":"OVN Network controllers: impacts in OVN configuration","text":"<p>OVN Network controllers on each node react to annotations on the NetworkAttachmentDefinition, processing the applicable <code>RouteAdvertisements</code> instances for the network and gathering information on how the network is being advertised.</p>"},{"location":"features/bgp-integration/route-advertisements/#ovn-snat-behavior-with-bgp-advertisement","title":"OVN SNAT behavior with BGP Advertisement","text":"<p>Usually N/S egress traffic from a pod is SNATed to the node IP. This does not happen when the network is advertised. In that case the traffic egresses the cluster with the pod IP as source. In shared gateway mode this is handled with a conditional SNAT on the gateway routers OVN configuration for the network which ensures that E/W egress traffic (right now, only pod-to-node traffic) continues to be SNATed. </p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ovn-nbctl list nat\n...\n_uuid               : 7855a3a5-412c-4083-963c-b11aa80b7784\nallowed_ext_ips     : []\nexempted_ext_ips    : []\nexternal_ids        : {}\nexternal_ip         : \"172.18.0.2\"\nexternal_mac        : []\nexternal_port_range : \"32768-60999\"\ngateway_port        : []\nlogical_ip          : \"10.244.1.3\"\nlogical_port        : []\nmatch               : \"ip4.dst == $a712973235162149816\" # added condition matching E/W traffic when advertised\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n\n...\n\n_uuid               : 7be1b70b-88c7-4482-85ff-487663be9eda\naddresses           : [\"172.18.0.2\", \"172.18.0.3\", \"172.18.0.4\", \"172.19.0.2\", \"172.19.0.3\", \"172.19.0.4\"]\nexternal_ids        : {ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:EgressIP:node-ips:v4:default\", \"k8s.ovn.org/name\"=node-ips, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=EgressIP, network=default}\nname                : a712973235162149816\n...\n</code></pre> <p>For CUDNs in local gateway mode, this is handled on a similar way with a conditional SNAT to the network's masquerade IP which would then finally be SNATed to the node IP on the host.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ovn-nbctl list nat\n...\n_uuid               : 61b26442-fa08-4aa8-b326-97afb71edab1\nallowed_ext_ips     : []\nexempted_ext_ips    : []\nexternal_ids        : {\"k8s.ovn.org/network\"=cluster_udn_udn-l2, \"k8s.ovn.org/topology\"=layer2}\nexternal_ip         : \"169.254.0.11\"\nexternal_mac        : []\nexternal_port_range : \"32768-60999\"\ngateway_port        : []\nlogical_ip          : \"22.100.0.0/16\"\nlogical_port        : []\nmatch               : \"ip4.dst == $a712973235162149816\"\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n...\n</code></pre> <p>Egress IP SNAT is unaffected.</p>"},{"location":"features/bgp-integration/route-advertisements/#route-import","title":"Route import","text":"<p>When BGP routes get installed in a node's routing table, OVN-Kubernetes synchronizes them to the gateway router of the corresponding OVN network making them available for egress in shared gateway mode.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ovn-nbctl lr-route-list 076a4cba-c680-4fa3-ae2f-1ce7e0a1e153\nIPv4 Routes\nRoute Table &lt;main&gt;:\n           169.254.0.0/17               169.254.0.4 dst-ip rtoe-GR_ovn-worker2\n            10.244.0.0/16                100.64.0.1 dst-ip\n            172.26.0.0/16                172.18.0.5 dst-ip rtoe-GR_ovn-worker2  # learned route synced from host VRF\n                0.0.0.0/0                172.18.0.1 dst-ip rtoe-GR_ovn-worker2\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#host-network-controllers-impacts-on-host-networking-stack","title":"Host network controllers: impacts on host networking stack","text":""},{"location":"features/bgp-integration/route-advertisements/#ingress-ovs-flows","title":"Ingress OVS flows","text":"<p>Flows are added to handle the ingress of N/S traffic addressing IPs of the advertised pod networks. This traffic is forwarded to the corresponding patch port of the network and is then handled by OVN with no extra changes required in shared gateway mode.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-q76br -c ovnkube-controller -- ovs-ofctl dump-flows breth0\n...\n # flows forwarding pod networks to the corresponding patch ports\n cookie=0xdeff105, duration=445.802s, table=0, n_packets=0, n_bytes=0, idle_age=445, priority=300,ip,in_port=1,nw_dst=10.244.0.0/24 actions=output:2\n cookie=0xdeff105, duration=300.323s, table=0, n_packets=0, n_bytes=0, idle_age=300, priority=300,ip,in_port=1,nw_dst=22.100.0.0/16 actions=output:3\n</code></pre> <p>In local gateway mode, the traffic is forwarded to the host networking stack from where it is routed to the network management port.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ovs-ofctl dump-flows breth0\n ...\n # flows forwarding pod networks to host\n cookie=0xdeff105, duration=57.620s, table=0, n_packets=0, n_bytes=0, idle_age=57, priority=300,ip,in_port=1,nw_dst=22.100.0.0/16 actions=LOCAL\n cookie=0xdeff105, duration=9589.541s, table=0, n_packets=0, n_bytes=0, idle_age=9706, priority=300,ip,in_port=1,nw_dst=10.244.1.0/24 actions=LOCAL\n ...\n\n\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ip route\n...\n# routing to the default pod network management port\n10.244.0.0/16 via 10.244.1.1 dev ovn-k8s-mp0 \n10.244.1.0/24 dev ovn-k8s-mp0 proto kernel scope link src 10.244.1.2\n...\n\n\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ip rule\n...\n# for a CUDN, an ip rule takes care of routing on the correct VRF\n2000: from all to 22.100.0.0/16 lookup 1010\n...\n\n\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ip r show table 1010\n...\n# also routing to the CUDN management port\n22.100.0.0/16 dev ovn-k8s-mp1 proto kernel scope link src 22.100.0.2 \n...\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#host-snat-behavior-with-bgp-advertisement","title":"Host SNAT behavior with BGP Advertisement","text":"<p>In the same way that was done for the OVN configuration, the host networking stack configuration is updated to inhibit the SNAT for N/S traffic.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- nft list ruleset\n...\n  set remote-node-ips-v4 {\n    type ipv4_addr\n    comment \"Block egress ICMP needs frag to remote Kubernetes nodes\"\n    elements = { 172.18.0.3, 172.18.0.4,\n           172.19.0.2, 172.19.0.4 }\n  }\n...\n  chain ovn-kube-pod-subnet-masq {\n    # ip daddr condition added if default pod network advertised\n    ip saddr 10.244.1.0/24 ip daddr @remote-node-ips-v4 masquerade # ip daddr condition if advertised\n  }\n...\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#vrf-lite-isolation","title":"VRF-Lite isolation","text":"<p>To ensure isolation in VRF-Lite configurations, the default route pointing to the default VRF gateway present on the network's VRF is inhibited. Thus only BGP installed routes will be used for N/S traffic.</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- ip r show table 1010\n# default match unreachable\nunreachable default metric 4278198272 \n...\n# installed route going through interface attached to VRF\n172.26.0.0/16 nhid 28 via 172.19.0.5 dev eth1 proto bgp metric 20 \n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#cudn-isolation_1","title":"CUDN isolation","text":"<p>To ensure CUDN isolation in local gateway mode filtering rules are added to the host configuration</p> <pre><code>\u276f kubectl exec -n ovn-kubernetes ovnkube-node-vkmkt -c ovnkube-controller -- nft list ruleset\n...\n  set advertised-udn-subnets-v4 {\n    type ipv4_addr\n    flags interval\n    comment \"advertised UDN V4 subnets\"\n    elements = { 22.100.0.0/16 comment \"cluster_udn_udn-l2\" }\n  }\n...\n  chain udn-bgp-drop {\n          comment \"Drop traffic generated locally towards advertised UDN subnets\"\n          type filter hook output priority filter; policy accept;\n          ct state new ip daddr @advertised-udn-subnets-v4 counter packets 0 bytes 0 drop\n          ct state new ip6 daddr @advertised-udn-subnets-v6 counter packets 0 bytes 0 drop\n  }\n...\n</code></pre> <p>These rules are inhibited if OVN-Kubernetes is configured in \"loose advertised UDN isolation mode\".</p>"},{"location":"features/bgp-integration/route-advertisements/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/bgp-integration/route-advertisements/#troubleshooting-routeadvertisements","title":"Troubleshooting RouteAdvertisements","text":"<p>Check <code>RouteAdvertisement</code> status for configuration errors:</p> <pre><code>\u276f kubectl get ra\nNAME       STATUS\ndefault    Accepted\nextranet   Not Accepted: configuration pending: no networks selected\n</code></pre> <p>Check that <code>FRRConfiguration</code> have been generated as expected:</p> <pre><code>\u276f kubectl get frrconfiguration -n frr-k8s-system\nNAME                   AGE\novnk-generated-66plb   14m\novnk-generated-fxncs   13m\novnk-generated-grdfg   14m\novnk-generated-qhz9b   14m\novnk-generated-sgphk   13m\novnk-generated-vtwpv   13m\nreceive-all            14m\n</code></pre> <p>Expected <code>FRRConfiguration</code> are: - Any manual configuration done to import routes - MetalLB generated FRRConfiguration if in use - One of ovnk-generated-XXXXX configuration per RouteAdvertisement and selected FRRConfiguration/Node combination</p>"},{"location":"features/bgp-integration/route-advertisements/#troubleshooting-frr-k8s","title":"Troubleshooting FRR-K8s","text":"<p>FRR-K8s merges all FRRConfiguration into a single FRR configuration for each node. The status of generating that configuration and applying it to FRR daemon running on each node is relayed through <code>FRRNodeStates</code>:</p> <pre><code>\u276f kubectl get -n frr-k8s-system frrnodestates\nNAME                AGE\novn-control-plane   16m\novn-worker          16m\novn-worker2         16m\n\n$ oc describe -n openshift-frr-k8s frrnodestates worker-0.ostest.test.metalkube.org \nName:         worker-0.ostest.test.metalkube.org\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  frrk8s.metallb.io/v1beta1\nKind:         FRRNodeState\nMetadata:\n  Creation Timestamp:  2025-09-10T11:29:44Z\n  Generation:          1\n  Resource Version:    52036\n  UID:                 34f67799-9642-40a3-a378-67ca3ad5dfd2\nSpec:\nStatus:\n  Last Conversion Result:  success # whether FRRConfiguration merge and conversion to FRR config was successful \n  Last Reload Result:      success # whether resulting FRR config was applied correctly\n  Running Config:\n    # the FRR running config is displayed here\n...\n</code></pre> <p>FRR-K8s provides metrics:</p> <pre><code>  Namespace = \"frrk8s\"\n  Subsystem = \"bgp\"\n\n  SessionUp = metric{\n    Name: \"session_up\",\n    Help: \"BGP session state (1 is up, 0 is down)\",\n  }\n\n  UpdatesSent = metric{\n    Name: \"updates_total\",\n    Help: \"Number of BGP UPDATE messages sent\",\n  }\n\n  Prefixes = metric{\n    Name: \"announced_prefixes_total\",\n    Help: \"Number of prefixes currently being advertised on the BGP session\",\n  }\n\n  ReceivedPrefixes = metric{\n    Name: \"received_prefixes_total\",\n    Help: \"Number of prefixes currently being received on the BGP session\",\n  }\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#troubleshooting-frr","title":"Troubleshooting FRR","text":"<p>FRR is deployed by FRR-K8s as a daemonset and runs on every node:</p> <pre><code>\u276f kubectl get pods -n frr-k8s-system -o wide\nNAME                                     READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\nfrr-k8s-daemon-5cqbq                     6/6     Running   0          22m   172.18.0.4   ovn-worker2         &lt;none&gt;           &lt;none&gt;\nfrr-k8s-daemon-6hmzb                     6/6     Running   0          22m   172.18.0.3   ovn-worker          &lt;none&gt;           &lt;none&gt;\nfrr-k8s-daemon-gsmml                     6/6     Running   0          22m   172.18.0.2   ovn-control-plane   &lt;none&gt;           &lt;none&gt;\n...\n</code></pre> <p>Different aspects of the running daemons can be checked through the <code>vtysh</code> CLI. Some examples are:</p> <ul> <li>The running configuration:</li> </ul> <pre><code>$ kubectl exec -ti -n frr-k8s-system frr-k8s-daemon-5cqbq -c frr -- vtysh -c \"show running-conf\"\nBuilding configuration...\n\nCurrent configuration:\n!\nfrr version 8.5.3\nfrr defaults traditional\nhostname ovn-worker2\nlog file /etc/frr/frr.log informational\nlog timestamp precision 3\nno ip forwarding\nservice integrated-vtysh-config\n!\nrouter bgp 64512\n no bgp ebgp-requires-policy\n no bgp hard-administrative-reset\n...\n</code></pre> <ul> <li>The BGP session states:</li> </ul> <pre><code>\u276f kubectl exec -ti -n frr-k8s-system frr-k8s-daemon-5cqbq -c frr -- vtysh -c \"show bgp neighbor 172.18.0.5\"\nBGP neighbor is 172.18.0.5, remote AS 64512, local AS 64512, internal link\n  \u2026\nHostname: 78d5a0f1d3cd\n  BGP version 4, remote router ID 172.18.0.5, local router ID 172.18.0.4\n  BGP state = Established, up for 00:01:29\n  ...\n    Last reset 00:03:30,  Peer closed the session\n...\n</code></pre> <ul> <li>The actual routes exchanged through BGP:</li> </ul> <pre><code>\u276f kubectl exec -ti -n frr-k8s-system frr-k8s-daemon-5cqbq -c frr -- vtysh -c \"show bgp ipv4\"\nBGP table version is 2, local router ID is 172.18.0.4, vrf id 0\nDefault local pref 100, local AS 64512\nStatus codes:  s suppressed, d damped, h history, * valid, &gt; best, = multipath,\n               i internal, r RIB-failure, S Stale, R Removed\nNexthop codes: @NNN nexthop's vrf id, &lt; announce-nh-self\nOrigin codes:  i - IGP, e - EGP, ? - incomplete\nRPKI validation codes: V valid, I invalid, N Not found\n\n    Network          Next Hop            Metric LocPrf Weight Path\n *&gt; 10.244.0.0/24    0.0.0.0                  0         32768 i\n *&gt; 22.100.0.0/16    0.0.0.0                  0         32768 i\n *&gt;i172.26.0.0/16    172.18.0.5               0    100      0 i\n\n\nDisplayed  2 routes and 2 total paths\n</code></pre> <ul> <li>Routes installed on the host and their origin:</li> </ul> <pre><code>\u276f kubectl exec -ti -n frr-k8s-system frr-k8s-daemon-5cqbq -c frr -- vtysh -c \"show ip route\"\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, F - PBR,\n       f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\n...\nB&gt;* 172.26.0.0/16 [200/0] via 172.18.0.5, breth0, weight 1, 00:41:11\n...\n</code></pre> <p>Most of these commands have variations to check the same information specific to a VRF:</p> <pre><code>\u276f kubectl exec -ti -n frr-k8s-system frr-k8s-daemon-gv76r -c frr -- vtysh -c \"show ip route vrf udn-l2\"\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n       T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR,\n       f - OpenFabric,\n       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup\n       t - trapped, o - offload failure\n\nVRF udn-l2:\n...\nB&gt;* 172.26.0.0/16 [200/0] via 172.18.0.5, breth0 (vrf default), weight 1, 01:39:55\n...\n</code></pre>"},{"location":"features/bgp-integration/route-advertisements/#troubleshooting-dataplane","title":"Troubleshooting dataplane","text":"<p>FRR applies its configuration to the host networking stack in the form of routes. Thus standard tooling can be used for dataplane troubleshooting: connectivity checks, tcpdump, ovn-trace, ovs-trace, ...</p>"},{"location":"features/bgp-integration/route-advertisements/#best-practices","title":"Best Practices","text":"<p>TBD</p>"},{"location":"features/bgp-integration/route-advertisements/#future-items","title":"Future Items","text":"<ul> <li>EVPN support</li> <li>No overlay support</li> </ul>"},{"location":"features/bgp-integration/route-advertisements/#known-limitations","title":"Known Limitations","text":"<ul> <li>The <code>route-advertisements</code> feature is only supported in inter-connect mode.</li> <li>Advertised CUDNs must have a name of length under 16 characters to use a   homonym and predictable VRF name.</li> <li>Pod network IPs must be advertised from all nodes. As such, a   <code>RouteAdvertisements</code> instance including <code>PodNetwork</code> as <code>advertisements</code> type   must select all nodes with its <code>nodeSelector</code>.</li> <li>VRF-Lite configurations are only supported in local gateway mode.</li> <li>Egress IP advertisements are not supported for Layer 2 CUDNs or in VRF-Lite   configurations.</li> <li>Egress IPs will be advertised over the selected BGP sessions regardless of   whether they are assigned to the same interface as those sessions are   established over or not, probably making the advertisements ineffective if   they are not the same.</li> </ul>"},{"location":"features/bgp-integration/route-advertisements/#references","title":"References","text":"<ul> <li>FRR-k8s</li> <li>FRR</li> </ul>"},{"location":"features/cluster-egress-controls/egress-ip/","title":"EgressIP","text":""},{"location":"features/cluster-egress-controls/egress-ip/#introduction","title":"Introduction","text":"<p>The Egress IP feature enables a cluster administrator to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network. East-West traffic (including pod -&gt; node IP) is excluded from Egress IP.  </p> <p>For more info, consider looking at the following links: - Egress IP CRD - Assigning an egress IP address - Managing Egress IP in OpenShift 4 with OVN-Kubernetes</p>"},{"location":"features/cluster-egress-controls/egress-ip/#example","title":"Example","text":"<p>An example of EgressIP might look like this:</p> <p><pre><code>apiVersion: k8s.ovn.org/v1\nkind: EgressIP\nmetadata:\n  name: egressip-prod\nspec:\n  egressIPs:\n    - 172.18.0.33\n    - 172.18.0.44\n  namespaceSelector:\n    matchExpressions:\n      - key: environment\n        operator: NotIn\n        values:\n          - development\n  podSelector:\n    matchLabels:\n      app: web\n</code></pre> It specifies to use <code>172.18.0.33</code> or <code>172.18.0.44</code> egressIP for pods that are labeled with <code>app: web</code> that run in a namespace without <code>environment: development</code> label. Both selectors use the generic kubernetes label selectors.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#layer-3-network","title":"Layer 3 network","text":"<p>Supported network configs: - Cluster default network - Role primary user defined networks</p>"},{"location":"features/cluster-egress-controls/egress-ip/#egressip-ip-is-assigned-to-the-primary-host-interface","title":"EgressIP IP is assigned to the primary host interface","text":"<p>If the Egress IP(s) are hosted on the OVN primary network then the implementation is redirecting the POD traffic to an egress node where it is SNATed and sent out.  </p> <p>Using the example EgressIP and a matching pod attached to the cluster default network with <code>10.244.1.3</code> IP, the following logical router policies are configured in <code>ovn_cluster_router</code>: <pre><code>Routing Policies\n  1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-control-plane */                            reroute  10.244.0.2\n  1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker */                                          reroute  10.244.1.2\n  1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-worker2 */                                        reroute  10.244.2.2\n\n   102 (ip4.src == $a12749576804119081385 || ip4.src == $a16335301576733828072) &amp;&amp; ip4.dst == $a11079093880111560446  allow    pkt_mark=1008\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16                                                           allow\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16                                                           allow\n\n   100 ip4.src == 10.244.1.3                                                                                          reroute  100.64.0.3, 100.64.0.4\n</code></pre> - Rules with <code>1004</code> priority are responsible for redirecting <code>pod -&gt; local host IP</code> traffic. - Rules with <code>102</code> priority are added by OVN-Kubernetes when EgressIP feature is enabled, they ensure that east-west traffic is not using egress IPs. - The rule with <code>100</code> priority is added for the pod matching <code>egressip-prod</code> EgressIP, and it redirects the traffic to one of the egress nodes (ECMP is used to balance the traffic between next hops).</p> <p>For a pod attached to the cluster default network and once the redirected traffic reaches one of the egress nodes it gets SNATed in the gateway router: <pre><code>ovn-nbctl lr-nat-list GR_ovn-worker\nTYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT\n...\nsnat                                   172.18.0.33                         10.244.1.3\novn-nbctl lr-nat-list  GR_ovn-worker2\nTYPE             GATEWAY_PORT          EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT\n...\nsnat                                   172.18.0.44                         10.244.1.3\n</code></pre></p> <p>For a pod attached to a role primary user defined network \"network1\", there is no NAT entry for the pod attached to the egress OVN gateway and instead a logical router policy is attached to the egress nodes OVN gateway router: <pre><code>sh-5.2# ovn-nbctl lr-policy-list GR_network1_ovn-worker\nRouting Policies\n        95             ip4.src == 10.128.1.3 &amp;&amp; pkt.mark == 0           allow               pkt_mark=50006\n</code></pre></p>"},{"location":"features/cluster-egress-controls/egress-ip/#egressip-ip-is-assigned-to-a-secondary-host-interface","title":"EgressIP IP is assigned to a secondary host interface","text":"<p>Note that this is unsupported for user defined networks. Lets now imagine the Egress IP(s) mentioned previously, are not hosted by the OVN primary network and is hosted by a secondary host network which is assigned to a standard linux interface, a redirect to the egress-able node management port IP address: <pre><code>Routing Policies\n  1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-control-plane */                            reroute  10.244.0.2\n  1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker */                                          reroute  10.244.1.2\n  1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-worker2 */                                        reroute  10.244.2.2\n\n   102 (ip4.src == $a12749576804119081385 || ip4.src == $a16335301576733828072) &amp;&amp; ip4.dst == $a11079093880111560446  allow    pkt_mark=1008\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16                                                           allow\n   102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16                                                           allow\n\n   100 ip4.src == 10.244.1.3                                                                                          reroute  10.244.1.2, 10.244.2.2\n</code></pre></p> <p>IPTables will have the following chain in NAT table and also rules within that chain to source NAT to the correct IP address: <pre><code>sh-5.2# iptables-save \n# Generated by iptables-save v1.8.7 on Tue Jul 25 13:09:39 2023\n*mangle\n:PREROUTING ACCEPT [14087:9430205]\n:INPUT ACCEPT [13923:9397241]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [11270:1030982]\n...\n:KUBE-POSTROUTING - [0:0]\n:OVN-KUBE-EGRESS-IP-Multi-NIC - [0:0]\n:OVN-KUBE-EGRESS-SVC - [0:0]\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-IP-MULTI-NIC\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A POSTROUTING -o ovn-k8s-mp0 -j OVN-KUBE-SNAT-MGMTPORT\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\n-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -j MASQUERADE --random-fully\n...\n-A OVN-KUBE-EGRESS-IP-MULTI-NIC -s 10.244.2.3/32 -o dummy -j SNAT --to-source 10.10.10.100\n...\n-A OVN-KUBE-EGRESS-SVC -m mark --mark 0x3f0 -m comment --comment DoNotSNAT -j RETURN\n-A OVN-KUBE-SNAT-MGMTPORT -o ovn-k8s-mp0 -m comment --comment \"OVN SNAT to Management Port\" -j SNAT --to-source 10.244.2.2\nCOMMIT\n...\n</code></pre></p> <p>IPRoute2 rules will look like the following - note rule with priority <code>6000</code> and also the table <code>1111</code>: <pre><code>sh-5.2# ip rule\n0:  from all lookup local\n30: from all fwmark 0x1745ec lookup 7\n6000:   from 10.244.2.3 lookup 1111\n32766:  from all lookup main\n32767:  from all lookup default\n</code></pre></p> <p>And the default route in the correct table <code>1111</code>: <pre><code>sh-5.2# ip route show table 1111\ndefault dev dummy\n</code></pre> Routes associated with the egress interface are copied from the main routing table to the routing table that was created to support EgressIP, as shown above. If the interface is enslaved to a VRF device, routes are copied from the VRF interfaces associated routing table.</p> <p>No NAT is required on the OVN primary network gateway router. OVN-Kubernetes (ovnkube-node) takes care of adding a rule to the rule table with src IP of the pod and routed towards a new routing table specifically created to route the traffic out the correct interface. IPTables rules are also altered and an entry is created within the chain <code>OVN-KUBE-EGRESS-IP-Multi-NIC</code> for each selected pod to allow SNAT to occur when egress-ing a particular interface. The routing table number <code>1111</code> is generated from the interface name. Routes within the main routing table who's output interface share the same interface used for Egress IP are also cloned into the VRF 1111.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#layer-2-network","title":"Layer 2 network","text":"<p>Not supported</p>"},{"location":"features/cluster-egress-controls/egress-ip/#localnet","title":"Localnet","text":"<p>Not supported</p>"},{"location":"features/cluster-egress-controls/egress-ip/#pod-to-node-ip-traffic","title":"Pod to node IP traffic","text":"<p>When a cluster networked pod matched by an egress IP tries to connect to a non-local node IP it hits the following logical router policy in <code>ovn_cluster_router</code>: <pre><code># $&lt;all_eip_pod_ips&gt; - address-set of all pod IPs matched by any EgressIP\n# $&lt;all_esvc_pod_ips&gt; - address-set of all pod IPs matched by any EgressService\n# $&lt;all_node_ips&gt; - address-set of all node IPs in the cluster\n102 (ip4.src == $&lt;all_eip_pod_ips&gt; || ip4.src == $&lt;all_esvc_pod_ips&gt;) &amp;&amp; ip4.dst == $&lt;all_node_ips&gt;  allow    pkt_mark=1008\n</code></pre> In addition to simply allowing the <code>pod -&gt; node IP</code> traffic so that EgressIP reroute policies  are not matched upon, it is also marked with the 1008 mark. If a pod is hosted on an egressNode the traffic will first get SNATed to the egress IP, and then it will hit  following flow on breth0 that will SNAT the traffic to local node IP: <pre><code># output truncated, 0x3f0 == 1008\npriority=105,pkt_mark=0x3f0,ip,in_port=2 actions=ct(commit,zone=64000,nat(src=&lt;NodeIP&gt;),exec(load:0x1-&gt;NXM_NX_CT_MARK[])),output:1\n</code></pre> This is required to make <code>pod -&gt; node IP</code> traffic behave the same regardless of where the pod is hosted. Implementation details: https://github.com/ovn-org/ovn-kubernetes/commit/e2c981a42a28e6213d9daf3b4489c18dc2b84b19.</p> <p>For local gateway mode, in which an Egress IP is assigned to a non-primary interface, an IP rule is added to send packets to the main routing table at a priority higher than that of EgressIP IP rules, which are set to priority <code>6000</code>: <pre><code>5999:   from all fwmark 0x3f0 lookup main\n</code></pre> Note: <code>0x3f0</code> is <code>1008</code> in hexadecimal. Lower IP rule priority number indicates higher precedence versus higher IP rule priority number.</p> <p>This ensures all traffic to node IPs will not be selected by EgressIP IP rules. However, reply traffic will not have the mark <code>1008</code> and would be dropped by reverse path filtering, therefore we add an IPTable rule to the mangle table to save and restore the <code>1008</code> mark: <pre><code>sh-5.2# iptables -t mangle -L  PREROUTING\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nCONNMARK   all  --  anywhere             anywhere             mark match 0x3f0 CONNMARK save\nCONNMARK   all  --  anywhere             anywhere             mark match 0x0 CONNMARK restore\n</code></pre></p>"},{"location":"features/cluster-egress-controls/egress-ip/#dealing-with-non-snated-traffic","title":"Dealing with non SNATed traffic","text":"<p>Egress IP is often configured on a node different from the one hosting the affected pods. Due to the fact that ovn-controllers on different nodes apply the changes independently, there is a chance that some pod traffic will reach the egress node before it configures the SNAT rules. The following flows are added on breth0 to address this scenario: <pre><code># Output traffic from local pods so they are not affected by the drop rule below, this is required for ICNIv2 and advertised UDNs\npriority=104,ip,in_port=2,nw_src=&lt;nodeSubnet&gt; actions=output:eth0\n\n# Drop non SNATed egress traffic coming from non-local pods\npriority=103,ip,in_port=2,nw_src=&lt;clusterSubnet&gt; actions=drop\n\n# Commit connections coming from IPs not in cluster network\npriority=100,ip,in_port=2 actions=ct(commit,zone=64000,exec(set_field:0x1-&gt;ct_mark)),output:1\n</code></pre></p>"},{"location":"features/cluster-egress-controls/egress-ip/#special-considerations-for-egress-ips-hosted-by-standard-linux-interfaces","title":"Special considerations for Egress IPs hosted by standard linux interfaces","text":"<p>If you wish to assign an Egress IP to a standard linux interface (non OVS type), then the following is required: * Link is up * IP address must have scope universe / global * Links and their addresses must not be removed during runtime after an egress IP is assigned to it. If you wish to remove the link, first remove the Egress IP and then remove the address / link. * IP forwarding must be enabled for the link</p>"},{"location":"features/cluster-egress-controls/egress-ip/#egress-nodes","title":"Egress Nodes","text":"<p>In order to select which node(s) may be used as egress, the following label must be added to the <code>node</code> resource:</p> <pre><code>kubectl label nodes &lt;node_name&gt; k8s.ovn.org/egress-assignable=\"\"\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-ip/#egress-ip-reachability","title":"Egress IP reachability","text":"<p>Once a node has been labeled with <code>k8s.ovn.org/egress-assignable</code>, the EgressIP operator in the leader ovnkube-master pod will periodically check if that node is usable. EgressIPs assigned to a node that is no longer reachable will get revalidated and moved to another useable node.</p> <p>Egress nodes normally have multiple IP addresses. For sake of Egress IP reachability, the management (aka internal SDN) addresses of the node are the ones used. In deployments of ovn-kubernetes this is known to be the <code>ovn-k8s-mp0</code> interface of a node.</p> <p>Even though the periodic checking of egress nodes is hard coded to trigger every 5 seconds, there are attributes that the user can set:</p> <ul> <li>egressIPTotalTimeout</li> <li>gRPC vs. DISCARD port</li> </ul>"},{"location":"features/cluster-egress-controls/egress-ip/#egressiptotaltimeout","title":"egressIPTotalTimeout","text":"<p>This attribute specifies the maximum amount of time, in seconds, that the egressIP operator will wait until it declares the node unreachable. The default value is 1 second.</p> <p>This value can be set in the following ways: - ovnkube binary flag: <code>--egressip-reachability-total-timeout=&lt;TIMEOUT&gt;</code> - inside config specified by <code>--config-file</code> flag: <pre><code>[ovnkubernetesfeature]\negressip-reachability-total-timeout=123\n</code></pre></p> <p>Note: Using value <code>0</code> will skip reachability. Use this to assume that egress nodes are available.</p>"},{"location":"features/cluster-egress-controls/egress-ip/#grpc-vs-discard-port","title":"gRPC vs. DISCARD port","text":"<p>Up until recently, the only method available for determining if an egress node was reachable relied on the <code>TCP port unreachable</code> icmp response from the probed node. The TCP port 9 (aka DISCARD) is the port used for that.</p> <p>Later implementation of ovn-kubernetes is capable of leveraging secure gRPC sessions in order to probe nodes. That requires the <code>ovnkube node</code> pods to be listening on a pre-specified TCP port, in addition to configuring the <code>ovnkube master</code> pod(s).</p> <p>This value can be set in the following ways: - ovnkube binary flag: <code>--egressip-node-healthcheck-port=&lt;TCP_PORT&gt;</code> - inside config specified by <code>--config-file</code> flag: <pre><code>[ovnkubernetesfeature]\negressip-node-healthcheck-port=9107\n</code></pre></p> <p>Note: If not specifying a value, or using <code>0</code> as the <code>egressip-node-healthcheck-port</code> will make Egress IP reachability probe the egress nodes using the DISCARD port method. Unlike egressip-reachability-total-timeout, it is important that both node and master pods of ovnkube get configured with the same value!</p>"},{"location":"features/cluster-egress-controls/egress-ip/#additional-details-on-the-implementation-of-the-grpc-probing","title":"Additional details on the implementation of the gRPC probing:","text":"<ul> <li>If available, the session uses the same TLS certs used by ovnkube to connect to the northbound OVSDB server. Conversely, an insecure gRPC session is used when no certs are specified.</li> <li>The message used for probing is the standard service health specified in gRPC.</li> <li>Special care was taken into consideration to handle cases when the gRPC session bounced for normal reasons. EgressIP implementation will not declare a node unreachable under these circumstances.</li> </ul>"},{"location":"features/cluster-egress-controls/egress-qos/","title":"EgressQoS","text":""},{"location":"features/cluster-egress-controls/egress-qos/#introduction","title":"Introduction","text":"<p>The EgressQoS feature enables marking pods egress traffic with a valid QoS Differentiated Services Code Point (DSCP) value. The QoS markings will be consumed and acted upon by network appliances outside of the Kubernetes cluster to optimize traffic flow throughout their networks.</p> <p>The EgressQoS resource is namespaced-scoped and allows specifying a set of QoS rules - each has a DSCP value, an optional destination CIDR (dstCIDR) and an optional PodSelector (podSelector). A rule applies its DSCP marking to traffic coming from pods whose labels match the podSelector heading to the dstCIDR. A namespace supports having only one EgressQoS resource named <code>default</code> (other EgressQoSes will be ignored).</p>"},{"location":"features/cluster-egress-controls/egress-qos/#example","title":"Example","text":"<pre><code>kind: EgressQoS\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - dscp: 30\n    dstCIDR: 1.2.3.0/24\n  - dscp: 42\n    podSelector:\n      matchLabels:\n        app: example\n  - dscp: 28\n</code></pre> <p>This example marks the packets originating from pods in the <code>default</code> namespace in the following way: * All traffic heading to an address that belongs to 1.2.3.0/24 is marked with DSCP 30. * Egress traffic from pods labeled <code>app: example</code> is marked with DSCP 42. * All egress traffic is marked with DSCP 28.</p> <p>The priority of a rule is determined by its placement in the egress array. An earlier rule is processed before a later rule. In this example, if the rules are reversed all traffic originating from pods in the <code>default</code> namespace is marked with DSCP 28 - regardless of its destination or pods labels. Because of that specific rules should always come before general ones in that array.</p>"},{"location":"features/cluster-egress-controls/egress-qos/#changes-in-ovn-northbound-database","title":"Changes in OVN northbound database","text":"<p>EgressQoS is implemented by reacting to events from <code>EgressQoSes</code>, <code>Pods</code> and <code>Nodes</code> changes - updating OVN's northbound database <code>QoS</code>, <code>Address_Set</code> and <code>Logical_Switch</code> objects. The code is implemented under <code>pkg/ovn/egressqos.go</code> and most of the logic for the events sits under the <code>syncEgressQoS</code>, <code>syncEgressQoSPod</code>, <code>syncEgressQoSNode</code> functions.</p> <p>We'll use the example YAML above and see how the related objects are changed in OVN's northbound database in a Dual-Stack kind cluster.</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES               \novn-control-plane   Ready    control-plane,master\novn-worker          Ready    &lt;none&gt;              \novn-worker2         Ready    &lt;none&gt;              \n</code></pre> <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels1   [map[ip:10.244.2.3] map[ip:fd00:10:244:3::3]]   map[app:example]\n</code></pre> <pre><code>$ kubectl get egressqoses\nNo resources found in default namespace.\n</code></pre> <p>At this point there are no QoS objects in the northbound database. We create the example EgressQoS: <pre><code>$ kubectl get egressqoses\nNAME      AGE\ndefault   6s\n</code></pre></p> <p>And the following is created in the northbound database (<code>SyncEgressQoS</code>): <pre><code># QoS\n\n_uuid               : 14b923a1-d7b0-42b8-a3d7-6a5028b09ae2\naction              : {dscp=30}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 1.2.3.0/24) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 1000\n\n_uuid               : 820a011d-0eda-43b7-994d-46a55620c4bf\naction              : {dscp=42}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a10759091379580272948 || ip6.src == $a10759093578603529370)\"\npriority            : 999\n\n_uuid               : 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67\naction              : {dscp=28}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 998\n</code></pre></p> <p>A QoS object is created for each rule specified in the EgressQoS, all attached to all of the nodes logical switches: <pre><code># Logical_Switch\n\nname                : ovn-worker\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n\nname                : ovn-worker2\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n\nname                : ovn-control-plane\nqos_rules           : [14b923a1-d7b0-42b8-a3d7-6a5028b09ae2, 1e35ea19-3353-4cbc-a1f5-7ea5bf831d67, 820a011d-0eda-43b7-994d-46a55620c4bf]\n</code></pre></p> <p>When a new node is added to the cluster we attach all of the <code>QoS</code> objects that belong to EgressQoSes to its logical switch as well (<code>SyncEgressQoSNode</code>).</p> <p>Notice that QoS objects that belong to rules without a <code>podSelector</code> reference the same address sets for their src match - which are the namespace's address sets: <pre><code># Address_Set\n\n_uuid               : 7cb04096-7107-4a35-ae34-be4b0e0c584e\naddresses           : [\"10.244.1.3\", \"10.244.2.3\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 467a93c9-9c76-40bf-8318-e91ee7f6f010\naddresses           : [\"fd00:10:244:2::3\", \"fd00:10:244:3::3\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre> We only use these address sets without editing them.</p> <p>For each rule that does have a <code>podSelector</code> an address set is created, adding the pods that match to it. Its name (external_ids) follows the pattern <code>egress-qos-pods-&lt;rule-namespace&gt;-&lt;rule-priority&gt;</code>, rule-priority is <code>1000 - rule's index in the array</code> which matches the QoS object's priority that relates to that rule. <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : [\"fd00:10:244:3::3\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : [\"10.244.2.3\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre></p> <p>When a new pod that matches a rule's <code>podSelector</code> is created in the namespace we add its IPs to the relevant address set (<code>SyncEgressQoSPod</code>): <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels1   [map[ip:10.244.2.3] map[ip:fd00:10:244:3::3]]   map[app:example]\nwith-labels2   [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:example]\n</code></pre></p> <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : [\"fd00:10:244:3::3\", \"fd00:10:244:3::4\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n--\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : [\"10.244.2.3\", \"10.244.2.4\"]\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre> <p>When a pod is deleted or its labels change and do not longer match the rule's <code>podSelector</code> its IPs are deleted from the relevant address set: <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName           IPs                                             LABELS\nno-labels      [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels2   [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:not-the-example]\n</code></pre></p> <pre><code># Address_Set\n\n_uuid               : 74b8df1e-5a2d-4fa4-a5e5-57bdea61d0c5\naddresses           : []\nexternal_ids        : {name=egress-qos-pods-default-999_v6}\nname                : a10759093578603529370\n--\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : []\nexternal_ids        : {name=egress-qos-pods-default-999_v4}\nname                : a10759091379580272948\n</code></pre> <p>When an EgressQoS is updated - we recreate the QoS and Address Set objects, detach the old QoSes from the logical switches and attach the new ones:</p> <pre><code>kind: EgressQoS\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - dscp: 48\n    podSelector:\n      matchLabels:\n        app: updated-example\n  - dscp: 28\n</code></pre> <pre><code>$ kubectl get pods -o=custom-columns=Name:.metadata.name,IPs:.status.podIPs,LABELS:.metadata.labels\nName                  IPs                                             LABELS\nno-labels             [map[ip:10.244.1.3] map[ip:fd00:10:244:2::3]]   &lt;none&gt;\nwith-labels2          [map[ip:10.244.2.4] map[ip:fd00:10:244:3::4]]   map[app:not-the-example]\nwith-updated-labels   [map[ip:10.244.1.4] map[ip:fd00:10:244:2::4]]   map[app:updated-example]\n</code></pre> <pre><code># QoS\n\n_uuid               : cf84322a-0b9e-4aef-97bb-4dcd13f0e73d\naction              : {dscp=48}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a17475935309050627288 || ip6.src == $a17475937508073883710)\"\npriority            : 1000\n\n_uuid               : 8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8\naction              : {dscp=28}\nbandwidth           : {}\ndirection           : to-lport\nexternal_ids        : {EgressQoS=default}\nmatch               : \"(ip4.dst == 0.0.0.0/0 || ip6.dst == ::/0) &amp;&amp; (ip4.src == $a5154718082306775057 || ip6.src == $a5154715883283518635)\"\npriority            : 999\n</code></pre> <pre><code># Logical_Switch\n\nname                : ovn-worker\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n\nname                : ovn-worker2\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n\nname                : ovn-control-plane\nqos_rules           : [8bf07e6b-ca0a-4d76-a206-b7ccb5e948d8, cf84322a-0b9e-4aef-97bb-4dcd13f0e73d]\n</code></pre> <pre><code># Address_Set\n\n_uuid               : a27cf559-0981-487f-a0ca-5a355da89cba\naddresses           : [\"10.244.1.4\"]\nexternal_ids        : {name=egress-qos-pods-default-1000_v4}\nname                : a17475935309050627288\n\n_uuid               : 93cdc1fd-995c-498b-88a1-4992af93c630\naddresses           : [\"fd00:10:244:2::4\"]\nexternal_ids        : {name=egress-qos-pods-default-1000_v6}\nname                : a17475937508073883710\n\n_uuid               : 7cb04096-7107-4a35-ae34-be4b0e0c584e\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.4\"]\nexternal_ids        : {name=default_v4}\nname                : a5154718082306775057\n\n_uuid               : 467a93c9-9c76-40bf-8318-e91ee7f6f010\naddresses           : [\"fd00:10:244:2::3\", \"fd00:10:244:2::4\", \"fd00:10:244:3::4\"]\nexternal_ids        : {name=default_v6}\nname                : a5154715883283518635\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-service/","title":"Egress Service","text":""},{"location":"features/cluster-egress-controls/egress-service/#introduction","title":"Introduction","text":"<p>The Egress Service feature enables the egress traffic of pods backing a LoadBalancer service to use a different network than the main one and/or their source IP to be the Service's ingress IP. This is useful for external systems that communicate with applications running on the Kubernetes cluster through a LoadBalancer service and expect that the source IP of egress traffic originating from the pods backing the service is identical to the destination IP they use to reach them - i.e the LoadBalancer's ingress IP. In addition, this allows to separate the egress traffic of specified LoadBalancer services by different networks (VRFs).</p> <p>By introducing a new CRD <code>EgressService</code>, users could request that egress packets originating from all of the pods that are endpoints of a LoadBalancer service would use a different network than the main one and/or their source IP will be the Service's ingress IP. The CRD is namespace scoped. The name of the EgressService corresponds to the name of a LoadBalancer Service that should be affected by this functionality. Note the mapping of EgressService to Kubernetes Service is 1to1. The feature is supported fully by \"Local\" gateway mode and almost entirely by \"Shared\" gateway mode (it does not support Network without LoadBalancer SNAT. In any case the affected traffic will be that which is coming from a pod to a destination outside of the cluster - meaning pod-pod / pod-service / pod-node traffic will not be affected.</p> <p>Announcing the service externally (for ingress traffic) is handled by a LoadBalancer provider (like MetalLB) and not by OVN-Kubernetes as explained later.</p>"},{"location":"features/cluster-egress-controls/egress-service/#details-modifying-the-source-ip-of-the-egress-packets","title":"Details - Modifying the source IP of the egress packets","text":"<p>Only SNATing a pod's IP to the LoadBalancer service ingress IP that it is backing is problematic, as usually the ingress IP is exposed via multiple nodes by the LoadBalancer provider. This means we can't just add an SNAT to the regular traffic flow of a pod before it exits its node because we don't have a guarantee that the reply will come back to the pod's node (where the traffic originated). An external client usually has multiple paths to reach the LoadBalancer ingress IP and could reply to a node that is not the pod's node - in that case the other node does not have the proper CONNTRACK entries to send the reply back to the pod and the traffic is lost. For that reason, we need to make sure that all traffic for the service's pods (ingress/egress) is handled by a single node so the right CONNTRACK entries are always matched and the traffic is not lost.</p> <p>The egress part is handled by OVN-Kubernetes, which chooses a node that acts as the point of ingress/egress, and steers the relevant pods' egress traffic to its mgmt port, by using logical router policies on the <code>ovn_cluster_router</code>. When that traffic reaches the node's mgmt port it will use its routing table and iptables before heading out. Because of that, it takes care of adding the necessary iptables rules on the selected node to SNAT traffic exiting from these pods to the service's ingress IP.</p> <p>These goals are achieved by introducing a new resource <code>EgressService</code> for users to create alongside LoadBalancer services with the following fields: - <code>sourceIPBy</code>: Determines the source IP of egress traffic originating from the pods backing the Service. When \"LoadBalancerIP\" the source IP is set to the Service's LoadBalancer ingress IP. When \"Network\" the source IP is set according to the interface of the Network, leveraging the masquerade rules that are already in place. Typically these rules specify SNAT to the IP of the outgoing interface, which means the packet will typically leave with the IP of the node.</p> <p><code>nodeSelector</code>: Allows limiting the nodes that can be selected to handle the service's traffic when sourceIPBy: \"LoadBalancerIP\". When present only a node whose labels match the specified selectors can be selected for handling the service's traffic as explained earlier. When the field is not specified any node in the cluster can be chosen to manage the service's traffic. In addition, if the service's <code>ExternalTrafficPolicy</code> is set to <code>Local</code> an additional constraint is added that only a node that has an endpoint can be selected - this is important as otherwise new ingress traffic will not work properly if there are no local endpoints on the host to forward to. This also means that when \"ETP=Local\" only endpoints local to the selected host will be used for ingress traffic and other endpoints will not be used.</p> <ul> <li><code>network</code>: The network which this service should send egress and corresponding ingress replies to. This is typically implemented as VRF mapping, representing a numeric id or string name of a routing table which by omission uses the default host routing.</li> </ul> <p>When a node is selected to handle the service's traffic both the status of the relevant <code>EgressService</code> is updated with <code>host: &lt;node_name&gt;</code> (which is consumed by <code>ovnkube-node</code>) and the node is labeled with <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code>, which can be consumed by a LoadBalancer provider to handle the ingress part.</p> <p>Similarly to the EgressIP feature, once a node is selected it is checked for readiness (TCP/gRPC) to serve traffic every x seconds. If a node fails the health check, its allocated services move to another node by removing the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label from it, removing the logical router policies from the cluster router, resetting the status of the relevant <code>EgressServices</code> and requeuing them - causing a new node to be selected for the services. If the node becomes not ready or its labels no longer match the service's selectors the same re-election process happens.</p> <p>The ingress part is handled by a LoadBalancer provider, such as MetalLB, that needs to select the right node (and only it) for announcing the LoadBalancer service (ingress traffic) according to the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label set by OVN-Kubernetes. A full example with MetalLB is detailed in Usage Example.</p> <p>Just to be clear, OVN-Kubernetes does not care which component advertises the LoadBalancer service or checks if it does it correctly - it is the user's responsibility to make sure ingress traffic arrives only to the node with the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label.</p> <p>Assuming an Egress Service has <code>172.19.0.100</code> as its ingress IP and <code>ovn-worker</code> selected to handle all of its traffic, the egress traffic flow of an endpoint pod with the ip <code>10.244.1.6</code> on <code>ovn-worker2</code> towards an external destination (172.19.0.5) will look like: <pre><code>                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502                    \u2502\n                     \u2502external destination\u2502\n                     \u2502    172.19.0.5      \u2502\n                     \u2502                    \u2502\n                     \u2514\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502\n     5. packet reaches   \u2502                      2. router policy rereoutes it\n        the external     \u2502                         to ovn-worker's mgmt port\n        destination with \u2502                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        src ip:          \u2502                  \u250c\u2500\u2500\u2500\u2524ovn cluster router\u2502\n        172.19.0.100     \u2502                  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                         \u2502                  \u2502               \u2502\n                         \u2502                  \u2502               \u25021. packet to 172.19.0.5\n                      \u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2510              \u2502   heads to the cluster router\n                   \u250c\u2500\u2500\u2518 eth1 \u2514\u2500\u2500\u2510  \u250c\u2500\u2500\u2518 mgmt \u2514\u2500\u2500\u2510           \u2502   as usual\n                   \u2502 172.19.0.2 \u2502  \u2502 10.244.0.2 \u2502           \u2502\n                   \u251c\u2500\u2500\u2500\u2500\u2500\u25b2\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n4. an iptables rule\u2502     \u2502   ovn-worker  \u25023.    \u2502           \u2502   \u2502  ovn-worker2   \u2502\n   that SNATs to   \u2502     \u2502               \u2502      \u2502           \u2502   \u2502                \u2502\n   the service's ip\u2502     \u2502               \u2502      \u2502           \u2502   \u2502                \u2502\n   is hit          \u2502     \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502      \u2502           \u2502   \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n                   \u2502     \u25024.\u2502routes +\u2502   \u2502      \u2502           \u2514\u2500\u2500\u2500\u253c\u2500\u2524    pod     \u2502 \u2502\n                   \u2502     \u2514\u2500\u2500\u2524iptables\u25c4\u2500\u2500\u2500\u2518      \u2502               \u2502 \u2502 10.244.1.6 \u2502 \u2502\n                   \u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502               \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n                   \u2502                            \u2502               \u2502                \u2502\n                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                3. from the mgmt port it hits ovn-worker's\n                   routes and iptables rules\n</code></pre> Notice how the packet exits <code>ovn-worker</code>'s eth1 and not breth0, as the packet goes through the host's routing table regardless of the gateway mode.</p> <p>When <code>sourceIPBy: \"Network\"</code> is set, <code>ovnkube-master</code> does not need to create any logical router policies because the egress packets of each pod would exit through the pod's node but will set the status field of the resource with <code>host: ALL</code> as decribed later.</p>"},{"location":"features/cluster-egress-controls/egress-service/#network","title":"Network","text":"<p>The <code>EgressService</code> supports a <code>network</code> field to specify to which network the egress traffic of the service should be steered to. When it is specified the relevant <code>ovnkube-nodes</code> take care of creating ip rules on their host - either the node which matches <code>Status.Host</code> or all of the nodes when <code>Status.Host</code> is \"ALL\". Assuming an <code>EgressService</code> has <code>Network: blue</code>, a ClusterIP of 10.96.135.5 and its endpoints are 10.244.0.3 and 10.244.1.6 the following will be created on the host:</p> <pre><code>$ ip rule list\n5000:   from 10.96.135.5 lookup blue\n5000:   from 10.244.0.3 lookup blue\n5000:   from 10.244.1.6 lookup blue\n</code></pre> <p>This makes the egress traffic of endpoints of an EgressService to be routed via the \"blue\" routing table. An ip rule is also created for the ClusterIP of the service which is needed in order for the ingress reply traffic (reply to an external client calling the service) to use the correct table - this is because the packet flow of contacting a LoadBalancer service goes: <code>lb ip -&gt; node -&gt; enter ovn with ClusterIP -&gt; exit ovn with ClusterIP -&gt; exit node with lb ip</code> so we need to make sure that packets from ClusterIPs are marked before being routed in order for them to hit the relevant ip rule in time.</p>"},{"location":"features/cluster-egress-controls/egress-service/#network-without-loadbalancer-snat","title":"Network without LoadBalancer SNAT","text":"<p>As mentioned earlier, it is possible to use the \"Network\" capability without SNATing the traffic to the service's ingress IP. This is done by creating an EgressService with the <code>Network</code> field specified and <code>sourceIPBy: \"Network\"</code>.</p> <p>An EgressService with <code>sourceIPBy: \"Network\"</code> does not need to have a host selected, as the traffic will exit each node with the IP of the interface corresponding to the \"Network\" by leveraging the masquerade rules that are already in place.</p> <p>This works only on clusters running on \"Local\" gateway mode, because on \"Shared\" gateway mode the ip rules created by the controller are ignored (like all the node's routing stack).</p> <p>When <code>sourceIPBy: \"Network\"</code>, <code>ovnkube-master</code> does not need to create any logical router policies as the egress packets of each pod would exit through the pod's node. However, <code>ovnkube-master</code> will set the status field of the resource with <code>host: ALL</code> to designate that no reroute logical router policies exist for the service, \"instructing\" all of the <code>ovnkube-nodes</code> to handle the resource's <code>Network</code> field without creating SNAT iptables rules.</p> <p>When <code>ovnkube-node</code> detects that the host of an EgressService is <code>ALL</code>, only the endpoints local to the node will have an ip rule created, and no SNAT iptables rules will be created.</p> <p>It is the user's responsibility to make sure that the pods backing an EgressService without SNAT run only on nodes that have the required \"Network\", as no additional steering (lrps) will take place by OVN and pods running on nodes without a correct \"Network\" will misbehave.</p>"},{"location":"features/cluster-egress-controls/egress-service/#changes-in-ovn-northbound-database-and-iptables","title":"Changes in OVN northbound database and iptables","text":"<p>The feature is implemented by reacting to events from <code>EgressServices</code>, <code>Services</code>, <code>EndpointSlices</code> and <code>Nodes</code> changes - updating OVN's northbound database <code>Logical_Router_Policy</code> objects to steer the traffic to the selected node and creating iptables SNAT rules in its <code>OVN-KUBE-EGRESS-SVC</code> chain, which is called by the POSTROUTING chain of its nat table.</p> <p>We'll see how the related objects are changed once a LoadBalancer is requested to act as an \"Egress Service\" by creating a corresponding <code>EgressService</code> named after it in a Dual-Stack kind cluster.</p> <p>We start with a clean cluster: <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES\novn-control-plane   Ready    control-plane\novn-worker          Ready    worker\novn-worker2         Ready    worker\n</code></pre></p> <pre><code>$ kubectl describe svc demo-svc\nName:                     demo-svc\nNamespace:                default\nType:                     LoadBalancer\nLoadBalancer Ingress:     5.5.5.5, 5555:5555:5555:5555:5555:5555:5555:5555\nEndpoints:                10.244.0.5:8080,10.244.2.7:8080\n                          fd00:10:244:1::5,fd00:10:244:3::7\n</code></pre> <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-control-plane */         reroute                10.244.2.2\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3 /* ovn-control-plane */         reroute          fd00:10:244:3::2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-worker */         reroute                10.244.0.2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4 /* ovn-worker */         reroute          fd00:10:244:1::2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker2 */         reroute                10.244.1.2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2 /* ovn-worker2 */         reroute          fd00:10:244:2::2\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.2/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.3/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.4/32           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd00:10:244::/48           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd98::/64           allow\n</code></pre> <p>At this point nothing related to Egress Services is in place. It is worth noting that the \"allow\" policies (102's) that make sure east-west traffic is not affected for EgressIPs are present here as well - if the EgressIP feature is enabled it takes care of creating them, otherwise the \"Egress Service\" feature does (sharing the same logic), as we do not want Egress Services to change the behavior of east-west traffic. Also, the policies created (seen later) for an Egress Service use a higher priority than the EgressIP ones, which means that if a pod belongs to both an EgressIP and an Egress Service the service's ingress IP will be used for the SNAT.</p> <p>We now request that our service \"demo-svc\" will act as an \"Egress Service\" by creating a corresponding <code>EgressService</code>, with the constraint that only a node with the <code>\"node-role.kubernetes.io/worker\": \"\"</code> label can be selected to handle its traffic: <pre><code>$ cat egress-service.yaml\napiVersion: k8s.ovn.org/v1\nkind: EgressService\nmetadata:\n  name: demo-svc\n  namespace: default\nspec:\n  sourceIPBy: \"LoadBalancerIP\"\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/worker: \"\"\n\n$ kubectl apply -f egress-service.yaml\negressservice.k8s.ovn.org/demo-svc created\n</code></pre></p> <p>Once the <code>EgressService</code> is created a node is selected to handle all of its traffic (ingress/egress) as described earlier. The <code>EgressService</code> status is updated with its name, logical router policies are created on ovn_cluster_router to steer the endpoints' traffic to its mgmt port, SNAT rules are created in its iptables and it is labeled as the node in charge of the service's traffic:</p> <p>The status points to <code>ovn-worker2</code>, meaning it was selected to handle the service's traffic: <pre><code>$ kubectl describe egressservice demo-svc\nName:         demo-svc\nNamespace:    default\nSpec:\n    Source IP By:                        LoadBalancerIP\n    Node Selector:\n      Match Labels:\n        node-role.kubernetes.io/worker:  \"\"\nStatus:\n  Host:  ovn-worker2\n</code></pre></p> <p>A logical router policy is created for each endpoint to steer its egress traffic towards <code>ovn-worker2</code>'s mgmt port: <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n       &lt;truncated 1004's and 102's&gt;\n       101                              ip4.src == 10.244.0.5         reroute                10.244.1.2\n       101                              ip4.src == 10.244.2.7         reroute                10.244.1.2\n       101                        ip6.src == fd00:10:244:1::5         reroute          fd00:10:244:2::2\n       101                        ip6.src == fd00:10:244:3::7         reroute          fd00:10:244:2::2\n</code></pre></p> <p>An SNAT rule to the service's ingress IP is created for each endpoint: <pre><code>$ hostname\novn-worker2\n\n$ iptables-save\n*nat\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s 10.244.0.5/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n-A OVN-KUBE-EGRESS-SVC -s 10.244.2.7/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n...\n\n$ ip6tables-save\n...\n*nat\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:3::7/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:1::5/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n...\n</code></pre></p> <p><code>ovn-worker2</code> is the only node holding the <code>egress-service.k8s.ovn.org/default-demo-svc=\"\"</code> label: <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNAME          STATUS   ROLES\novn-worker2   Ready    worker\n</code></pre></p> <p>When the endpoints of the service change, the logical router policies and iptables rules are changed accordingly.</p> <p>We will now simulate a failover of the service when a node fails its health check. By stopping <code>ovn-worker2</code>'s container we see that all of the resources \"jump\" to <code>ovn-worker</code>, as it is the only node left matching the <code>nodeSelector</code>:</p> <pre><code>$ docker stop ovn-worker2\novn-worker2\n</code></pre> <p>The status now points to <code>ovn-worker</code>: <pre><code>$ kubectl describe egressservice demo-svc\nName:         demo-svc\nNamespace:    default\nSpec:\n    Source IP By:                        LoadBalancerIP\n    Node Selector:\n      Match Labels:\n        node-role.kubernetes.io/worker:  \"\"\nStatus:\n  Host:  ovn-worker\n</code></pre></p> <p>The reroute destination changed to <code>ovn-worker</code>'s mgmt port (10.244.1.2 -&gt; 10.244.0.2, fd00:10:244:2::2 -&gt; fd00:10:244:1::2): <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n       &lt;truncated 1004's and 102's&gt;\n       101                              ip4.src == 10.244.0.5         reroute                10.244.0.2\n       101                              ip4.src == 10.244.2.7         reroute                10.244.0.2\n       101                        ip6.src == fd00:10:244:1::5         reroute          fd00:10:244:1::2\n       101                        ip6.src == fd00:10:244:3::7         reroute          fd00:10:244:1::2\n</code></pre></p> <p>The iptables rules were created on <code>ovn-worker</code>: <pre><code>$ hostname\novn-worker\n\n$ iptables-save\n*nat\n...\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s 10.244.0.5/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n-A OVN-KUBE-EGRESS-SVC -s 10.244.2.7/32 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5.5.5.5\n...\n\n$ ip6tables-save\n...\n*nat\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:3::7/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n-A OVN-KUBE-EGRESS-SVC -s fd00:10:244:1::5/128 -m comment --comment \"default/demo-svc\" -j SNAT --to-source 5555:5555:5555:5555:5555:5555:5555:5555\n...\n</code></pre></p> <p>The label moved to <code>ovn-worker</code>: <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNAME         STATUS   ROLES\novn-worker   Ready    worker\n</code></pre></p> <p>Finally, deleting the <code>EgressService</code> resource resets the cluster to the point we started from: <pre><code>$ kubectl delete egressservice demo-svc\negressservice.k8s.ovn.org \"demo-svc\" deleted\n</code></pre></p> <pre><code>$ ovn-nbctl lr-policy-list ovn_cluster_router\nRouting Policies\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip4.dst == 172.18.0.3 /* ovn-control-plane */         reroute                10.244.2.2\n      1004 inport == \"rtos-ovn-control-plane\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3 /* ovn-control-plane */         reroute          fd00:10:244:3::2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip4.dst == 172.18.0.4 /* ovn-worker */         reroute                10.244.0.2\n      1004 inport == \"rtos-ovn-worker\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4 /* ovn-worker */         reroute          fd00:10:244:1::2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip4.dst == 172.18.0.2 /* ovn-worker2 */         reroute                10.244.1.2\n      1004 inport == \"rtos-ovn-worker2\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2 /* ovn-worker2 */         reroute          fd00:10:244:2::2\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.2/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.3/32           allow\n       102 ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 172.18.0.4/32           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::2/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fc00:f853:ccd:e793::4/128           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd00:10:244::/48           allow\n       102 ip6.src == fd00:10:244::/48 &amp;&amp; ip6.dst == fd98::/64           allow\n</code></pre> <pre><code>$ hostname\novn-worker\n\n$ iptables-save | grep EGRESS\n:OVN-KUBE-EGRESS-SVC - [0:0]\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n\n$ ip6tables-save | grep EGRESS\n:OVN-KUBE-EGRESS-SVC - [0:0]\n-A POSTROUTING -j OVN-KUBE-EGRESS-SVC\n</code></pre> <pre><code>$ kubectl get nodes -l egress-service.k8s.ovn.org/default-demo-svc=\"\"\nNo resources found\n</code></pre>"},{"location":"features/cluster-egress-controls/egress-service/#tbd-dealing-with-non-snated-traffic","title":"TBD: Dealing with non SNATed traffic","text":"<p>The host of an Egress Service is often in charge of pods (endpoints) that run in different nodes. Due to the fact that ovn-controllers on different nodes apply the changes independently, there is a chance that some pod traffic will reach the host before it configures the relevant SNAT iptables rules. In that timeframe, the egress traffic from these pods will exit the host with their ip instead of the LB's ingress ip, and the it will not be able to return properly because an external client is not aware of a pod's inner ip.</p> <p>This is currently a known issue for EgressService because we can't leverage the same as EgressIP currently does by setting a flow on breth0 - the flow won't be hit because the traffic \"exits\" OVN when using EgressService (= doesn't hit the host's breth0) as opposed to how EgressIP \"keeps everything\" inside OVN.</p>"},{"location":"features/cluster-egress-controls/egress-service/#usage-example","title":"Usage Example","text":"<p>While the user does not need to know all of the details of how \"Egress Services\" work, they need to know that in order for a service to work properly the access to it from outside the cluster (ingress traffic) has to go only through the node labeled with the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label - i.e the node designated by OVN-Kubernetes to handle all of the service's traffic. As mentioned earlier, OVN-Kubernetes does not care which component advertises the LoadBalancer service or checks if it does it correctly.</p> <p>Here we look at an example of \"Egress Services\" using MetalLB to advertise the LoadBalancer service externally. A user of MetalLB can follow these steps to create a LoadBalancer service whose endpoints exit the cluster with its ingress IP. We already assume MetalLB's <code>BGPPeers</code> are configured and the sessions are established.</p> <ol> <li> <p>Create the IPAddressPool with the desired IP for the service. It makes sense to set <code>autoAssign: false</code> so it is not taken by another service by mistake - our service will request that pool explicitly.  <pre><code>apiVersion: metallb.io/v1beta1\nkind: IPAddressPool\nmetadata:\n  name: example-pool\n  namespace: metallb-system\nspec:\n  addresses:\n  - 172.19.0.100/32\n  autoAssign: false\n</code></pre></p> </li> <li> <p>Create the LoadBalancer service and the corresponding EgressService. We create the service with the <code>metallb.universe.tf/address-pool</code> annotation to explicitly request its IP to be from the <code>example-pool</code> and the EgressService with a <code>nodeSelector</code> so that the traffic exits from a node that matches these selectors. <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: example-service\n  namespace: some-namespace\n  annotations:\n    metallb.universe.tf/address-pool: example-pool\nspec:\n  selector:\n    app: example\n  ports:\n    - name: http\n      protocol: TCP\n      port: 8080\n      targetPort: 8080\n  type: LoadBalancer\n---\napiVersion: k8s.ovn.org/v1\nkind: EgressService\nmetadata:\n  name: example-service\n  namespace: some-namespace\nspec:\n  sourceIPBy: \"LoadBalancerIP\"\n  nodeSelector:\n    matchLabels:\n      node-role.kubernetes.io/worker: \"\"\n</code></pre></p> </li> <li> <p>Advertise the service from the node in charge of the service's traffic. So far the service is \"broken\" - it is not reachable from outside the cluster and if the pods try to send traffic outside it would probably not come back as it is SNATed to an IP which is unknown. We create the advertisements targeting only the node that is in charge of the service's traffic using the <code>nodeSelector</code> field, relying on ovn-k to label the node properly. <pre><code>apiVersion: metallb.io/v1beta1\nkind: BGPAdvertisement\nmetadata:\n  name: example-bgp-adv\n  namespace: metallb-system\nspec:\n  ipAddressPools:\n  - example-pool\n  nodeSelector:\n  - matchLabels:\n      egress-service.k8s.ovn.org/some-namespace-example-service: \"\"\n</code></pre> While possible to create more advertisements resources for the <code>example-pool</code>, it is the user's responsibility to make sure that the pool is advertised only by advertisements targeting the node holding the <code>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: \"\"</code> label - otherwise the traffic of the service will be broken.</p> </li> </ol>"},{"location":"features/hardware-offload/derive-from-mgmt-port/","title":"From PCI Address Gateway Interface Feature","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#overview","title":"Overview","text":"<p>The \"derive-from-mgmt-port\" gateway interface feature is a new capability in OVN-Kubernetes that enables automatic gateway interface resolution in DPU (Data Processing Unit) host mode deployments. This feature automatically discovers and configures the appropriate Physical Function (PF) interface as the gateway interface based on the Virtual Function (VF) used for the management port.</p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#problem-statement","title":"Problem Statement","text":"<p>In DPU deployments, the host typically has access to Virtual Functions (VFs) for management purposes, while the Physical Functions (PFs) are used for external connectivity. Previously, administrators had to manually specify the gateway interface, which required:</p> <ol> <li>Knowledge of the hardware topology</li> <li>Manual mapping of VF to PF relationships</li> <li>Configuration updates when hardware changes</li> <li>Potential for misconfiguration</li> </ol>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#solution","title":"Solution","text":"<p>The \"derive-from-mgmt-port\" feature automates the gateway interface discovery process by:</p> <ol> <li>Automatic Discovery: Automatically finds the PF interface associated with the management port VF</li> <li>Hardware Abstraction: Eliminates the need for manual hardware topology knowledge</li> <li>Dynamic Configuration: Adapts to hardware changes automatically</li> <li>Reduced Configuration: Simplifies deployment configuration</li> </ol>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#benefits","title":"Benefits","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#for-administrators","title":"For Administrators","text":"<ul> <li>Simplified Configuration: No need to manually specify gateway interfaces</li> <li>Reduced Errors: Eliminates manual mapping errors</li> <li>Hardware Agnostic: Works with any SR-IOV capable hardware</li> <li>Dynamic Adaptation: Automatically adapts to hardware changes</li> </ul>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#for-operations","title":"For Operations","text":"<ul> <li>Faster Deployment: Reduced configuration time</li> <li>Consistent Setup: Standardized gateway interface selection</li> <li>Reduced Maintenance: Less manual intervention required</li> <li>Better Reliability: Fewer configuration-related issues</li> </ul>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#for-development","title":"For Development","text":"<ul> <li>Cleaner Code: Centralized gateway interface logic</li> <li>Better Testing: Comprehensive unit test coverage</li> <li>Extensible Design: Foundation for future enhancements</li> </ul>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#technical-implementation","title":"Technical Implementation","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#code-changes","title":"Code Changes","text":"<ol> <li>New Constant: Added <code>DeriveFromMgmtPort = \"derive-from-mgmt-port\"</code> constant in <code>go-controller/pkg/types/const.go</code></li> <li>Enhanced Logic: Extended gateway initialization in <code>go-controller/pkg/node/default_node_network_controller.go</code></li> <li>Comprehensive Testing: Added unit tests covering success and failure scenarios</li> </ol>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#key-functions","title":"Key Functions","text":"<ul> <li><code>getManagementPortNetDev()</code>: Resolves management port device name</li> <li><code>GetPciFromNetDevice()</code>: Retrieves PCI address from network device</li> <li><code>GetPfPciFromVfPci()</code>: Resolves PF PCI address from VF PCI address</li> <li><code>GetNetDevicesFromPci()</code>: Discovers network devices associated with PCI address</li> </ul>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#error-handling","title":"Error Handling","text":"<p>The implementation includes robust error handling for: - Missing network devices - PCI address resolution failures - SR-IOV operation failures - Hardware compatibility issues</p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#configuration-examples","title":"Configuration Examples","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#basic-configuration","title":"Basic Configuration","text":"<pre><code>--ovnkube-node-mode=dpu-host\n--ovnkube-node-mgmt-port-netdev=pf0vf0\n--gateway-interface=derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#helm-configuration","title":"Helm Configuration","text":"<pre><code>ovnkube-node:\n  mode: dpu-host\n  mgmtPortNetdev: pf0vf0\n\ngateway:\n  interface: derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#configuration-file","title":"Configuration File","text":"<pre><code>[OvnKubeNode]\nmode=dpu-host\nmgmt-port-netdev=pf0vf0\n\n[Gateway]\ninterface=derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#migration-guide","title":"Migration Guide","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#from-manual-configuration","title":"From Manual Configuration","text":"<p>Before: <pre><code>--gateway-interface=eth0\n</code></pre></p> <p>After: <pre><code>--gateway-interface=derive-from-mgmt-port\n</code></pre></p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#verification-steps","title":"Verification Steps","text":"<ol> <li>Verify SR-IOV configuration is correct</li> <li>Ensure management port device is properly configured</li> <li>Check that PF interfaces are available</li> <li>Monitor logs for successful gateway interface resolution</li> </ol>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#testing","title":"Testing","text":""},{"location":"features/hardware-offload/derive-from-mgmt-port/#unit-tests","title":"Unit Tests","text":"<p>Comprehensive unit tests cover: - Successful gateway interface resolution - Error handling for missing devices - PCI address resolution failures - Network device discovery failures</p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#integration-tests","title":"Integration Tests","text":"<p>The feature integrates with existing: - Gateway initialization - DPU host mode functionality - SR-IOV operations - Network configuration</p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements include: - Support for multiple gateway interfaces - Enhanced device selection criteria - Integration with device plugins - Support for non-SR-IOV hardware - Advanced error reporting and diagnostics</p>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#related-documentation","title":"Related Documentation","text":"<ul> <li>DPU Gateway Interface Configuration</li> <li>DPU Support</li> <li>Gateway Accelerated Interface Configuration</li> <li>Configuration Guide</li> </ul>"},{"location":"features/hardware-offload/derive-from-mgmt-port/#support","title":"Support","text":"<p>For issues related to this feature: 1. Check the troubleshooting section in the DPU Gateway Interface Configuration guide 2. Verify SR-IOV hardware and driver support 3. Review error messages and logs 4. Consult the OVN-Kubernetes community for additional support </p>"},{"location":"features/hardware-offload/dpu-gateway-interface/","title":"DPU Gateway Interface Configuration","text":""},{"location":"features/hardware-offload/dpu-gateway-interface/#overview","title":"Overview","text":"<p>In DPU (Data Processing Unit) host mode deployments, OVN-Kubernetes supports automatic gateway interface resolution from PCI address. This feature is particularly useful when the management port is a Virtual Function (VF) and you want to automatically select the corresponding Physical Function (PF) interface as the gateway.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#background","title":"Background","text":"<p>In DPU deployments, the host typically has access to Virtual Functions (VFs) for management purposes, while the Physical Functions (PFs) are used for external connectivity. The \"derive-from-mgmt-port\" feature allows OVN-Kubernetes to automatically discover and configure the appropriate PF interface as the gateway interface based on the VF used for the management port.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#how-it-works","title":"How It Works","text":"<p>When configured with <code>--gateway-interface=derive-from-mgmt-port</code>, OVN-Kubernetes performs the following steps:</p> <ol> <li>Management Port Resolution: Gets the management port network device name (specified by <code>--ovnkube-node-mgmt-port-netdev</code>)</li> <li>VF PCI Address Retrieval: Retrieves the PCI address of the management port device (VF)</li> <li>PF PCI Address Resolution: Gets the Physical Function (PF) PCI address from the Virtual Function (VF) PCI address</li> <li>Network Device Discovery: Retrieves all network devices associated with the PF PCI address</li> <li>Interface Selection: Selects the first available network device as the gateway interface</li> </ol>"},{"location":"features/hardware-offload/dpu-gateway-interface/#configuration","title":"Configuration","text":""},{"location":"features/hardware-offload/dpu-gateway-interface/#command-line-options","title":"Command Line Options","text":"<pre><code>--ovnkube-node-mode=dpu-host\n--ovnkube-node-mgmt-port-netdev=pf0vf0\n--gateway-interface=derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#configuration-file","title":"Configuration File","text":"<pre><code>[OvnKubeNode]\nmode=dpu-host\nmgmt-port-netdev=pf0vf0\n\n[Gateway]\ninterface=derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#helm-configuration","title":"Helm Configuration","text":"<pre><code>ovnkube-node:\n  mode: dpu-host\n  mgmtPortNetdev: pf0vf0\n\ngateway:\n  interface: derive-from-mgmt-port\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#example-scenario","title":"Example Scenario","text":"<p>Consider a DPU setup with the following configuration:</p> <ul> <li>Management port device: <code>pf0vf0</code> (Virtual Function)</li> <li>VF PCI address: <code>0000:01:02.3</code></li> <li>PF PCI address: <code>0000:01:00.0</code></li> <li>Available PF interfaces: <code>eth0</code>, <code>eth1</code></li> </ul> <p>With <code>--gateway-interface=derive-from-mgmt-port</code>, OVN-Kubernetes will:</p> <ol> <li>Start with the management port device <code>pf0vf0</code></li> <li>Get its PCI address <code>0000:01:02.3</code></li> <li>Resolve the PF PCI address to <code>0000:01:00.0</code></li> <li>Find all network devices associated with PF <code>0000:01:00.0</code>: <code>eth0</code>, <code>eth1</code></li> <li>Select <code>eth0</code> (first device) as the gateway interface</li> </ol>"},{"location":"features/hardware-offload/dpu-gateway-interface/#requirements","title":"Requirements","text":""},{"location":"features/hardware-offload/dpu-gateway-interface/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>SR-IOV capable network interface card</li> <li>Virtual Function (VF) and Physical Function (PF) setup</li> <li>Management port configured as a VF</li> </ul>"},{"location":"features/hardware-offload/dpu-gateway-interface/#software-requirements","title":"Software Requirements","text":"<ul> <li>SR-IOV utilities available on the system</li> <li>OVN-Kubernetes running in DPU host mode</li> <li>Proper VF/PF driver support</li> </ul>"},{"location":"features/hardware-offload/dpu-gateway-interface/#configuration-requirements","title":"Configuration Requirements","text":"<ul> <li>Must be used in DPU host mode (<code>--ovnkube-node-mode=dpu-host</code>)</li> <li>Management port netdev must be specified (<code>--ovnkube-node-mgmt-port-netdev</code>)</li> <li>Gateway interface must be set to <code>derive-from-mgmt-port</code></li> </ul>"},{"location":"features/hardware-offload/dpu-gateway-interface/#error-handling","title":"Error Handling","text":"<p>The system will return an error in the following scenarios:</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#no-network-devices-found","title":"No Network Devices Found","text":"<pre><code>no netdevs found for pci address 0000:01:00.0\n</code></pre> <p>Cause: The PF PCI address doesn't have any associated network devices.</p> <p>Resolution: Verify that the PF has network interfaces configured and are visible to the system.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#pci-address-resolution-failure","title":"PCI Address Resolution Failure","text":"<pre><code>failed to get PCI address\n</code></pre> <p>Cause: Unable to retrieve the PCI address from the management port device.</p> <p>Resolution: Ensure the management port device exists and is properly configured.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#pf-pci-address-resolution-failure","title":"PF PCI Address Resolution Failure","text":"<pre><code>failed to get PF PCI address\n</code></pre> <p>Cause: Unable to resolve the PF PCI address from the VF PCI address.</p> <p>Resolution: Verify SR-IOV configuration and driver support.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#network-device-discovery-failure","title":"Network Device Discovery Failure","text":"<pre><code>failed to get network devices\n</code></pre> <p>Cause: Unable to retrieve network devices associated with the PF PCI address.</p> <p>Resolution: Check SR-IOV utilities and system configuration.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/hardware-offload/dpu-gateway-interface/#verify-sr-iov-configuration","title":"Verify SR-IOV Configuration","text":"<pre><code># Check if SR-IOV is enabled\nlspci | grep -i ethernet\n\n# Check VF configuration\nip link show\n\n# Check PF/VF relationship\nls /sys/bus/pci/devices/*/virtfn*\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#verify-management-port-device","title":"Verify Management Port Device","text":"<pre><code># Check if management port device exists\nip link show pf0vf0\n\n# Check PCI address\nethtool -i pf0vf0 | grep bus-info\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#debug-pci-address-resolution","title":"Debug PCI Address Resolution","text":"<pre><code># Get VF PCI address\ncat /sys/class/net/pf0vf0/device/address\n\n# Get PF PCI address (if available)\ncat /sys/class/net/pf0vf0/device/physfn/address\n</code></pre>"},{"location":"features/hardware-offload/dpu-gateway-interface/#integration-with-existing-features","title":"Integration with Existing Features","text":""},{"location":"features/hardware-offload/dpu-gateway-interface/#gateway-accelerated-interface","title":"Gateway Accelerated Interface","text":"<p>The \"derive-from-mgmt-port\" feature is used in conjunction with management interface to select the appropriate gateway accelerated interface.</p> <p>The management port can be specified through one of the following options: <pre><code>  --ovnkube-node-mgmt-port-netdev)\n    OVNKUBE_NODE_MGMT_PORT_NETDEV=$VALUE\n</code></pre></p> <pre><code>  --ovnkube-node-mgmt-port-dp-resource-name)\n    OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME=$VALUE\n</code></pre> <p>OVNKUBE_NODE_MGMT_PORT_DP_RESOURCE_NAME has priority over OVNKUBE_NODE_MGMT_PORT_NETDEV and it is easier to use since it points to a SRIOV Device Plugin pool name.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#multiple-network-support","title":"Multiple Network Support","text":"<p>This feature works with multiple network support and can be used in environments where pods have multiple interfaces connected to different networks.</p>"},{"location":"features/hardware-offload/dpu-gateway-interface/#limitations","title":"Limitations","text":"<ul> <li>Only available in DPU host mode</li> <li>Requires SR-IOV capable hardware</li> <li>Limited to the first available network device from the PF</li> <li>Depends on proper VF/PF driver support</li> <li>May not work with all SR-IOV implementations</li> </ul>"},{"location":"features/hardware-offload/dpu-gateway-interface/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements to this feature could include:</p> <ul> <li>Support for selecting specific network devices based on criteria</li> <li>Integration with device plugin resources</li> <li>Support for multiple gateway interfaces</li> <li>Enhanced error reporting and diagnostics</li> <li>Support for non-SR-IOV hardware configurations </li> </ul>"},{"location":"features/hardware-offload/dpu-support/","title":"Dpu support","text":""},{"location":"features/hardware-offload/dpu-support/#dpu-support","title":"DPU support","text":"<p>With the emergence of Data Processing Units (DPUs),  NIC vendors can now offer greater hardware acceleration capability, flexibility and security. </p> <p>It is desirable to leverage DPU in OVN-kubernetes to accelerate networking and secure the network control plane.</p> <p>A DPU consists of: - Industry-standard, high-performance, software-programmable multi-core CPU - High-performance network interface - Flexible and programmable acceleration engines</p> <p>Similarly to Smart-NICs, a DPU follows the kernel switchdev model. In this model, every VF/PF net-device on the host has a corresponding representor net-device existing on the embedded CPU.</p> <p>Any vendor that manufactures a DPU which supports the above model should work with current design.</p> <p>Design document can be found here.</p>"},{"location":"features/hardware-offload/dpu-support/#ovn-kubernetes-in-a-dpu-accelerated-environment","title":"OVN-Kubernetes in a DPU-Accelerated Environment","text":"<p>The ovn-kubernetes deployment will have two parts one on the host and another on the DPU side.</p> <p>These aforementioned parts are expected to be deployed also on two different Kubernetes clusters, one for the host and another for the DPUs.</p>"},{"location":"features/hardware-offload/dpu-support/#host-cluster","title":"Host Cluster","text":""},{"location":"features/hardware-offload/dpu-support/#ovn-kubernetes-control-plane-related-component","title":"OVN-Kubernetes control plane related component","text":"<ul> <li>ovn-cluster-manager</li> </ul>"},{"location":"features/hardware-offload/dpu-support/#ovn-kubernetes-components-on-a-standard-host-non-dpu","title":"OVN-Kubernetes components on a Standard Host (Non-DPU)","text":"<ul> <li>local-nb-ovsdb</li> <li>local-sb-ovsdb</li> <li>run-ovn-northd</li> <li>ovnkube-controller-with-node</li> <li>ovn-controller</li> <li>ovs-metrics</li> </ul>"},{"location":"features/hardware-offload/dpu-support/#ovn-kubernetes-component-on-a-dpu-enabled-host","title":"OVN-Kubernetes component on a DPU-Enabled Host","text":"<ul> <li>ovn-node</li> </ul> <p>For detailed configuration of gateway interfaces in DPU host mode, see DPU Gateway Interface Configuration.</p>"},{"location":"features/hardware-offload/dpu-support/#dpu-cluster","title":"DPU Cluster","text":""},{"location":"features/hardware-offload/dpu-support/#ovn-kubernetes-components","title":"OVN-Kubernetes components","text":"<ul> <li>local-nb-ovsdb </li> <li>local-sb-ovsdb</li> <li>run-ovn-northd</li> <li>ovnkube-controller-with-node</li> <li>ovn-controller</li> <li>ovs-metrics</li> </ul>"},{"location":"features/hardware-offload/ovs-doca/","title":"OVS Acceleration with DOCA datapath","text":""},{"location":"features/hardware-offload/ovs-doca/#introduction","title":"Introduction","text":"<p>OVS supports Hardware Acceleration features which allows to offload OVS packet processing to the hardware (switch chassis, NIC etc.) while maintaining OVS control path (client facing apis and tools etc.) unmodified. It utilizes Single Root-IO Virtualization (SR-IOV) technology with VF representor host net-device.</p> <p>DOCA (Data Center Infrastructure-on-a-Chip Architecture) is a software stack for NVIDIA Smart NIC (ConnectX) and Data Processing Unit (BlueField) product lines. It contains a runtime and development environment, including libraries and drivers for device management and programmability, for the host (Smart NIC) or as part of the Data Processing Unit (DPU).</p> <p>OVS-DOCA extends traditional OVS-DPDK and OVS-Kernel data-path offload interfaces (DPIF), adds  OVS-DOCA as an additional DPIF implementation. It preserves the same interfaces as OVS-DPDK and OVS-Kernel while utilizing the DOCA Flow library. OVS-DOCA uses unique hardware offload mechanisms and application techniques to maximize performance and adds other features. OVS Acceleration with kernel datapath is documented here</p> <p></p>"},{"location":"features/hardware-offload/ovs-doca/#motivation-why-use-ovs-doca-instead-of-ovs-dpdk","title":"Motivation: Why use ovs-doca instead of ovs-dpdk","text":"<p>OVS-DOCA is also a userland based virtual switch application like OVS-DPDK. Like OVS-DPDK it uses DPDK and PMD (Poll Mode Driver). It preserves the existing north bound APIs (Openflow) and is a drop in replacement for upstream OVS. But the main difference is that OVS-DOCA uses the DOCA-flow api instead of the rte_flow used by DPDK. This allows it to use hardware steering for offloads instead of software steering. The main benefits are:</p> <ul> <li>Enhanced performance, scalability and faster feature rollouts</li> <li>Huge scale up in insertion rate and Connection Tracking (CT)</li> <li>Optimized OVS data path offloads</li> <li>Make better use of capabilities of the DOCA platform</li> <li>Future HW compatible - DOCA API compatibility</li> </ul>"},{"location":"features/hardware-offload/ovs-doca/#supported-controllers","title":"Supported Controllers","text":"<ul> <li>Nvidia ConnectX-6DX/LX NIC</li> <li>Nvidia ConnectX-7 NIC and newer</li> <li>Nvidia BlueField 2/3 DPU and newer</li> </ul>"},{"location":"features/hardware-offload/ovs-doca/#installing-ovs-doca","title":"Installing OVS-DOCA","text":"<p>OVS-DOCA is part of the DOCA-Host package. DOCA-Host is available in different installation profiles, each of which provides subset of the full DOCA installation. For the purposes of OVN-Kubernetes we need the DOCA packaged version of OVS which is available in the <code>doca-networking</code> profile. This includes:</p> <ul> <li>MLNX_OFED drivers and tools</li> <li>DOCA Core</li> <li>MLNX-DPDK</li> <li>OVS-DOCA</li> <li>DOCA Flow</li> <li>DOCA IPsec</li> </ul> <p>See more details including supported OS and kernel versions, installation packages and instructions for installation at the corresponding product page for DOCA and OVS-DOCA. Additional tuning options for OVS are also provided there.</p>"},{"location":"features/hardware-offload/ovs-doca/#worker-node-configuation","title":"Worker Node Configuation","text":""},{"location":"features/hardware-offload/ovs-doca/#configure-huge-pages","title":"Configure Huge Pages","text":"<p><pre><code>mkdir -p /hugepages\nmount -t hugetlbfs hugetlbfs /hugepages\necho 4096 &gt; /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages\n</code></pre> Note: Persistentance of huge pages (node reboot etc.) is beyond the scope of this document.</p>"},{"location":"features/hardware-offload/ovs-doca/#switchdev-mode","title":"Switchdev mode","text":"<ul> <li>Unbind all VFs, turn on switchdev and bind all VFs back. <pre><code>echo 0000:04:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:04:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\n\necho switchdev &gt; /sys/class/net/enp4s0f0/compat/devlink/mode\n\n\necho 0000:04:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:04:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/bind\n</code></pre></li> </ul>"},{"location":"features/hardware-offload/ovs-doca/#enable-doca-and-hardware-offloads","title":"Enable DOCA and hardware offloads","text":"<pre><code>ovs-vsctl --no-wait set Open_vSwitch . other_config:doca-init=true\novs-vsctl set Open_vSwitch . other_config:hw-offload=true\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#restart-ovs","title":"Restart OVS","text":"<pre><code>systemctl restart openvswitch\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#usage","title":"Usage","text":"<ul> <li>Create OVS-DOCA bridge of type <code>netdev</code>:</li> </ul> <pre><code>ovs-vsctl --no-wait add-br br-ex -- set bridge br-ex datapath_type=netdev\n</code></pre> <ul> <li>Adding interfaces to OVS: use type dpdk</li> </ul> <pre><code># PF\novs-vsctl add-port br0-ovs enp4s0f0 -- set Interface enp4s0f0 type=dpdk\n# Representor\novs-vsctl add-port br0-ovs enp4s0f0_0 -- set Interface enp4s0f0_0 type=dpdk\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#ovn-integration-bridge","title":"OVN Integration Bridge","text":"<p>Following configuration will ensure that OVN integration bridge (br-int) will be netdev data type.</p> <pre><code>ovs-vsctl set open . external-ids:ovn-bridge-datapath-type=netdev\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#worker-node-sr-iov-network-device-plugin-configuration","title":"Worker Node SR-IOV network device plugin configuration","text":"<p>This configuration data for the SRIOV device plugin.</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: sriovdp-config\n  namespace: kube-system\ndata:\n  config.json: |\n    {\n        \"resourceList\": [\n            {\n                \"resourceName\": \"p0_vfs\",\n                \"resourcePrefix\": \"nvidia.com\",\n                \"selectors\": {\n                    \"vendors\": [\"15b3\"],\n                    \"devices\": [\"1014\", \"1016\", \"1018\", \"101a\", \"101c\", \"101e\"],\n                    \"pfNames\": [\"p0#1-3\"],\n                    \"isRdma\": true\n                  }\n                }\n              ]\n            }\n</code></pre> <p>Note: Adjust the values on the field pfNames to your setup. Replace p0 with your PF Name (eg: enp4s0f0) and use the selector to include the range of VFs to be used for Kubernetes pods. VF0 is usually reserved for management port.  You can override on a per node basis with a config file in <code>/etc/pcidp/config.json</code>.</p> <p>Deploy SR-IOV network device plugin as daemonset see https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin</p>"},{"location":"features/hardware-offload/ovs-doca/#multus-cni-configuration","title":"Multus CNI configuration","text":"<p>Deploy multus CNI as daemonset based on https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/deployments/multus-daemonset.yml</p>"},{"location":"features/hardware-offload/ovs-doca/#networkattachementdefinition","title":"NetworkAttachementDefinition","text":"<p>Create NetworkAttachementDefinition CRD with OVN CNI config</p> <pre><code>apiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: ovn-primary\n  namespace: default\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: nvidia.com/p0_vfs\nspec:\n  config: '{\n    \"cniVersion\" : \"0.4.0\",\n    \"name\" : \"ovn-primary\",\n    \"type\" : \"ovn-k8s-cni-overlay\",\n    \"logFile\": \"/var/log/ovn-kubernetes/ovn-k8s-cni-overlay.log\",\n    \"logLevel\": \"5\",\n    \"logfile-maxsize\": 100,\n    \"logfile-maxbackups\": 5,\n    \"logfile-maxage\": 5\n    }'\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#deploy-pod-with-ovs-hardware-offload-based-on-ovs-doca","title":"Deploy POD with OVS hardware-offload based on ovs-doca","text":"<p>Create POD spec and</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netshoot-deploy\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: netshoot\n  template:\n    metadata:\n      annotations:\n        v1.multus-cni.io/default-network: default/ovn-primary # &lt;== Network\n      name: netshoot\n      labels:\n        app: netshoot\n    spec:\n      containers:\n      - name: netshoot\n        command:\n        - /bin/sh\n        - -c\n        - 'trap : TERM INT; sleep infinity &amp; wait'\n        image: \"nicolaka/netshoot:v0.12\"\n        securityContext:\n          capabilities:\n            add: [\"NET_ADMIN\"]\n        resources:\n          requests:\n            nvidia.com/p0_vfs: '1'   # \u21d0 Notice the resource used here\n          limits:\n            nvidia.com/p0_vfs: '1'\n</code></pre>"},{"location":"features/hardware-offload/ovs-doca/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>No special configuration or knobs need to be set on the OVN-Kubernetes side. There are no user facing API changes or changes in OVN constructs or OVS Flows as such.</p> <p>At the minimum following OVS configuration to ensure that OVN integration bridge (br-int) will be netdev data type. This can also serve as a feature flag for selecting ovs-doca based datapath.</p> <pre><code>ovs-vsctl set open . external-ids:ovn-bridge-datapath-type=netdev\n</code></pre> <p>OVN-Kubernetes will detect the datapath type and set interface configurations as needed.</p> <p>Also, the external bridge may be also set to type <code>netdev</code>.</p> <p>Fine tuning OVS configuration may be desirable in some contexts. For those refer to the product documentation pages.</p>"},{"location":"features/hardware-offload/ovs-kernel/","title":"OVS Acceleration with Kernel datapath","text":"<p>The OVS software based solution is CPU intensive, affecting system performance and preventing fully utilizing available bandwidth. OVS 2.8 and above support new feature called OVS Hardware Offload which improves performance significantly.  This feature allows to offload the OVS data-plane to the NIC while maintaining  OVS control-plane unmodified. It is using SR-IOV technology with VF representor host net-device. The VF representor plays the same role as TAP devices in Para-Virtual (PV) setup. A packet sent through the VF representor on the host arrives to the VF, and a packet sent through the VF is received by its representor.</p>"},{"location":"features/hardware-offload/ovs-kernel/#supported-ethernet-controllers","title":"Supported Ethernet controllers","text":"<p>The following manufacturers are known to work:</p> <ul> <li>Mellanox ConnectX-5 NIC</li> <li>Mellanox ConnectX-6DX NIC</li> </ul>"},{"location":"features/hardware-offload/ovs-kernel/#prerequisites","title":"Prerequisites","text":"<ul> <li>Linux Kernel 5.7.0 or above</li> <li>Open vSwitch 2.13 or above</li> <li>iproute &gt;= 4.12</li> <li>sriov-device-plugin</li> <li>multus-cni</li> </ul>"},{"location":"features/hardware-offload/ovs-kernel/#worker-node-sr-iov-configuration","title":"Worker Node SR-IOV Configuration","text":"<p>In order to enable Open vSwitch hardware offloading, the following steps are required. Please make sure you have root privileges to run the commands below.</p> <p>Check the Number of VF Supported on the NIC</p> <pre><code>cat /sys/class/net/enp3s0f0/device/sriov_totalvfs\n8\n</code></pre> <p>Create the VFs</p> <pre><code>echo '4' &gt; /sys/class/net/enp3s0f0/device/sriov_numvfs\n</code></pre> <p>Verfiy the VFs are created</p> <pre><code>ip link show enp3s0f0\n8: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT qlen 1000\n   link/ether a0:36:9f:8f:3f:b8 brd ff:ff:ff:ff:ff:ff\n   vf 0 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 1 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 2 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n   vf 3 MAC 00:00:00:00:00:00, spoof checking on, link-state auto\n</code></pre> <p>Setup the PF to be up</p> <pre><code>ip link set enp3s0f0 up\n</code></pre> <p>Unbind the VFs from the driver</p> <pre><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\necho 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/unbind\n</code></pre> <p>Configure SR-IOV VFs to switchdev mode</p> <pre><code>devlink dev eswitch set pci/0000:03:00.0 mode switchdev\nethtool -K enp3s0f0 hw-tc-offload on\n</code></pre> <p>Bind the VFs to the driver</p> <pre><code>echo 0000:03:00.2 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.3 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.4 &gt; /sys/bus/pci/drivers/mlx5_core/bind\necho 0000:03:00.5 &gt; /sys/bus/pci/drivers/mlx5_core/bind\n</code></pre> <p>Set hw-offload=true restart Open vSwitch</p> <pre><code>systemctl enable openvswitch.service\novs-vsctl set Open_vSwitch . other_config:hw-offload=true\nsystemctl restart openvswitch.service\n</code></pre>"},{"location":"features/hardware-offload/ovs-kernel/#worker-node-sr-iov-network-device-plugin-configuration","title":"Worker Node SR-IOV network device plugin configuration","text":"<p>This plugin creates device plugin endpoints based on the configurations given in file <code>/etc/pcidp/config.json</code>. This configuration file is in json format as shown below:</p> <pre><code>{\n    \"resourceList\": [\n         {\n            \"resourceName\": \"cx5_sriov_switchdev\",\n            \"selectors\": {\n                \"vendors\": [\"15b3\"],\n                \"devices\": [\"1018\"]\n            }\n        }\n    ]\n}\n</code></pre> <p>Deploy SR-IOV network device plugin as daemonset see https://github.com/intel/sriov-network-device-plugin</p>"},{"location":"features/hardware-offload/ovs-kernel/#worker-node-multus-cni-configuration","title":"Worker Node Multus CNI configuration","text":"<p>Multus Config <pre><code>{\n  \"name\": \"multus-cni-network\",\n  \"type\": \"multus\",\n  \"clusterNetwork\": \"default\",\n  \"defaultNetworks\":[],\n  \"kubeconfig\": \"/etc/kubernetes/node-kubeconfig.yaml\"\n}\n</code></pre></p> <p>Deploy multus CNI as daemonset see https://github.com/intel/multus-cni</p> <p>Create NetworkAttachementDefinition CRD with OVN CNI config</p> <pre><code>Kubernetes Network CRD Spec:\napiVersion: \"k8s.cni.cncf.io/v1\"\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: default\n  annotations:\n    k8s.v1.cni.cncf.io/resourceName: mellanox.com/cx5_sriov_switchdev\nspec:\n  Config: '{\"cniVersion\":\"0.3.1\",\"name\":\"ovn-kubernetes\",\"type\":\"ovn-k8s-cni-overlay\",\"ipam\":{},\"dns\":{}}'\n</code></pre>"},{"location":"features/hardware-offload/ovs-kernel/#deploy-pod-with-ovs-hardware-offload","title":"Deploy POD with OVS hardware-offload","text":"<p>Create POD spec and</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: ovs-offload-pod1\n  annotations:\n    v1.multus-cni.io/default-network: default\nspec:\n  containers:\n  - name: appcntr1\n    image: centos/tools\n    resources:\n      requests:\n        mellanox.com/cx5_sriov_switchdev: '1'\n      limits:\n        mellanox.com/cx5_sriov_switchdev: '1'\n</code></pre>"},{"location":"features/hardware-offload/ovs-kernel/#verify-hardware-offload-is-working","title":"Verify Hardware-Offload is working","text":"<p>Lookup VF representor, in this example it is e5a1c8fcef0f327</p> <pre><code>$ ip link show enp3s0f0\n6: enp3s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP mode DEFAULT group default qlen 1000\n   link/ether ec:0d:9a:46:9e:84 brd ff:ff:ff:ff:ff:ff\n   vf 0 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 1 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 2 MAC 00:00:00:00:00:00, spoof checking off, link-state enable, trust off, query_rss off\n   vf 3 MAC fa:16:3e:b9:b8:ce, vlan 57, spoof checking on, link-state enable, trust off, query_rss off\n\ncompute_node2# ls -l /sys/class/net/\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth0 -&gt; ../../devices/virtual/net/eth0\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth1 -&gt; ../../devices/virtual/net/eth1\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 eth2 -&gt; ../../devices/virtual/net/eth2\nlrwxrwxrwx 1 root root 0 Sep 11 10:54 e5a1c8fcef0f327 -&gt; ../../devices/virtual/net/e5a1c8fcef0f327\n</code></pre> <p>Access the POD</p> <pre><code>kubectl exec -it ovs-offload-pod1 -- /bin/bash\n</code></pre> <p>Ping other POD on second worker node <pre><code>ping ovs-offload-pod2\n</code></pre></p> <p>Check traffic on the VF representor port. Verify that only the first ICMP packet appears <pre><code>tcpdump -nnn -i e5a1c8fcef0f327\n\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\n17:12:41.260487 IP 172.0.0.13 &gt; 172.0.0.10: ICMP echo request, id 1263, seq 1, length 64\n17:12:41.260778 IP 172.0.0.10 &gt; 172.0.0.13: ICMP echo reply, id 1263, seq 1, length 64\n17:12:46.268951 ARP, Request who-has 172.0.0.13 tell 172.0.0.10, length 42\n17:12:46.271771 ARP, Reply 172.0.0.13 is-at fa:16:3e:1a:10:05, length 46\n17:12:55.354737 IP6 fe80::f816:3eff:fe29:8118 &gt; ff02::1: ICMP6, router advertisement, length 64\n17:12:56.106705 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 62:21:f0:89:40:73, length 30\n</code></pre></p>"},{"location":"features/hardware-offload/ovs-kernel/#ovs-hardware-offload-dpu-support","title":"OVS hardware offload DPU support","text":"<p>Data Processing Units (DPU) combine the advanced capabilities of a Smart-NIC (such as Mellanox ConnectX-6DX NIC) with a general purpose embedded CPU and a high-speed memory controller.</p> <p>Similarly to Smart-NICs, a DPU follows the kernel switchdev model. In this model, every VF/PF net-device on the host has a corresponding representor net-device existing on the embedded CPU.</p>"},{"location":"features/hardware-offload/ovs-kernel/#supported-dpus","title":"Supported DPUs","text":"<p>The following manufacturers are known to work:</p> <ul> <li>Mellanox Bluefield-2</li> </ul> <p>Deployment guide can be found here.</p>"},{"location":"features/hardware-offload/ovs-kernel/#vdpa","title":"vDPA","text":"<p>vDPA (Virtio DataPath Acceleration) is a technology that enables the acceleration of virtIO devices while allowing the implementations of such devices (e.g: NIC vendors) to use their own control plane.</p> <p>vDPA can be combined with the SR-IOV OVS Hardware offloading setup to expose the workload to an open standard interface such as virtio-net.</p>"},{"location":"features/hardware-offload/ovs-kernel/#additional-prerequisites","title":"Additional Prerequisites:","text":"<ul> <li>Linux Kernel &gt;= 5.12</li> <li>iproute &gt;= 5.14</li> </ul>"},{"location":"features/hardware-offload/ovs-kernel/#supported-hardware","title":"Supported Hardware:","text":"<ul> <li>Mellanox ConnectX-6DX NIC</li> </ul>"},{"location":"features/hardware-offload/ovs-kernel/#additional-configuration","title":"Additional configuration","text":"<p>In addition to all the steps listed above, insert the virtio-vdpa driver and the mlx-vdpa driver:</p> <pre><code>$ modprobe vdpa\n$ modprobe virtio-vdpa\n$ modprobe mlx5-vdpa\n</code></pre> <p>The the <code>vdpa</code> tool (part of iproute package) is used to create a vdpa device on top of an existing VF:</p> <pre><code>$ vdpa mgmtdev show\npci/0000:65:00.2:\n  supported_classes net\n$ vdpa dev add name vdpa2 mgmtdev pci/0000:65:00.2\n$ vdpa dev list\nvdpa2: type network mgmtdev pci/0000:65:00.2 vendor_id 5555 max_vqs 16 max_vq_size 256\n</code></pre> <p>After a device has been created, the SR-IOV Device Plugin plugin configuration has to be modified for it to select and expose the vdpa device:</p> <pre><code>{\n    \"resourceList\": [\n         {\n            \"resourceName\": \"cx6_sriov_vpda_virtio\",\n            \"selectors\": {\n               \"vendors\": [\"15b3\"],\n               \"devices\": [\"101e\"],\n               \"vdpaType\": \"virtio\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"features/infrastructure-security-controls/node-identity/","title":"OVN-Kubernetes Node Identity","text":""},{"location":"features/infrastructure-security-controls/node-identity/#introduction","title":"Introduction","text":"<p>The OVN-Kubernetes node identity feature introduces a per-node client certificate for ovnkube-node pods, together with a validating admission webhook,  enabling granular permission enforcement specific to ovn-kubernetes. Previously, all <code>ovnkube-node</code> pods shared a common service account,  with their API permissions managed only through Kubernetes RBAC rules.\\ The goal of this feature is to limit <code>ovnkube-node</code> permissions to the minimum required for networking management on a specific Kubernetes node. We will mimic the approach used by kubelet in which every node has a unique identity,  and its API write requests are verified using a NodeRestriction validating admission webhook.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#per-node-client-certificates","title":"Per-node client certificates","text":"<p>This process mimics the bootstrap initialization in kubelet. When the <code>ovnkube-node</code> starts for the first time, it uses the host's <code>kubeconfig</code> to create a CertificateSigningRequest that requests a client certificate for the <code>system:ovn-node:&lt;nodeName&gt;</code> user in the <code>system:ovn-nodes</code> group. This request is then signed by the kubernetes.io/kube-apiserver-client signer.</p> <p>For the certificate to be signed, it must first be approved. The newly introduced <code>OVNKubeCSRController</code> component approves or denies <code>CertificateSigningRequests</code> created for users with the <code>system:ovn-node</code> prefix. The <code>OVNKubeCSRController</code> performs several checks to validate the requested certificate, including: - Ensuring that the node name extracted from the request matches the node name of the user that sent it. - Verifying that the group is set to <code>system:ovn-nodes</code>. - Checking that the certificate expiration does not exceed the maximum allowed duration.</p> <p>Once the certificate is approved and signed, <code>ovnkube-node</code> uses it to communicate with the API server and request a new certificate upon expiration. The RBAC rules are consistent across all <code>ovnkube-node</code> pods, we use the <code>system:ovn-nodes</code> group as the subject for Role/ClusterRole bindings to avoid duplication: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n    name: ovnkube-node\nroleRef:\n    name: ovnkube-node\n    kind: ClusterRole\n    apiGroup: rbac.authorization.k8s.io\nsubjects:\n    - kind: Group\n      name: system:ovn-nodes\n      apiGroup: rbac.authorization.k8s.io\n</code></pre> Note: By default, the generated certificates have a brief lifetime set (10min). This is intentional as it helps ensure that the certificate rotation works seamlessly, but it might be adjusted in production environments.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#validating-admission-webhook","title":"Validating Admission Webhook","text":"<p>The feature introduces a validating webhook for updates to <code>pod/status</code> (Interconnect only) and <code>node/status</code>.\\ The <code>ovnkube-node</code> pod exclusively updates the status on both resources, so it is sufficient to verify only update requests.\\ The webhooks include the following checks for each <code>ovnkube-node</code> pod: - Modifying annotations on pods hosted on its own node. - Modifying annotations on its own node. - Modifying only allowed annotations. - Not modifying anything other than annotations.</p> <p>The allowed annotations list contains both common and feature specific values:  - By default, the webhook will verify a set of common node annotations used in all deployments.  - When <code>enable-interconnect</code> parameter is provided the webhook will validate additional pod/node annotations set by the ovnkube-node component in interconnect environments.  - When <code>enable-hybrid-overlay</code> parameter is provided the webhook will validate additional node annotations set by the ovnkube-node component in interconnect environments.</p> <p>The specific annotation values can be found in <code>go-controller/pkg/ovnwebhook/nodeadmission.go</code> and <code>go-controller/pkg/ovnwebhook/podadmission.go</code> files.</p> <p>Some of the allowed annotations have additional checks; for instance, the IP addresses in k8s.ovn.org/pod-networks must match the node's k8s.ovn.org/node-subnets networks.</p>"},{"location":"features/infrastructure-security-controls/node-identity/#daemonset","title":"DaemonSet","text":"<p>In Kind, the feature is enabled by default and can be disabled with <code>--disable-ovnkube-identity</code> when creating the cluster.\\ By default, the webhook listens on the nodes <code>localhost</code> address so there has to be an instance running on the control-plane node: <pre><code>kubectl get pod -lname=ovnkube-identity -n ovn-kubernetes  -o wide\nNAME                                READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\novnkube-identity-57f9778d99-llfqz   1/1     Running   0          30m   172.18.0.3   ovn-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre> This approach allows us to collocate the webhook with the API server for better response times and avoid relying on cluster networking.</p>"},{"location":"features/multiple-networks/multi-homing/","title":"Multihoming","text":""},{"location":"features/multiple-networks/multi-homing/#introduction","title":"Introduction","text":"<p>Multihoming allows the configuration of secondary-network interfaces for K8s pods. OVN-Kubernetes secondary-network has three configurable topologies: layer 3, layer 2, or localnet.</p> <ul> <li>A layer 3 topology is a simplification of the topology for the cluster default network - but without egress.</li> <li>A layer 2 topology provides an isolated (no egress) cluster-wide layer2 network providing east-west traffic to the   cluster workloads.</li> <li>A localnet topology is based on layer 2 topology, but also allows connecting to an existent (configured) physical   network to provide north-south traffic to the workloads.</li> </ul> <p>For layer 2 and localnet topologies, multihoming also allows IP features on secondary interfaces such as static IP allocation and persistent IP addresses for virtualization workloads.</p> <p>To allow pods to have multiple network interfaces, the user must provide the configurations specifying how to connect to these networks; these configurations are defined in a CRD named <code>NetworkAttachmentDefinition</code>, defined by the Kubernetes Network Custom Resource Definition De-facto Standard.</p> <p>[!NOTE] layer 2 and layer 3 topologies are overlays - thus, they do not need any previous physical network configuration.</p>"},{"location":"features/multiple-networks/multi-homing/#prerequisites","title":"Prerequisites","text":"<ul> <li>multus-cni</li> </ul>"},{"location":"features/multiple-networks/multi-homing/#motivation","title":"Motivation","text":"<p>Multihoming is essential when you need more than one network interface on your pods. This can be useful for various use cases, such as virtual network functions (VNFs), firewalls, or virtualization (virt) where the default cluster network might not be suitable.</p> <p>In OVN-K, multihoming supports several virt-specific features. These include persistent IP addresses for virtualization workloads, ensuring that VMs retain their IP addresses even when they move across nodes. This enhances workload mobility and minimizes disruptions.</p> <p>Multihoming is also compatible with the multi-network policy API, which can provide further security rules on the traffic.</p>"},{"location":"features/multiple-networks/multi-homing/#user-stories","title":"User-Stories","text":"<ul> <li>As a Cluster-Admin, I want to configure secondary networks for specific pods so that I can enable   specialized/sensitive workloads with distinct network requirements.</li> <li>As a Cluster-Admin, I want to facilitate seamless live migration of VMs within so that I can maintain established TCP   connections and preserve VM IP configurations during migration.</li> </ul>"},{"location":"features/multiple-networks/multi-homing/#user-cases","title":"User-Cases","text":"<ul> <li>cluster-wide overlay network on layer 2:   In this case example, two VMs from different namespaces - VMA and VMC - are connected over a secondary-network.   VMB is not exposed to this traffic.   </li> <li>cluster-wide localnet network:   In this case example, Pod and VM workloads accessing a relational DB reachable via the physical network (i.e. deployed   outside Kubernetes).   </li> </ul>"},{"location":"features/multiple-networks/multi-homing/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>The <code>multi-network</code> feature must be enabled in the OVN-Kubernetes configuration. Please use the <code>Feature Config</code> option <code>enable-multi-network</code> under <code>OVNKubernetesFeatureConfig</code> config to enable it.</p>"},{"location":"features/multiple-networks/multi-homing/#workflow-description","title":"Workflow Description","text":"<p>After a pod is scheduled on a particular Kubernetes node, kubelet will invoke the meta-plugin installed on the cluster (such as Multus) to prepare the pod for networking. The meta-plugin will invoke the CNI responsible for setting up the pod's default cluster network. After that, the meta-plugin iterates the list of secondary networks, invoking the corresponding CNI implementing the logic to attach the pod to that particular secondary-network. The CNI will use the details specified on the <code>network-attachment-definition</code> in order to do that.</p> <p>[!NOTE] networks are not namespaced - i.e. creating multiple <code>network-attachment-definition</code>s with different   configurations pointing at the same network (same <code>NetConf.Name</code> attribute) is not supported.</p>"},{"location":"features/multiple-networks/multi-homing/#implementation-details","title":"Implementation Details","text":""},{"location":"features/multiple-networks/multi-homing/#user-facing-api-changes","title":"User facing API Changes","text":"<p>There are no user facing API Changes.</p>"},{"location":"features/multiple-networks/multi-homing/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p>Below you will find example attachment configurations for each of the current topologies OVN-K allows for secondary networks.</p>"},{"location":"features/multiple-networks/multi-homing/#routed-layer-3-topology","title":"Routed - layer 3 - topology","text":"<p>This topology is a simplification of the topology for the cluster default network - but without egress.</p> <p>There is a logical switch per node - each with a different subnet - and a router interconnecting all the logical switches.</p> <p>The following net-attach-def configures the attachment to a routed secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l3-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"1.0.0\",\n            \"name\": \"tenantblue\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer3\",\n            \"subnets\": \"10.128.0.0/16/24\",\n            \"mtu\": 1300,\n            \"netAttachDefName\": \"ns1/l3-network\"\n    }\n</code></pre>"},{"location":"features/multiple-networks/multi-homing/#network-configuration-reference","title":"Network Configuration reference:","text":"<ul> <li><code>name</code> (string, required): the name of the network. This attribute is not namespaced.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"layer3\".</li> <li><code>subnets</code> (string, required): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> </ul> <p>[!NOTE] the <code>subnets</code> attribute indicates both the subnet across the cluster, and per node.   The example above means you have a /16 subnet for the network, but each node has   a /24 subnet.</p> <p>[!NOTE] routed - layer3 - topology networks only allow for east/west traffic.</p>"},{"location":"features/multiple-networks/multi-homing/#switched-layer-2-topology","title":"Switched - layer 2 - topology","text":"<p>This topology interconnects the workloads via a cluster-wide logical switch.</p> <p>The following net-attach-def configures the attachment to a layer 2 secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l2-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"1.0.0\",\n            \"name\": \"tenantyellow\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer2\",\n            \"subnets\": \"10.100.200.0/24\",\n            \"mtu\": 1300,\n            \"netAttachDefName\": \"ns1/l2-network\",\n            \"excludeSubnets\": \"10.100.200.0/29\"\n    }\n</code></pre>"},{"location":"features/multiple-networks/multi-homing/#network-configuration-reference_1","title":"Network Configuration reference","text":"<ul> <li><code>name</code> (string, required): the name of the network. This attribute is not namespaced.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"layer2\".</li> <li><code>subnets</code> (string, optional): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> <li><code>excludeSubnets</code> (string, optional): a comma separated list of CIDRs / IPs.   These IPs will be removed from the assignable IP pool, and never handed over   to the pods.</li> <li><code>allowPersistentIPs</code> (boolean, optional): persist the OVN-Kubernetes assigned   IP addresses in a <code>ipamclaims.k8s.cni.cncf.io</code> object. This IP addresses will   be reused by other pods if requested. Useful for KubeVirt VMs. Only makes   sense if the <code>subnets</code> attribute is also defined.</li> </ul> <p>[!NOTE] when the subnets attribute is omitted, the logical switch implementing the   network will only provide layer 2 communication, and the users must configure   IPs for the pods. Port security will only prevent MAC spoofing.</p> <p>[!NOTE] switched - layer2 - secondary networks only allow for east/west traffic.</p>"},{"location":"features/multiple-networks/multi-homing/#switched-localnet-topology","title":"Switched - localnet - topology","text":"<p>This topology interconnects the workloads via a cluster-wide logical switch to a physical network.</p> <p>The following net-attach-def configures the attachment to a localnet secondary network.</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: localnet-network\n  namespace: ns1\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"1.0.0\",\n            \"name\": \"tenantblack\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"localnet\",\n            \"subnets\": \"202.10.130.112/28\",\n            \"vlanID\": 33,\n            \"mtu\": 1500,\n            \"netAttachDefName\": \"ns1/localnet-network\"\n    }\n</code></pre> <p>Note that in order to connect to the physical network, it is expected that ovn-bridge-mappings is configured appropriately on the chassis for this localnet network.</p>"},{"location":"features/multiple-networks/multi-homing/#network-configuration-reference_2","title":"Network Configuration reference","text":"<ul> <li><code>name</code> (string, required): the name of the network.</li> <li><code>type</code> (string, required): \"ovn-k8s-cni-overlay\".</li> <li><code>topology</code> (string, required): \"localnet\".</li> <li><code>subnets</code> (string, optional): a comma separated list of subnets. When multiple subnets   are provided, the user will get an IP from each subnet.</li> <li><code>mtu</code> (integer, optional): explicitly set MTU to the specified value. Defaults to the value chosen by the kernel.</li> <li><code>netAttachDefName</code> (string, required): must match <code>&lt;namespace&gt;/&lt;net-attach-def name&gt;</code>   of the surrounding object.</li> <li><code>excludeSubnets</code> (string, optional): a comma separated list of CIDRs / IPs.   These IPs will be removed from the assignable IP pool, and never handed over   to the pods.</li> <li><code>vlanID</code> (integer, optional): assign VLAN tag. Defaults to none.</li> <li><code>allowPersistentIPs</code> (boolean, optional): persist the OVN-Kubernetes assigned   IP addresses in a <code>ipamclaims.k8s.cni.cncf.io</code> object. This IP addresses will   be reused by other pods if requested. Useful for KubeVirt VMs. Only makes   sense if the <code>subnets</code> attribute is also defined.</li> <li><code>physicalNetworkName</code> (string, optional): the name of the physical network to   which the OVN overlay will connect. When omitted, it will default to the value   of the localnet network name on the NAD's <code>.spec.config.name</code>.</li> </ul> <p>[!NOTE] when the subnets attribute is omitted, the logical switch implementing the   network will only provide layer 2 communication, and the users must configure   IPs for the pods. Port security will only prevent MAC spoofing.</p> <p>[!NOTE] updates to the network specification require the attached workloads restart. All the network-attachment-definitions    pointing to the same network must have a consistent configuration, and then workloads must be restarted.</p>"},{"location":"features/multiple-networks/multi-homing/#setting-a-secondary-network-on-the-pod","title":"Setting a secondary-network on the pod","text":"<p>The user must specify the secondary-network attachments via the <code>k8s.v1.cni.cncf.io/networks</code> annotation.</p> <p>The following example provisions a pod with two secondary attachments, one for each of the attachment configurations presented in Configuring secondary networks.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/networks: l3-network,l2-network\n  name: tinypod\n  namespace: ns1\nspec:\n  containers:\n  - args:\n    - pause\n    image: registry.k8s.io/e2e-test-images/agnhost:2.36\n    imagePullPolicy: IfNotPresent\n    name: agnhost-container\n</code></pre>"},{"location":"features/multiple-networks/multi-homing/#setting-static-ip-addresses-on-a-pod","title":"Setting static IP addresses on a pod","text":"<p>The user can specify attachment parameters via network-selection-elements, namely IP, MAC, and interface name.</p> <p>Refer to the following yaml for an example on how to request a static IP for a pod, a MAC address, and specify the pod interface name.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/networks: '[\n      {\n        \"name\": \"l2-network\",\n        \"mac\": \"02:03:04:05:06:07\",\n        \"interface\": \"myiface1\",\n        \"ips\": [\n          \"192.0.2.20/24\"\n        ]\n      }\n    ]'\n  name: tinypod\n  namespace: ns1\nspec:\n  containers:\n  - args:\n    - pause\n    image: registry.k8s.io/e2e-test-images/agnhost:2.36\n    imagePullPolicy: IfNotPresent\n    name: agnhost-container\n</code></pre> <p>[!NOTE] the user can specify the IP address for a pod's secondary attachment   only for an L2 or localnet attachment.</p> <p>[!NOTE] specifying a static IP address for the pod is only possible when the   attachment configuration does not feature subnets.</p>"},{"location":"features/multiple-networks/multi-homing/#multiple-interfaces-on-the-same-network","title":"Multiple interfaces on the same network","text":"<p>OVN-Kubernetes supports attaching a pod to the same non-primary user-defined network multiple times, allowing the pod to have multiple interfaces connected to the same network. This is useful for workloads that require multiple network interfaces on the same network for advanced networking scenarios. Note that only layer 2 and layer 3 networks are supported.</p> <p>To request multiple interfaces on the same network, specify the network multiple times in the <code>k8s.v1.cni.cncf.io/networks</code> annotation:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/networks: l3-network,l3-network\n  name: multi-nic-pod\n  namespace: ns1\nspec:\n  containers:\n  - args:\n    - pause\n    image: registry.k8s.io/e2e-test-images/agnhost:2.36\n    imagePullPolicy: IfNotPresent\n    name: agnhost-container\n</code></pre> <p>In this example, the pod will have two interfaces both connected to the <code>l3-network</code>. Each interface will receive its own IP address from the network's subnet.</p>"},{"location":"features/multiple-networks/multi-homing/#persistent-ip-addresses-for-virtualization-workloads","title":"Persistent IP addresses for virtualization workloads","text":"<p>OVN-Kubernetes provides persistent IP addresses for virtualization workloads, allowing VMs to have the same IP addresses when they migrate, when they restart, and when they stop, the resume operation.</p> <p>For that, the network admin must configure the network accordingly - the <code>allowPersistentIPs</code> flag must be enabled in the NAD of the network. As with the other network knobs, all NADs pointing to the same network must feature the same configuration - i.e. all NADs in the network must either allow (or reject) persistent IPs.</p> <p>The client application (which creates the VM, and manages its lifecycle) is responsible for creating the <code>ipamclaims.k8s.cni.cncf.io</code> object, and point to it in the network selection element upon pod creation; OVN-Kubernetes will then persist the IP addresses it has allocated the pod in the <code>IPAMClaim</code>. This flow is portrayed in the sequence diagram below.</p> <pre><code>sequenceDiagram\n  actor user\n  participant KubeVirt\n  participant apiserver\n  participant OVN-Kubernetes\n\n  user-&gt;&gt;KubeVirt: createVM(name=vm-a)\n  KubeVirt--&gt;&gt;user: OK\n\n  KubeVirt-&gt;&gt;apiserver: createIPAMClaims(networks=...)\n  apiserver--&gt;&gt;KubeVirt: OK\n\n  KubeVirt-&gt;&gt;apiserver: createPOD(ipamClaims=...)\n  apiserver--&gt;&gt;KubeVirt: OK\n\n  apiserver-&gt;&gt;OVN-Kubernetes: reconcilePod(podKey=...)\n  OVN-Kubernetes-&gt;&gt;OVN-Kubernetes: ips = AllocateNextIPs(nad.subnet)\n  OVN-Kubernetes-&gt;&gt;apiserver: IPAMClaim.UpdateStatus(status.ips = ips)\n  apiserver--&gt;&gt;OVN-Kubernetes: OK\n</code></pre> <p>Whenever a VM is migrated, restarted, or stopped / then started a new pod will be scheduled to host the VM; it will also point to the same <code>IPAMClaim</code>s, and OVN-Kubernetes will fulfill the IP addresses being requested by the client. This flow is shown in the sequence diagram below.</p> <pre><code>sequenceDiagram\n  actor user\n  participant KubeVirt\n  participant apiserver\n  participant OVN-Kubernetes\n\n  user-&gt;&gt;KubeVirt: startVM(vmName) or migrateVM(vmName)\n  KubeVirt--&gt;&gt;user: OK\n\n  note over KubeVirt: podName := \"launcher-\"\n  KubeVirt-&gt;&gt;apiserver: createPod(name=podName, ipam-claims=...)\n  apiserver--&gt;&gt;KubeVirt: OK\n\n  apiserver-&gt;&gt;OVN-Kubernetes: reconcilePod(podKey=...)\n  OVN-Kubernetes-&gt;&gt;OVN-Kubernetes: ipamClaim := readIPAMClaim(claimName)\n  OVN-Kubernetes-&gt;&gt;OVN-Kubernetes: allocatePodIPs(ipamClaim.Status.IPs)\n\n<p>Managing the life-cycle of the <code>IPAMClaim</code>s objects is the responsibility of the\nclient application that created them in the first place. In this case, KubeVirt.</p>\n<p>This feature is described in detail in the following KubeVirt\ndesign proposal.</p>"},{"location":"features/multiple-networks/multi-homing/#ipv4-and-ipv6-dynamic-configuration-for-virtualization-workloads-on-l2-primary-udn","title":"IPv4 and IPv6 dynamic configuration for virtualization workloads on L2 primary UDN","text":"<p>For virtualization workloads using a primary UDN with layer2 topology ovn-k \nconfigure some DHCP and NDP flows to server ipv4 and ipv6 configuration for them.</p>\n<p>For both ipv4 and ipv6 the following parameters are configured using DHCP or RAs:\n- address\n- gateway\n- dns (read notes below)\n- hostname (vm's name)\n- mtu (taken from network attachment definition)</p>"},{"location":"features/multiple-networks/multi-homing/#configuring-dns-server","title":"Configuring dns server","text":"<p>By default, the DHCP server at ovn-kubernetes will configure the kubernetes\ndefault dns service <code>kube-system/kube-dns</code> as the name server. This can be\noverridden with the following command line options:\n- dns-service-namespace\n- dns-service-name</p>"},{"location":"features/multiple-networks/multi-homing/#limitations","title":"Limitations","text":"<p>OVN-Kubernetes currently does not support:</p>\n<ul>\n<li>updates to the network selection elements lists - i.e. <code>k8s.v1.cni.cncf.io/networks</code> annotation</li>\n<li>external IPAM - i.e. the user can't define the IPAM attribute in the configuration. They must use the subnets\n  attribute.</li>\n<li>IPv6 link local addresses not derived from the MAC address as described in RFC 2373, like  Privacy Extensions defined by RFC 4941, \n  or the Opaque Identifier generation methods defined in RFC 7217.</li>\n</ul>"},{"location":"features/multiple-networks/multi-network-policies/","title":"MultiNetworkPolicies","text":""},{"location":"features/multiple-networks/multi-network-policies/#introduction","title":"Introduction","text":"<p>multi-networkpolicy provides network policy features for secondary networks, and allows enhanced traffic security.</p>"},{"location":"features/multiple-networks/multi-network-policies/#motivation","title":"Motivation","text":"<p>In Kubernetes the paradigm is that by default, all pods can reach other pods, and security is provided by implementing Network Policy. MultiNetworkPolicies provide the same security solution to secondary interfaces. For users seeking network security using multi-homing - traffic restrictions based on user network configuration is a recommended practice.</p> <p>For example, by defining policies that specify which services a workload can access, they mitigate supply chain attacks and reduce the risk of lateral movement. Even if a component is compromised, it won\u2019t be able to jeopardize other components of the system.</p>"},{"location":"features/multiple-networks/multi-network-policies/#user-stories","title":"User-Stories","text":"<p>As a project administrator, I want to enforce network policies that must be adhered to by all workloads on my namespace. This will secure my namespace\u2019s network traffic.</p>"},{"location":"features/multiple-networks/multi-network-policies/#user-case","title":"User-case","text":"<ul> <li>Limit access between workloads running on the cluster connected on a flat layer2 topology:   In this case example, VMA is a data processor, VMC is a real-time data producer, and VMB is a monitoring and alerting unit.   The data producer (VMC) can send data to the data processor (VMA), but cannot communicate with the monitoring unit (VMB).   The data processor (VMA) can send alerts and metrics to the monitoring unit (VMB).    </li> <li>Limit access to a database set outside the cluster, connected to VM and Pod workloads via localnet overlay network:   In this case example, only the VM has access to the database.    </li> </ul>"},{"location":"features/multiple-networks/multi-network-policies/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>The <code>multi-network-policies</code> feature must be enabled in the OVN-Kubernetes configuration. Please use the <code>Feature Config</code> option <code>enable-multi-network</code> under <code>OVNKubernetesFeatureConfig</code> config to enable it.</p>"},{"location":"features/multiple-networks/multi-network-policies/#workflow-description","title":"Workflow Description","text":"<p>To configure pod isolation, the user must: - provision a <code>network-attachment-definition</code>. - provision a <code>MultiNetworkPolicy</code> indicating to which secondary networks it   applies via the   policy-for   annotation.</p> <p>Please refer to the following example:</p> <pre><code>---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: tenant-blue\nspec:\n    config: '{\n        \"cniVersion\": \"0.4.0\",\n        \"name\": \"tenant-blue-net\",\n        \"netAttachDefName\": \"default/tenant-blue\",\n        \"topology\": \"layer2\",\n        \"type\": \"ovn-k8s-cni-overlay\",\n        \"subnets\": \"192.168.100.0/24\"\n    }'\n---\napiVersion: k8s.cni.cncf.io/v1beta1\nkind: MultiNetworkPolicy\nmetadata:\n  annotations:\n    k8s.v1.cni.cncf.io/policy-for: default/tenant-blue # indicates the net-attach-defs this policy applies to\n  name: allow-ports-same-ns\nspec:\n  podSelector:\n    matchLabels:\n      app: stuff-doer # the policy will **apply** to all pods with this label\n  ingress:\n  - ports:\n    - port: 9000\n      protocol: TCP\n    from:\n    - namespaceSelector:\n        matchLabels:\n          role: trusted # only pods on namespaces with this label will be allowed on port 9000\n  policyTypes:\n    - Ingress\n</code></pre> <p>Please note the <code>MultiNetworkPolicy</code> has the exact same API of the native <code>networking.k8s.io/v1</code> <code>NetworkPolicy</code>object; check its documentation for more information.</p> <p>[!NOTE] <code>net-attach-def</code>s referred to by the <code>k8s.v1.cni.cncf.io/policy-for</code> annotation without the subnet attribute defined are possible if the policy only features <code>ipBlock</code> peers. If the <code>net-attach-def</code> features the <code>subnet</code> attribute, it can also feature <code>namespaceSelectors</code> and <code>podSelectors</code>.</p>"},{"location":"features/multiple-networks/multi-network-policies/#user-facing-api-changes","title":"User facing API Changes","text":"<p>There are no user facing API Changes.</p>"},{"location":"features/multiple-networks/multi-vtep/","title":"Multi-VTEP","text":""},{"location":"features/multiple-networks/multi-vtep/#introduction","title":"Introduction","text":"<p>When a K8s cluster has multiple networks and K8s nodes have multiple SR-IOV network adapters to provide different overlaying virtual networks over the physical networks, multiple VTEP(Virtual Tunnel EndPoint) devices are required for encapsulating and de-encapsulating the network traffic from/to virtual networks. The challenge is in deciding which VTEP to use to send/receive the packets to/from the wire. This feature instructs OVN to use the appropriate VTEP IP as the source or destination IP for east-west traffic.</p>"},{"location":"features/multiple-networks/multi-vtep/#motivation","title":"Motivation","text":"<p>For applications with network traffic that can be categorized by purpose and are sensitive to network latency, it is advantageous to use multiple NICs designated for different purposes. This enhances the performance, security, and flexibility of the network. Additionally, with network adapters that support SR-IOV, traffic can be offloaded to hardware, further reducing network latency and decreasing host CPU usage.</p>"},{"location":"features/multiple-networks/multi-vtep/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>Consider a cloud environment providing online video streaming, where each node is equipped with three NICs. One NIC could be dedicated to management traffic, another to video streaming traffic, and the third to storage. In this setup, each NIC would have its own VTEP IP. This configuration ensures that overlay traffic from a Pod\u2019s VF is steered through the specific NIC associated with that VF. For example, streaming traffic must be routed exclusively through the second NIC, optimizing network performance and traffic segregation.</p> <p></p> <ul> <li>vf<sup>R</sup> is VF representor.</li> </ul>"},{"location":"features/multiple-networks/multi-vtep/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>On OVN-Kubernetes side, no additional configuration required to enable this feature.</p> <p>This feature depends on a specific underlay network setup; it cannot be turned on without an adequate underlay network configuration.</p>"},{"location":"features/multiple-networks/multi-vtep/#implementation-details","title":"Implementation Details","text":"<p>This feature requires an OVN version newer than v24.03.2.</p> <p>When the source and destination are overlay IPs, the packets need to be encapsulated and sent through a tunnel. When there are multiple NICs used for tunneling traffic, each with a different VTEP IP, OVN needs to select the right tunnel endpoint to send the traffic, based on the below criteria:</p> <ul> <li>The tunnel/NIC selection should be based on the source &amp; destination   interface of the traffic, which should be aligned with the VF selection   specified by ovn-kubernetes.</li> <li>The source tunnel IP should be selected according to the source   interface of the packet.</li> <li>The destination tunnel IP should be selected according to the destination   interface of the packet.</li> <li>If an interface is not assigned a VTEP, traffic to/from the interface can   choose any VTEP of the host.</li> <li>For BUM(broadcast, unknown-unicast, and multicast traffic) traffic, it   is ok to select any VTEP.</li> </ul>"},{"location":"features/multiple-networks/multi-vtep/#ovn-constructs-created-in-the-databases","title":"OVN Constructs created in the databases","text":"<p>OVN Sourthbound Database: * Port_Binding:encap - Populated by ovn-controller according to VIF   interface\u2019s external_ids:ovn-encap-ip. * Encap:IP - The IP in the corresponding Encap record will be used   as the VTEP IP.</p> <p>OVS Database on chassis: * Each Chassis may have multiple Encaps, determined by   <code>open_vswitch:external_ids:ovn-encap-ip</code> which is a comma separated   string of IPs. * Each VIF interface (if needs to be assigned to a particular VTEP)   will have <code>external_ids:ovn-encap-ip</code>, which must be one of the IPs   configured in open_vswitch:external_ids:ovn-encap-ip of the chassis. * A option <code>open_vswitch:external_ids:ovn-pf-encap-ip-mapping</code> is   added to define the mapping between PF interface name and its VTEP IP.</p>"},{"location":"features/multiple-networks/multi-vtep/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p>To support this feature, the VTEP interfaces must be configured in advance. This can be accomplished using a system network management tool, such as Network Manager.</p> <p>Assuming a node has 3 NICs and VTEP interfaces: | PF       | VTEP  | VTEP IP      | | :------- | :---- | :----------- | | enp1s0f0 | vtep0 | 10.0.0.1/23 | | enp2s0f0 | vtep1 | 10.0.0.2/23 | | enp3s0f0 | vtep2 | 10.0.0.3/23 |</p> <p>The VTEP interfaces on OVS bridges should be configured as below: <pre><code>$ ovs-vsctl  show\ne620a080-c761-44d0-adb3-203bb1df6d67\n    Bridge brenp1s0f1\n        ...\n        Port vtep0\n            tag: 25 # just an example VLAN-ID\n            Interface vtep0\n                type: internal\n    Bridge brenp2s0f0\n        ...\n        Port vtep1\n            tag: 25\n            Interface vtep1\n                type: internal\n    Bridge brenp3s0f0\n        ...\n        Port vtep2\n            tag: 25\n            Interface vtep2\n                type: internal\n</code></pre></p> <p>Below Open_vSwitch external_ids need to be configured in osvsdb: - <code>external_ids:ovn-encap-ip</code>: set it to a list of vtep IPs, separated by   commas, in any order. - <code>external_ids:ovn-pf-encap-ip-mapping</code>: set it to list of vtep-interface   to IP pairs separated by comma.</p> <p>For example: <pre><code>$ ovs-vsctl --timeout=15 --if-exists get Open_vSwitch . external_ids:ovn-encap-ip\n\"10.0.0.1,10.0.0.2,10.0.0.3\"\n\n$ ovs-vsctl --timeout=15 --if-exists get Open_vSwitch . external_ids:ovn-pf-encap-ip-mapping\n\"enp1s0f0:10.0.0.1,enp2s0f0:10.0.0.2,enp3s0f0:10.0.0.3\"\n</code></pre> With above <code>external_ids:ovn-encap-ip</code> settings, OVN creates 3 tunnel ports for each encap-ip on remote node, that's, if remote node has 2 encap-ips in <code>external_ids:ovn-encap-ip</code>, 6 tunnel ports will be created on the local node. For example, if remote node only has one encap-ip \"10.0.0.10\", OVN creates below tunnel ports on the local node: <pre><code>$ ovs-vsctl  show\n    Bridge br-int\n        Port ovn0-5e3e5-0\n            Interface ovn0-5e3e5-0\n                type: geneve\n                options: {csum=\"true\", key=flow, local_ip=\"10.0.0.1\", remote_ip=\"10.0.0.10\", tos=inherit}\n        Port ovn0-e815c-1\n            Interface ovn0-e815c-1\n                type: geneve\n                options: {csum=\"true\", key=flow, local_ip=\"10.0.0.2\", remote_ip=\"10.0.0.10\", tos=inherit}\n        Port ovn-33f2e5-0\n            Interface ovn-33f2e5-0\n                type: geneve\n                options: {csum=\"true\", key=flow, local_ip=\"10.0.0.3\", remote_ip=\"10.0.0.10\", tos=inherit}\n</code></pre></p> <p>Ovn-controller programs physical flows that create mappings between  and  according to the aforementioned data model. The  information needs to be carried through a global register. The bits 16-31 of the reg13 can be used for this purpose. The reg13 is currently used to store conntrack zones for each VIF. However, it doesn\u2019t really require 32 bits, and the first 16 bits (0-15) are sufficient. Its current usage with assignment of all 32 bits in certain pipelines need to be updated. <p>When ovnkube-node adding a Pod's network interfaces to OVS br-int during the Pod creation, it identifies the corresponding PF interface for each VF interface, it then looks up <code>external_ids:ovn-pf-encap-ip-mapping</code> to determine the encap IP, which is subsequently assigned to the OVS Port's <code>external_ids:ovn-encap-ip</code>. This encap IP will be utilized as the source IP in the tunnel header.</p> <p>To steer outgoing traffic through the correct physical interface, the following source-based routing rules are required on K8s node:</p> <pre><code>$ ip rule list\n...\n6081:   from 10.0.0.1 lookup 6081 proto static\n6082:   from 10.0.0.2 lookup 6082 proto static\n6083:   from 10.0.0.3 lookup 6083 proto static\n\n$ ip route list table 6081\n10.0.0.0/23 dev vtep0 scope link\n10.0.0.0/16 via 10.0.0.1 dev vtep0 proto static metric 804\n$ ip route list table 6082\n10.0.0.0/23 dev vtep1 scope link\n10.0.0.0/16 via 10.0.0.1 dev vtep1 proto static metric 802\n$ ip route list table 6083\n10.0.0.0/23 dev vtep2 scope link\n10.0.0.0/16 via 10.0.0.1 dev vtep2 proto static metric 806\n</code></pre>"},{"location":"features/multiple-networks/multi-vtep/#known-limitations","title":"Known Limitations","text":"<p>This feature only works with network adapters that support SR-IOV, e.g. NVIDIA CONNECTX-6 or NVIDIA BlueField-2 in NIC mode.</p>"},{"location":"features/network-security-controls/admin-network-policy/","title":"AdminNetworkPolicy","text":""},{"location":"features/network-security-controls/admin-network-policy/#introduction","title":"Introduction","text":"<p>Being able to enforce airtight application security at the cluster-wide level has been a popular ask from cluster administrators. Key admin user stories include:</p> <ul> <li>As a cluster administrator, I want to enforce non-overridable admin network   policies that have to be adhered to by all user tenants in the cluster, thus   securing my cluster\u2019s network traffic.</li> <li>As a cluster administrator, I want to implement specific network restrictions   across multiple tenants (a tenant here refers to one or more namespaces),   thus facilitating network multi-tenancy.</li> <li>As a cluster administrator, I want to enforce the tenant isolation model   (a tenant cannot receive traffic from any other tenant) as the cluster\u2019s default   network security standard, thus delegating the responsibility of explicitly allowing   traffic from other tenants to the tenant owner using NetworkPolicies.</li> </ul> <p>AdminNetworkPolicy has been designed precisely to solve these problems. It provides a comprehensive cluster-wide network security solution for cluster administrators using two cluster-scoped CRDs:</p> <ul> <li>AdminNetworkPolicy API and</li> <li>BaselineAdminNetworkPolicy API</li> </ul> <p>which are shipped by the sig-network-policy-api working group as out-of-tree Kubernetes APIs. Both these APIs can be used to define cluster-wide admin policies in order to secure your cluster\u2019s network traffic. These APIs are currently in the v1alpha1 version and subject to change based on upstream developments before becoming stable v1.</p>"},{"location":"features/network-security-controls/admin-network-policy/#motivation","title":"Motivation","text":"<p>NetworkPolicies in Kubernetes were created for developers to be able to secure their applications. Hence they are under the control of potentially untrusted application authors. They are namespace scoped and are not suitable for solving the above user stories for cluster administrators because:</p> <ul> <li>They are not cluster-scoped and hence its hard to define a network of policies   that spans across namespaces - you'd have to duplicate the same policy over and over   in each namespace</li> <li>They cannot be created before the namespace is created (KAPI server expects the namespace   to be created first since it's namespace scoped); thus they cannot satisfy the requirements   where admins may want the policies to be applicable to arbitrary future   namespaces that could get created in the cluster.</li> </ul> <p>Another motivation was to fix some of the usability aspects of the NetworkPolicy API. NetworkPolicy API is designed with an implicit deny-all policy, meaning that with the first created policy all traffic not allowed by it will be denied. NetworkPolicy only defines which connection will be allowed. Then we make other exceptions like <code>except</code> blocks in <code>ipBlock</code> construct which is an explicit what not to allow within the allowList. Network administrators prefer to have the power of defining what exactly to deny and allow instead of this implicit model.</p>"},{"location":"features/network-security-controls/admin-network-policy/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>Check out the original User Stories for AdminNetworkPolicy. Follow the new developments for this API via the Network Policy Enhancement Proposals. Let us look at some practical use cases of when to use admin network policies in your cluster:</p> <ul> <li>Isolate tenants</li> </ul> <p></p> <ul> <li>Always allow egress traffic from all workloads to DNS namespace or splunk namespace</li> </ul> <p></p> <ul> <li>Always allow ingress traffic from monitoring namespace to all workloads</li> </ul> <p></p> <ul> <li>Delegate to namespace scoped network policies (we will learn more about what this is   in the following sections)</li> </ul> <p></p> <ul> <li>Always deny traffic to sensitive namespaces from everywhere</li> </ul> <p></p>"},{"location":"features/network-security-controls/admin-network-policy/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>This feature is enabled by default on all OVN-Kubernetes clusters. You don't need to do anything extra to start using this feature. There is a Feature Config option <code>enable-admin-network-policy</code> under <code>OVNKubernetesFeatureConfig</code> config that can be used to disable this feature. However note that disabling the feature will not remove existing CRs in the cluster.</p> <p>When the feature is enabled, the CRDs are installed on the cluster and the <code>AdminNetworkPolicy</code> controller is invoked.</p> <pre><code>$ oc get crd adminnetworkpolicies.policy.networking.k8s.io\nNAME                                            CREATED AT\nadminnetworkpolicies.policy.networking.k8s.io   2023-10-01T11:33:58Z\n$ oc get crd baselineadminnetworkpolicies.policy.networking.k8s.io\nNAME                                                    CREATED AT\nbaselineadminnetworkpolicies.policy.networking.k8s.io   2023-10-01T11:33:58Z\n</code></pre>"},{"location":"features/network-security-controls/admin-network-policy/#workflow-description","title":"Workflow Description","text":""},{"location":"features/network-security-controls/admin-network-policy/#implementation-details","title":"Implementation Details","text":""},{"location":"features/network-security-controls/admin-network-policy/#user-facing-api-changes","title":"User facing API Changes","text":"<p>Like mentioned above, the API lives in the kubernetes-sigs repo.</p>"},{"location":"features/network-security-controls/admin-network-policy/#adminnetworkpolicy-sample-api","title":"AdminNetworkPolicy Sample API","text":"<p>Let us look at a sample ANP yaml:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: cluster-control\nspec:\n  priority: 34\n  subject:\n    namespaces: {}\n  ingress:\n  - name: \"allow-from-ingress-nginx\" # rule0\n    action: \"Allow\"\n    from:\n    - namespaces:\n        matchLabels:\n          kubernetes.io/metadata.name: ingress-nginx\n  - name: \"allow-from-monitoring\" # rule1\n    action: \"Allow\"\n    from:\n    - namespaces:\n        matchLabels:\n          kubernetes.io/metadata.name: monitoring\n    ports:\n    - portNumber:\n        protocol: TCP\n        port: 7564\n    - namedPort: \"scrape\"\n  - name: \"allow-from-open-tenants\" # rule2\n    action: \"Allow\"\n    from:\n    - namespaces: # open tenants\n        matchLabels:\n          tenant: open\n  - name: \"pass-from-restricted-tenants\" # rule3\n    action: \"Pass\"\n    from:\n    - namespaces: # restricted tenants\n        matchLabels:\n          tenant: restricted\n  - name: \"default-deny\" # rule4\n    action: \"Deny\"\n    from:\n    - namespaces: {}\n  egress:\n  - name: \"allow-to-dns\" # rule0\n    action: \"Allow\"\n    to:\n    - pods:\n        namespaceSelector:\n          matchLabels:\n            kubernetes.io/metadata.name: kube-system\n        podSelector:\n          matchLabels:\n            app: dns\n    ports:\n    - portNumber:\n        protocol: UDP\n        port: 5353\n  - name: \"allow-to-kapi-server\" # rule1\n    action: \"Allow\"\n    to:\n    - nodes:\n        matchExpressions:\n        - key: node-role.kubernetes.io/control-plane\n          operator: Exists\n    ports:\n    - portNumber:\n        protocol: TCP\n        port: 6443\n  - name: \"allow-to-splunk\" # rule2\n    action: \"Allow\"\n    to:\n    - namespaces:\n        matchLabels:\n          tenant: splunk\n    ports:\n    - portNumber:\n        protocol: TCP\n        port: 8991\n    - portNumber:\n        protocol: TCP\n        port: 8992\n  - name: \"allow-to-open-tenants-and-intranet-and-worker-nodes\" # rule3\n    action: \"Allow\"\n    to:\n    - nodes: # worker-nodes\n        matchExpressions:\n        - key: node-role.kubernetes.io/worker\n          operator: Exists\n    - networks: # intranet\n      - 172.30.0.0/30\n      - 10.0.54.0/19\n      - 10.0.56.38/32\n      - 10.0.69.0/24\n    - namespaces: # open tenants\n        matchLabels:\n          tenant: open\n  - name: \"pass-to-restricted-tenants\" # rule4\n    action: \"Pass\"\n    to:\n    - namespaces: # restricted tenants\n        matchLabels:\n          tenant: restricted\n  - name: \"default-deny\"\n    action: \"Deny\"\n    to:\n    - networks:\n      - 0.0.0.0/0\n</code></pre> <ul> <li><code>subject</code> is the set of pods selected by <code>cluster-control</code> ANP, on   which the rules in the policy are applied on. Here its all the pods   in the cluster.</li> <li>Each AdminNetworkPolicy resource will have a<code>.spec.priority</code> field. <code>priority</code>   is 34 here that determines the priority of this ANP versus other   ANPs in your cluster.  The lower the number the higher the precedence.   Thus 0 is the highest priority and 99 (the largest number supported   by OVN-Kubernetes) is the lowest priority.</li> <li><code>ingress</code> has 5 rules:<ul> <li>rule0: always allow ingress traffic coming from ingress-nginx namespace   to all the pods in the cluster</li> <li>rule1: always allow TCP ingress traffic coming from port 7564 of   pods in monitoring namespace</li> <li>rule2: always allow all ingress traffic from <code>open</code> tenant's namespaces</li> <li>rule3: always pass all ingress traffic from <code>restricted</code> tenant's namespaces   (please check the following sections to learn more about what <code>pass</code> is)</li> <li>rule4: deny all other ingress traffic including same namespace pods</li> </ul> </li> <li><code>egress</code> has 6 rules:<ul> <li>rule0: always allow egress traffic coming from all pods to dns pods</li> <li>rule1: always allow TCP egress traffic coming from all pods to KAPI pods</li> <li>rule2: always allow all egress traffic coming from all pods to splunk logging   tenant at TCP ports 8991 and 8992</li> <li>rule3: always allow all egress traffic to <code>open</code> tenant's namespaces, company's   intranet CIDR ranges and all worker nodes in the cluster</li> <li>rule4: always pass all egress traffic to <code>restricted</code> tenant's namespaces   (please check the following sections to learn more about what <code>pass</code> is)</li> <li>rule5: deny all other egress traffic including same namespace pods</li> </ul> </li> </ul> <p>You can do <code>oc apply -f cluster-control.yaml</code> to create this ANP in your cluster <pre><code>$ oc get anp\nNAME              PRIORITY   AGE\ncluster-control   34         3s\n</code></pre></p>"},{"location":"features/network-security-controls/admin-network-policy/#baselineadminnetworkpolicy-sample-api","title":"BaselineAdminNetworkPolicy Sample API","text":"<p>Unlike ANP's BANP is a singleton object in your cluster. BANP specifies baseline policies that apply unless they are overridden by a NetworkPolicy or ANP.</p> <p>Let us look at a sample BANP yaml:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: BaselineAdminNetworkPolicy\nmetadata:\n  name: default\nspec:\n  subject:\n    namespaces: {}\n  ingress:\n  - name: \"default-deny\"\n    action: \"Deny\"\n    from:\n    - namespaces: {}\n  egress:\n  - name: \"default-deny\"\n    action: \"Deny\"\n    to:\n    - namespaces: {}\n</code></pre> <p>You can do <code>oc apply -f default.yaml</code> to create this ANP in your cluster <pre><code>$ oc get banp\nNAME      AGE\ndefault   16m\n</code></pre></p> <p>Note that when you use <code>{}</code> selectors to select pods you are selecting ALL the pods in the cluster including <code>kube-system</code> and <code>ovn-kubernetes</code> namespaces which is something you probably don't want to do. If you do this, then the cluster will go into a non-functional state since basic infrastructued pods cannot communicate unless you explicitly define higher priority allow rules.</p> <p>Almost always you probably need a policy that looks like: <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: BaselineAdminNetworkPolicy\nmetadata:\n  name: default\nspec:\n  subject:\n    namespaces:\n      matchLabels:\n          type: all-workload-namespaces\n  ingress:\n  - name: \"default-deny\"\n    action: \"Deny\"\n    from:\n    - namespaces:\n        matchLabels:\n          type: all-workload-namespaces\n  egress:\n  - name: \"default-deny\"\n    action: \"Deny\"\n    to:\n    - namespaces:\n        matchLabels:\n          type: all-workload-namespaces\n</code></pre></p>"},{"location":"features/network-security-controls/admin-network-policy/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p>We have a new level driven controller in OVN-Kubernetes called admin_network_policy which watches for the following objects:</p> <ul> <li>AdminNetworkPolicy</li> <li>BaselineAdminNetworkPolicy</li> <li>Namespaces</li> <li>Pods</li> <li>Nodes</li> </ul> <p>in your cluster. For each add, update and delete event that this controller cares about for each of the above objects, it add's it to the centralized ANP and BANP queue in a single threaded fashion.</p> <p>The controller maintains an internal cache state for each corresponding ANP and tries to level the current state configuration for a given ANP with the desired state configuration for that ANP.</p> <p>It also emits events like <code>ANPWithUnsupportedPriority</code> and <code>ANPWithDuplicatePriority</code> so that users are aware in those circumstances.</p>"},{"location":"features/network-security-controls/admin-network-policy/#pass-action-delegate-decision-to-networkpolicies","title":"Pass Action: Delegate decision to NetworkPolicies","text":"<p>In addition to setting <code>Deny</code> and <code>Allow</code> actions on ANP API rules, one can also use the <code>Pass</code> action for a rule. What this means is ANP controller defers the decision of connections that match the pass action rule to either the NetworkPolicy OR to the BaselineAdminNetworkPolicy defined in the cluster (if either of them match the same set of pods, then they will take effect and if not, the result will be an <code>Allow</code>). Order of precedence: <code>AdminNetworkPolicy (Tier1) &gt; NetworkPolicy(Tier2) &gt; BaselineAdminNetworkPolicy (Tier3)</code>.</p> <p></p> <p>Ingress rule3 and Egress rule4 are examples of <code>Pass</code> action. Such rules are created when the administrator wants to delegate the decision of whether that connection should be allowed or not to NetworkPolicies defined by tenant owners.</p> <p>If we now define a network policy in <code>restricted</code> tenant's namespace that matches on same ingress peers as in the subject of our <code>cluster-control</code> admin network policy, that network policy will take effect. This is how administrators can delegate a decision making to the namespace owners in a cluster.</p>"},{"location":"features/network-security-controls/admin-network-policy/#pass-action-delegate-decision-to-baselineadminnetworkpolicies","title":"Pass Action: Delegate decision to BaselineAdminNetworkPolicies","text":"<p>Since we can delegate decisions from administrators to namespace owners, what if namespace owners don't have policies in place for the same set of subjects? Admins in such cases might want to keep a default set of guardrails in the cluster. Thus we allow one BANP to be created in the cluster with the name <code>default</code>. BANP doesn't have any priority field, since we can have only one in the cluster. Rest of the implementation details for ANP is applicable to BANP as well.</p>"},{"location":"features/network-security-controls/admin-network-policy/#ovn-constructs-created-in-the-databases","title":"OVN Constructs created in the databases","text":"<ul> <li>Each AdminNetworkPolicy CRD can have upto 100 ingress rules and 100 egress rules,   thus 200 rules in total. The ordering of each rule is important. If the rule is   at the top of the list then it has the highest precedence and if the rule is   at the bottom it has the lowest precedence. Each rule translates to <code>nbdb.ACL</code>.   We will have:<ul> <li>one single <code>nbdb.ACL</code> if <code>len(rule.ports) == 0 &amp;&amp; len(rule.namedPorts) == 0</code></li> <li>one <code>nbdb.ACL</code> per protocol if <code>len(rule.ports) &gt; 0 and len(rule.namedPorts) == 0</code>   (so max 3 <code>nbdb.ACLs</code> (tcp,udp,sctp) per rule {<code>portNumber</code>, <code>portRange</code> type ports ONLY})</li> <li>one <code>nbdb.ACL</code> per protocol if <code>len(rule.namedPorts) &gt; 0 and len(rule.ports) == 0</code>   (so max 3 ACLs (tcp,udp,sctp) per rule {<code>namedPort</code> type ports ONLY})</li> <li>one ACL per protocol if <code>len(rule.ports) &gt; 0</code> and one ACL per protocol <code>if len(rule.namedPorts) &gt; 0</code> (so max 6 ACLs (2tcp,2udp,2sctp) per rule {<code>portNumber</code>, <code>portRange</code>, <code>namedPorts</code> all are present})</li> </ul> </li> <li>Since we can have upto 100 policies and each one can have upto 100 gress rules   (100*100), we have blocked out the range: 30,000 - 20,000 priority range for   the OVN <code>nbdb.ACL</code> table in the <code>Tier1</code> block for ANP's implementation.</li> <li>Since we can have only 1 BANP in the cluster, we keep <code>nbdb.ACL</code>'s priority range   1750 - 1649 range reserved for BANP ACLs in the <code>Tier3</code> block for BANP's implementation.</li> <li>Each AdminNetworkPolicy CRD will have a subject on which the policy is    applied on - this is translated to one <code>nbdb.PortGroup</code> on which the <code>nbdb.ACLs</code>   of each rule in the policy are attached on. So there is one <code>nbdb.PortGroup</code> per ANP created.</li> <li>Each gress rule can have upto 100 peers. Each rule will also create an <code>nbdb.AddressSet</code>   which will contain the IPs of all the pods, nodes and networks that are selected   by the peer selector across all the peers of that given rule.</li> </ul> <p>For the ANP created above, let us look at how these OVN constructs would look like using the following commands:</p> <p>NOTE: All commands must be run inside the ovnkube-node pod for interconnect mode and ovnkube-master pod for default mode since that is where the NBDB container is running.</p> <p><code>nbdb.ACL</code>: These can be listed using <code>ovn-nbctl list acl</code></p> <p>The Ingress ACLs for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : f4523ce3-fb80-4d68-a50b-8aa6a795f895\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"0\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:0:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a14545668191619617708))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:0\"\noptions             : {}\npriority            : 26600\nseverity            : []\ntier                : 1\n\n_uuid               : 4117970c-c138-45b5-a6de-8e5e54efcb04\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"1\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:1:tcp\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=tcp}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; tcp.dst==7564\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:1\"\noptions             : {}\npriority            : 26599\nseverity            : []\ntier                : 1\n\n_uuid               : 1b1b1409-b96a-4e6d-9776-00d88c0c7ac9\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"1\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:1:tcp-namedPort\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=tcp-namedPort}\nlabel               : 0\nlog                 : false\nmatch               : \"((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; ((ip4.dst == 10.244.1.4 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:1\"\noptions             : {}\npriority            : 26599\nseverity            : []\ntier                : 1\n\n_uuid               : 00400c05-7225-4d64-9428-8b2a04c6c3da\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"2\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:2:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13730899355151937870))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:2\"\noptions             : {}\npriority            : 26598\nseverity            : []\ntier                : 1\n\n_uuid               : 7f5ea19b-d465-4de4-8369-bdf66fdcd7d9\naction              : pass\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"3\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:3:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a764182844364804195))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:3\"\noptions             : {}\npriority            : 26597\nseverity            : []\ntier                : 1\n\n_uuid               : 3e82df77-ad40-4dce-bd96-5b9e7e0b3603\naction              : drop\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"4\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:4:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13814616246365836720))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Ingress:4\"\noptions             : {}\npriority            : 26596\nseverity            : []\ntier                : 1\n</code></pre> <p>The Egress ACLs for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : fdb266b7-758a-4471-9cc1-84d00dc4f2ae\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"0\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:0:udp\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=udp}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a13517855690389298082)) &amp;&amp; udp &amp;&amp; udp.dst==5353\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:0\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26600\nseverity            : []\ntier                : 1\n\n_uuid               : 8f1b8170-c350-4e9d-8ff0-a3177f9749ca\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"1\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:1:tcp\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=tcp}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a10706246167277696183)) &amp;&amp; tcp &amp;&amp; tcp.dst==6443\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:1\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26599\nseverity            : []\ntier                : 1\n\n_uuid               : 04dbde8f-eaa8-4973-a158-ec56d0be672f\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"2\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:2:tcp\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=tcp}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a18396736153283155648)) &amp;&amp; tcp &amp;&amp; tcp.dst=={8991,8992}\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:2\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26598\nseverity            : []\ntier                : 1\n\n_uuid               : c37d0794-8444-440e-87d0-02ef95c6e248\naction              : allow-related\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"3\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:3:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a10622494091691694581))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:3\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26597\nseverity            : []\ntier                : 1\n\n_uuid               : 2f3292f0-9676-4d02-a553-230c3a2885b8\naction              : pass\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"4\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:4:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a5972452606168369118))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:4\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26596\nseverity            : []\ntier                : 1\n\n_uuid               : 924b81fd-65cc-494b-9c29-0c5287d69bb1\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"5\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:5:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a11452480169090787059))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:5\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26595\nseverity            : []\ntier                : 1\n</code></pre> <p><code>nbdb.AdressSet</code>: These can be listed using <code>ovn-nbctl list address-set</code></p> <p>The Ingress Address-Sets for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : c51329b3-e1f6-48b7-b674-129819279bbe\naddresses           : [\"10.244.2.5\"]\nexternal_ids        : {direction=Ingress, gress-index=\"0\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:0:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a14545668191619617708\n\n_uuid               : 7e9e5dcd-8576-443b-8f32-0b31dca64e06\naddresses           : [\"10.244.1.4\", \"10.244.2.8\"]\nexternal_ids        : {direction=Ingress, gress-index=\"1\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:1:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a6786643370959569281\n\n_uuid               : 59c30748-af15-4a75-8f0c-a36eb134bbca\naddresses           : []\nexternal_ids        : {direction=Ingress, gress-index=\"2\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:2:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a13730899355151937870\n\n_uuid               : 191ea649-5849-4147-b07b-3f252c59cddb\naddresses           : [\"10.244.1.3\", \"10.244.2.7\"]\nexternal_ids        : {direction=Ingress, gress-index=\"3\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:3:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a764182844364804195\n\n_uuid               : 562cc870-5be5-43a4-a996-be3814ca5e49\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.3\", \"10.244.2.4\", \"10.244.2.5\", \"10.244.2.6\", \"10.244.2.7\", \"10.244.2.8\"]\nexternal_ids        : {direction=Ingress, gress-index=\"4\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Ingress:4:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a13814616246365836720\n</code></pre> <p>The Egress Address-Sets for the above AdminNetworkPolicy are:</p> <pre><code>_uuid               : fef274ee-28bd-4c05-8e6a-893283fbd84c\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.3\", \"10.244.2.4\", \"10.244.2.5\", \"10.244.2.6\", \"10.244.2.7\", \"10.244.2.8\"]\nexternal_ids        : {direction=Egress, gress-index=\"0\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:0:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a13517855690389298082\n\n_uuid               : ec77552f-f895-4f35-aa52-a4e44cb06e7e\naddresses           : [\"172.18.0.3\"]\nexternal_ids        : {direction=Egress, gress-index=\"1\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:1:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a10706246167277696183\n\n_uuid               : b406e4a6-4dba-4d03-96b5-383b2c6a44a4\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.3\", \"10.244.2.4\", \"10.244.2.5\", \"10.244.2.6\", \"10.244.2.7\", \"10.244.2.8\"]\nexternal_ids        : {direction=Egress, gress-index=\"2\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:2:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a18396736153283155648\n\n_uuid               : 42e115fb-d1cb-437b-b3c0-dbf23c46e3b1\naddresses           : [\"10.0.54.26/32\", \"10.0.56.38/32\", \"10.0.69.31/32\", \"172.30.0.1/32\"]\nexternal_ids        : {direction=Egress, gress-index=\"3\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:3:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a10622494091691694581\n\n_uuid               : 77554661-34c7-4d24-82e2-819c9be98e55\naddresses           : [\"10.244.1.3\", \"10.244.2.7\"]\nexternal_ids        : {direction=Egress, gress-index=\"4\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:4:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a5972452606168369118\n\n_uuid               : c8b87fe8-dd6c-4ef3-8cea-e7ea8a5bdb3d\naddresses           : [\"0.0.0.0/0\"]\nexternal_ids        : {direction=Egress, gress-index=\"5\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:5:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a11452480169090787059\n</code></pre> <p>The <code>nbdb.PortGroup</code> created by the above ACL is:</p> <pre><code>_uuid               : ff405445-be4f-4cde-8660-ee740c628f61\nacls                : [00400c05-7225-4d64-9428-8b2a04c6c3da, 04dbde8f-eaa8-4973-a158-ec56d0be672f, 1b1b1409-b96a-4e6d-9776-00d88c0c7ac9, 2f3292f0-9676-4d02-a553-230c3a2885b8, 3e82df77-ad40-4dce-bd96-5b9e7e0b3603, 4117970c-c138-45b5-a6de-8e5e54efcb04, 7f5ea19b-d465-4de4-8369-bdf66fdcd7d9, 8f1b8170-c350-4e9d-8ff0-a3177f9749ca, 924b81fd-65cc-494b-9c29-0c5287d69bb1, c37d0794-8444-440e-87d0-02ef95c6e248, f4523ce3-fb80-4d68-a50b-8aa6a795f895, fdb266b7-758a-4471-9cc1-84d00dc4f2ae]\nexternal_ids        : {\"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a14645450421485494999\nports               : [050c1f3d-fe1d-4d40-8f3a-9e21d09f0506, 30b5ed88-005a-4c86-80bb-6850173dec4b, 4f14f82b-2ec2-4b40-a6a7-7303105e45bb, 6868266a-b06f-4660-a5b6-394c0dbaf16b, d3196102-853f-4c4a-af60-01b0047fc48b, f44a954f-3a57-4b62-ac89-5a4b55eebf5c]\n</code></pre> <p>For the ANP created above, let us look at how these OVN constructs would look like:</p> <p><code>nbdb.PortGroup</code>:</p> <pre><code>_uuid               : 32648b42-e9f9-48e8-a1b0-78526fb60a41\nacls                : [c3c90b09-a68d-4d7f-aade-4aa17df798a8, ea9660bd-d118-4e26-8280-5e7520fa9e9c]\nexternal_ids        : {\"k8s.ovn.org/id\"=\"default-network-controller:BaselineAdminNetworkPolicy:default\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy}\nname                : a9550609891683691927\nports               : [050c1f3d-fe1d-4d40-8f3a-9e21d09f0506, 30b5ed88-005a-4c86-80bb-6850173dec4b, 4f14f82b-2ec2-4b40-a6a7-7303105e45bb, 6868266a-b06f-4660-a5b6-394c0dbaf16b, d3196102-853f-4c4a-af60-01b0047fc48b, f44a954f-3a57-4b62-ac89-5a4b55eebf5c]\n</code></pre> <p><code>nbdb.ACL</code>:</p> <pre><code>_uuid               : c3c90b09-a68d-4d7f-aade-4aa17df798a8\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"0\", \"k8s.ovn.org/id\"=\"default-network-controller:BaselineAdminNetworkPolicy:default:Egress:0:None\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a9550609891683691927 &amp;&amp; ((ip4.dst == $a17509128412806720482))\"\nmeter               : acl-logging\nname                : \"BANP:default:Egress:0\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 1750\nseverity            : []\ntier                : 3\n\n_uuid               : ea9660bd-d118-4e26-8280-5e7520fa9e9c\naction              : drop\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, gress-index=\"0\", \"k8s.ovn.org/id\"=\"default-network-controller:BaselineAdminNetworkPolicy:default:Ingress:0:None\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a9550609891683691927 &amp;&amp; ((ip4.src == $a168374317940583916))\"\nmeter               : acl-logging\nname                : \"BANP:default:Ingress:0\"\noptions             : {}\npriority            : 1750\nseverity            : []\ntier                : 3\n</code></pre> <p><code>nbdb.AddressSet</code>:</p> <pre><code>_uuid               : 60ae32d1-b0f9-49c9-ac5a-082bfc4b2c07\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.3\", \"10.244.2.4\", \"10.244.2.5\", \"10.244.2.6\", \"10.244.2.7\", \"10.244.2.8\"]\nexternal_ids        : {direction=Ingress, gress-index=\"0\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:BaselineAdminNetworkPolicy:default:Ingress:0:v4\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy}\nname                : a168374317940583916\n\n_uuid               : 9e04c2f6-e5c0-4fe9-a1cc-7a72c277ed08\naddresses           : [\"10.244.1.3\", \"10.244.1.4\", \"10.244.2.3\", \"10.244.2.4\", \"10.244.2.5\", \"10.244.2.6\", \"10.244.2.7\", \"10.244.2.8\"]\nexternal_ids        : {direction=Egress, gress-index=\"0\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:BaselineAdminNetworkPolicy:default:Egress:0:v4\", \"k8s.ovn.org/name\"=default, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=BaselineAdminNetworkPolicy}\nname                : a17509128412806720482\n</code></pre>"},{"location":"features/network-security-controls/admin-network-policy/#ovn-sbdb-flows-generated","title":"OVN SBDB Flows generated","text":"<p>Sample flows created for policies look like this on  OVN SBDB level:</p> <pre><code>  table=4 (ls_out_acl_eval    ), priority=27600, match=(reg8[30..31] == 1 &amp;&amp; reg0[7] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a14545668191619617708)))), action=(log(name=\"ANP:cluster-control:Ingress:0\", severity=alert, verdict=allow, meter=\"acl-logging__f4523ce3-fb80-4d68-a50b-8aa6a795f895\"); reg8[16] = 1; reg0[1] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27600, match=(reg8[30..31] == 1 &amp;&amp; reg0[8] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a14545668191619617708)))), action=(log(name=\"ANP:cluster-control:Ingress:0\", severity=alert, verdict=allow, meter=\"acl-logging__f4523ce3-fb80-4d68-a50b-8aa6a795f895\"); reg8[16] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27599, match=(reg8[30..31] == 1 &amp;&amp; reg0[7] == 1 &amp;&amp; (((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; ((ip4.dst == 10.244.1.4 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080)))), action=(log(name=\"ANP:cluster-control:Ingress:1\", severity=alert, verdict=allow, meter=\"acl-logging__1b1b1409-b96a-4e6d-9776-00d88c0c7ac9\"); reg8[16] = 1; reg0[1] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27599, match=(reg8[30..31] == 1 &amp;&amp; reg0[7] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; tcp.dst==7564)), action=(log(name=\"ANP:cluster-control:Ingress:1\", severity=alert, verdict=allow, meter=\"acl-logging__4117970c-c138-45b5-a6de-8e5e54efcb04\"); reg8[16] = 1; reg0[1] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27599, match=(reg8[30..31] == 1 &amp;&amp; reg0[8] == 1 &amp;&amp; (((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; ((ip4.dst == 10.244.1.4 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080) || (ip4.dst == 10.244.2.8 &amp;&amp; tcp.dst == 8080)))), action=(log(name=\"ANP:cluster-control:Ingress:1\", severity=alert, verdict=allow, meter=\"acl-logging__1b1b1409-b96a-4e6d-9776-00d88c0c7ac9\"); reg8[16] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27599, match=(reg8[30..31] == 1 &amp;&amp; reg0[8] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a6786643370959569281)) &amp;&amp; tcp &amp;&amp; tcp.dst==7564)), action=(log(name=\"ANP:cluster-control:Ingress:1\", severity=alert, verdict=allow, meter=\"acl-logging__4117970c-c138-45b5-a6de-8e5e54efcb04\"); reg8[16] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27598, match=(reg8[30..31] == 1 &amp;&amp; reg0[7] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13730899355151937870)))), action=(log(name=\"ANP:cluster-control:Ingress:2\", severity=alert, verdict=allow, meter=\"acl-logging__00400c05-7225-4d64-9428-8b2a04c6c3da\"); reg8[16] = 1; reg0[1] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27598, match=(reg8[30..31] == 1 &amp;&amp; reg0[8] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13730899355151937870)))), action=(log(name=\"ANP:cluster-control:Ingress:2\", severity=alert, verdict=allow, meter=\"acl-logging__00400c05-7225-4d64-9428-8b2a04c6c3da\"); reg8[16] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=27597, match=(reg8[30..31] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a764182844364804195)))), action=(log(name=\"ANP:cluster-control:Ingress:3\", severity=warning, verdict=pass, meter=\"acl-logging__7f5ea19b-d465-4de4-8369-bdf66fdcd7d9\"); next;)\n  table=4 (ls_out_acl_eval    ), priority=27596, match=(reg8[30..31] == 1 &amp;&amp; reg0[10] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13814616246365836720)))), action=(log(name=\"ANP:cluster-control:Ingress:4\", severity=alert, verdict=drop, meter=\"acl-logging__3e82df77-ad40-4dce-bd96-5b9e7e0b3603\"); reg8[17] = 1; ct_commit { ct_mark.blocked = 1; }; next;)\n  table=4 (ls_out_acl_eval    ), priority=27596, match=(reg8[30..31] == 1 &amp;&amp; reg0[9] == 1 &amp;&amp; (outport == @a14645450421485494999 &amp;&amp; ((ip4.src == $a13814616246365836720)))), action=(log(name=\"ANP:cluster-control:Ingress:4\", severity=alert, verdict=drop, meter=\"acl-logging__3e82df77-ad40-4dce-bd96-5b9e7e0b3603\"); reg8[17] = 1; next;)\n  table=4 (ls_out_acl_eval    ), priority=2750 , match=(reg8[30..31] == 3 &amp;&amp; reg0[10] == 1 &amp;&amp; (outport == @a9550609891683691927 &amp;&amp; ((ip4.src == $a168374317940583916)))), action=(reg8[17] = 1; ct_commit { ct_mark.blocked = 1; }; next;)\n  table=4 (ls_out_acl_eval    ), priority=2750 , match=(reg8[30..31] == 3 &amp;&amp; reg0[9] == 1 &amp;&amp; (outport == @a9550609891683691927 &amp;&amp; ((ip4.src == $a168374317940583916)))), action=(reg8[17] = 1; next;)\n</code></pre>"},{"location":"features/network-security-controls/admin-network-policy/#multi-tenant-isolation","title":"Multi Tenant Isolation","text":"<p>In order to isolate your tenants in the cluster, unlike network policies you don't need to create 1 policy per namespace. You need to only create 1 policy per tenant. Sample ANPs for multi-tenant isolation where we have 4 tenants <code>blue</code>, <code>green</code>, <code>yellow</code> and <code>red</code> are:</p> <pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: blue-tenant\nspec:\n  priority: 40\n  subject:\n    namespaces: \n      matchLabels:\n        tenant: blue\n  ingress:\n    - action: Allow\n      from:\n      - namespaces:\n          matchLabels:\n            tenant: blue\n  egress:\n    - action: Allow\n      to:\n      - namespaces:\n          matchLabels:\n            tenant: blue\n---\napiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: red-tenant\nspec:\n  priority: 40\n  subject:\n    namespaces: \n      matchLabels:\n        tenant: red\n  ingress:\n    - action: Allow\n      from:\n      - namespaces:\n          matchLabels:\n            tenant: red\n  egress:\n    - action: Allow\n      to:\n      - namespaces:\n          matchLabels:\n            tenant: red\n---\napiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: yellow-tenant\nspec:\n  priority: 40\n  subject:\n    namespaces: \n      matchLabels:\n        tenant: yellow\n  ingress:\n    - action: Allow\n      from:\n      - namespaces:\n          matchLabels:\n            tenant: yellow\n  egress:\n    - action: Allow\n      to:\n      - namespaces:\n          matchLabels:\n            tenant: yellow\n---\napiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: green-tenant\nspec:\n  priority: 40\n  subject:\n    namespaces: \n      matchLabels:\n        tenant: green\n  ingress:\n    - action: Allow\n      from:\n      - namespaces:\n          matchLabels:\n            tenant: green\n  egress:\n    - action: Allow\n      to:\n      - namespaces:\n          matchLabels:\n            tenant: green\n---\napiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: multi-tenant-isolation\nspec:\n  priority: 43\n  subject:\n    namespaces: \n      matchExpressions:\n        - {key: tenant, operator: In, values: [red, blue, green, yellow]}\n  ingress:\n    - action: Deny\n      from:\n      - namespaces: {}\n  egress:\n    - action: Deny\n      to:\n      - namespaces: {}\n</code></pre> <p>A limitation of doing the above approach of creating explicit <code>Allow</code> policies for inter-tenant communication followed by explicit <code>Deny</code> is that you cannot have finer grain controls for inter-tenant communication on a pod level.</p> <p>So in that case a better way to achieve the same thing above is to flip the posture and deny straight on each tenant ANP. In this case you can have networkpolicies created within the tenant yellow or blue that tells what can or cannot be allowed:</p> <p><pre><code>apiVersion: policy.networking.k8s.io/v1alpha1\nkind: AdminNetworkPolicy\nmetadata:\n  name: yellow-tenant\nspec:\n  priority: 40\n  subject:\n    namespaces: \n      matchLabels:\n        tenant: yellow\n  ingress:\n    - action: Deny\n      from:\n      - namespaces:\n          matchExpressions:\n          - key: tenant\n            operator: NotIn\n            value: yellow\n  egress:\n    - action: Deny\n      to:\n      - namespaces:\n          matchExpressions:\n          - key: tenant\n            operator: NotIn\n            value: yellow\n</code></pre> In that case you don't need a 43 priority ANP. However note that if you define it this way then each address-set will contain ALL the pods to be excluded on each tenant ANP which might be a big number causing scale bottlenecks. So its better to have smaller non-overlapping address-sets as much as possible.</p> <p>In order to be able to express \"allow-to-yourself\" and \"deny-from-everyone-else\" better, we have some ongoing upstream design work where we hope to be able to condense 1 policy per tenant to be 1 policy for all tenants. Until then unfortunately you'd need to create 1 policy per tenant to achieve isolation.</p>"},{"location":"features/network-security-controls/admin-network-policy/#troubleshooting","title":"Troubleshooting","text":"<p>In order to ensure the ANP and BANP are correctly created, do <code>kubectl get anp</code> or <code>kubectl get banp</code> to see the status of the CRs. A sample good status will look like this:</p> <pre><code>  Conditions:\n    Last Transition Time:  2024-06-08T20:29:00Z\n    Message:               Setting up OVN DB plumbing was successful\n    Reason:                SetupSucceeded\n    Status:                True\n    Type:                  Ready-In-Zone-ovn-control-plane\n    Last Transition Time:  2024-06-08T20:29:00Z\n    Message:               Setting up OVN DB plumbing was successful\n    Reason:                SetupSucceeded\n    Status:                True\n    Type:                  Ready-In-Zone-ovn-worker\n    Last Transition Time:  2024-06-08T20:29:00Z\n    Message:               Setting up OVN DB plumbing was successful\n    Reason:                SetupSucceeded\n    Status:                True\n    Type:                  Ready-In-Zone-ovn-worker2\n</code></pre> <p>If the plumbing went wrong, you would see an error reported from that respective zone controller on the status. Then the next step is to check the logs of <code>ovnkube-controller</code> container on the data plane side in the <code>ovnkube-node</code> pod.</p>"},{"location":"features/network-security-controls/admin-network-policy/#acl-logging","title":"ACL Logging","text":"<p>ACL logging feature can be enabled on a per policy level. You can do</p> <pre><code>kubectl annotate anp cluster-control k8s.ovn.org/acl-logging='{ \"deny\": \"alert\", \"allow\": \"alert\", \"pass\" : \"warning\" }'\n</code></pre> <p>to activate logging for the above admin network policy. In order to ensure it was correctly applied you can do <code>kubectl describe anp cluster-control</code>:</p> <pre><code>Name:         cluster-control\nNamespace:    \nLabels:       &lt;none&gt;\nAnnotations:  k8s.ovn.org/acl-logging: { \"deny\": \"alert\", \"allow\": \"alert\", \"pass\" : \"warning\" }\nAPI Version:  policy.networking.k8s.io/v1alpha1\nKind:         AdminNetworkPolicy\n</code></pre> <p>In order to view what's happening with the traffic that matches this policy, you can do:</p> <pre><code>kubectl logs -n ovn-kubernetes ovnkube-node-ggrxt ovn-controller | grep acl\n</code></pre> <p>and it will show you all the ACL logs logged by ovn-controller container. Sample logging output:</p> <pre><code>2024-06-09T19:00:10.441Z|00162|acl_log(ovn_pinctrl0)|INFO|name=\"ANP:cluster-control:Egress:1\", verdict=allow, severity=alert, direction=from-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:06,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.6,nw_dst=172.18.0.3,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=33690,tp_dst=6443,tcp_flags=ack\n2024-06-09T19:00:10.696Z|00163|acl_log(ovn_pinctrl0)|INFO|name=\"ANP:cluster-control:Egress:1\", verdict=allow, severity=alert, direction=from-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=172.18.0.3,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=57658,tp_dst=6443,tcp_flags=psh|ack\n2024-06-09T19:00:10.698Z|00164|acl_log(ovn_pinctrl0)|INFO|name=\"ANP:cluster-control:Egress:1\", verdict=allow, severity=alert, direction=from-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=172.18.0.3,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=57658,tp_dst=6443,tcp_flags=ack\n2024-06-09T19:00:11.386Z|00165|acl_log(ovn_pinctrl0)|INFO|name=\"ANP:cluster-control:Egress:5\", verdict=drop, severity=alert, direction=from-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:07,dl_dst=0a:58:0a:f4:02:08,nw_src=10.244.2.7,nw_dst=10.244.2.8,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0\n</code></pre> <p>The same is applicable to debug baseline admin network policy as well except for the fact that there is no <code>pass</code> in BANP:</p> <pre><code>kubectl annotate banp default k8s.ovn.org/acl-logging='{ \"deny\": \"alert\", \"allow\": \"alert\" }'\n</code></pre>"},{"location":"features/network-security-controls/admin-network-policy/#ensuring-nbdb-objects-are-correctly-created","title":"Ensuring NBDB objects are correctly created","text":"<p>See the details outlined in the OVN constructs section on what are the OVN database constructs created for ANPs and BANP in a cluster. For example, in order to troubleshoot the <code>cluster-control</code> ANP at priority 34, we can list the address-sets, acls and port-group like shown in the above section. But that will list all the acls, address-sets and port-groups in your cluster also used by other features. To narrow it down to the specific policy in question you can grep for the name of the ANP you care about:</p> <ul> <li><code>ovn-nbctl list acl | grep cluster-control</code> (Do grep with -A, -B, -C to see the full ACL)</li> <li><code>ovn-nbctl list address-set | grep anp-name</code></li> <li><code>ovn-nbctl list port-group | grep anp-name</code></li> </ul> <p>An easier way to search for the relevant ACLs/AddressSets/PortGroups is:</p> <ul> <li><code>ovn-nbctl find ACL 'external_ids{&gt;=}{\"k8s.ovn.org/owner-type\"=AdminNetworkPolicy,\"k8s.ovn.org/name\"=cluster-control}'</code></li> <li><code>ovn-nbctl find Address_Set 'external_ids{&gt;=}{\"k8s.ovn.org/owner-type\"=AdminNetworkPolicy,\"k8s.ovn.org/name\"=cluster-control}'</code></li> <li><code>ovn-nbctl find Port_Group 'external_ids{&gt;=}{\"k8s.ovn.org/owner-type\"=AdminNetworkPolicy,\"k8s.ovn.org/name\"=cluster-control}'</code></li> </ul> <p>If you want to know the specific rule's ACLs you can use the gress-index field as well:</p> <ul> <li><code>ovn-nbctl find ACL 'external_ids{&gt;=}{\"k8s.ovn.org/owner-type\"=AdminNetworkPolicy,\"k8s.ovn.org/name\"=cluster-control,gress-index=\"5\"}'</code></li> <li><code>ovn-nbctl find Address_Set 'external_ids{&gt;=}{\"k8s.ovn.org/owner-type\"=AdminNetworkPolicy,\"k8s.ovn.org/name\"=cluster-control,gress-index=\"5\"}'</code></li> </ul> <p>This will print out all the ACLs and address-sets and portgroups owned by that ANP. In order to troubleshoot a specific rule; example Rule5 <code>default-deny</code> in egress of <code>cluster-control</code> ANP:</p> <pre><code>  - name: \"default-deny\"\n    action: \"Deny\"\n    to:\n    - networks:\n      - 0.0.0.0/0\n</code></pre> <p>We can start with the knowledge that this rule must be translated into one or more ACLs like mentioned in the above section. Since there is no L4 match it should be translated to a single ACL. Knowing the rule number, we can grep for the correct ACL:</p> <p><code>ovn-nbctl list acl | grep cluster-control:Egress:5</code></p> <p>So <code>&lt;anpName&gt;:&lt;direction&gt;:&lt;ruleNumber&gt;</code> can be the grep pattern to get the right ACL. For this rule it looks like this:</p> <pre><code>sh-5.2# ovn-nbctl list acl | grep cluster-control:Egress:5 -C 4\n\n_uuid               : 924b81fd-65cc-494b-9c29-0c5287d69bb1\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=Egress, gress-index=\"5\", \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:5:None\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy, port-policy-protocol=None}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a11452480169090787059))\"\nmeter               : acl-logging\nname                : \"ANP:cluster-control:Egress:5\"\noptions             : {apply-after-lb=\"true\"}\npriority            : 26595\nseverity            : []\ntier                : 1\n</code></pre> <p>The important part in an ACL is its <code>match</code> and <code>action</code>. What this above rule says is that if a packet matches the provided <code>match</code> then OVN takes the provided <code>action</code>.</p> <p>Match here is: <code>\"inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a11452480169090787059))\"</code></p> <p>which makes no sense at face value. Let's dissect the first part here <code>inport == @a14645450421485494999</code>.</p> <p><code>inport</code> here refers to the port-group that this ACL is attached to. So you can do:</p> <pre><code>sh-5.2# ovn-nbctl list port-group a14645450421485494999\n\n_uuid               : ff405445-be4f-4cde-8660-ee740c628f61\nacls                : [00400c05-7225-4d64-9428-8b2a04c6c3da, 04dbde8f-eaa8-4973-a158-ec56d0be672f, 1b1b1409-b96a-4e6d-9776-00d88c0c7ac9, 2f3292f0-9676-4d02-a553-230c3a2885b8, 3e82df77-ad40-4dce-bd96-5b9e7e0b3603, 4117970c-c138-45b5-a6de-8e5e54efcb04, 7f5ea19b-d465-4de4-8369-bdf66fdcd7d9, 8f1b8170-c350-4e9d-8ff0-a3177f9749ca, 924b81fd-65cc-494b-9c29-0c5287d69bb1, c37d0794-8444-440e-87d0-02ef95c6e248, f4523ce3-fb80-4d68-a50b-8aa6a795f895, fdb266b7-758a-4471-9cc1-84d00dc4f2ae]\nexternal_ids        : {\"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a14645450421485494999\nports               : [050c1f3d-fe1d-4d40-8f3a-9e21d09f0506, 30b5ed88-005a-4c86-80bb-6850173dec4b, 4f14f82b-2ec2-4b40-a6a7-7303105e45bb, 6868266a-b06f-4660-a5b6-394c0dbaf16b, d3196102-853f-4c4a-af60-01b0047fc48b, f44a954f-3a57-4b62-ac89-5a4b55eebf5c]\n</code></pre> <p>that shows you all the ports present in this port-group. Each port in the list <code>ports</code> represents a pod that is the subject of this policy.</p> <p>You can do <code>ovn-nbctl list logical-switch-port &lt;uuid-port&gt;</code> to know which pod is present.</p> <p>Let's look into the second half of the match <code>ip4.dst == $a11452480169090787059</code>.</p> <p>The destination must be an IP inside the address-set <code>a11452480169090787059</code>. That's what it means:</p> <pre><code>_uuid               : c8b87fe8-dd6c-4ef3-8cea-e7ea8a5bdb3d\naddresses           : [\"0.0.0.0/0\"]\nexternal_ids        : {direction=Egress, gress-index=\"5\", ip-family=v4, \"k8s.ovn.org/id\"=\"default-network-controller:AdminNetworkPolicy:cluster-control:Egress:5:v4\", \"k8s.ovn.org/name\"=cluster-control, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=AdminNetworkPolicy}\nname                : a11452480169090787059\n</code></pre> <p>Here you can see that <code>0.0.0.0/0</code> is the only IP in the set but it means all IPs.</p> <p>So now match makes more sense: Where we are saying source of traffic shoule be the port of any one of these pods in the port-group AND destination is any IP, then we <code>drop</code> the packet which is the <code>action</code> on the ACL.</p> <p>Using this method each policy rule can be debugged to see if constructs at OVN level are created correctly. An easier way is to simply run an ovnkube-trace.</p>"},{"location":"features/network-security-controls/admin-network-policy/#ovn-tracing","title":"OVN Tracing","text":"<p>You can do an <code>ovnkube-trace</code> or <code>ovn-trace</code> to figure out if the traffic is getting blocked or allowed or passed correctly using OVN ACLs.</p> <p>Sample tracing output for a pod in <code>restricted</code> namespace trying to ping a pod in <code>monitoring</code> namespace can be seen here:</p> <pre><code>$ oc rsh -n ovn-kubernetes ovnkube-node-ggrxt\nDefaulted container \"nb-ovsdb\" out of: nb-ovsdb, sb-ovsdb, ovn-northd, ovnkube-controller, ovn-controller, ovs-metrics-exporter\nsh-5.2# ovn-trace --ct new 'inport==\"restricted_restricted-65948898b-b2hhj\" &amp;&amp; eth.src==0a:58:0a:f4:02:07 &amp;&amp; eth.dst==0a:58:0a:f4:02:01 &amp;&amp; ip4.src==10.244.2.7 &amp;&amp; ip4.dst==10.244.2.8 &amp;&amp; icmp'\n# icmp,reg14=0x7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:07,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.7,nw_dst=10.244.2.8,nw_tos=0,nw_ecn=0,nw_ttl=0,nw_frag=no,icmp_type=0,icmp_code=0\n\ningress(dp=\"ovn-worker2\", inport=\"restricted_restricted-65948898b-b2hhj\")\n-------------------------------------------------------------------------\n 0. ls_in_check_port_sec (northd.c:8691): 1, priority 50, uuid a6855493\n    reg0[15] = check_in_port_sec();\n    next;\n 4. ls_in_pre_acl (northd.c:5997): ip, priority 100, uuid ecaba77c\n    reg0[0] = 1;\n    next;\n 5. ls_in_pre_lb (northd.c:6203): ip, priority 100, uuid e37304d7\n    reg0[2] = 1;\n    next;\n 6. ls_in_pre_stateful (northd.c:6231): reg0[2] == 1, priority 110, uuid cac1d984\n    ct_lb_mark;\n\nct_lb_mark\n----------\n 7. ls_in_acl_hint (northd.c:6297): ct.new &amp;&amp; !ct.est, priority 7, uuid a5d96cbe\n    reg0[7] = 1;\n    reg0[9] = 1;\n    next;\n 8. ls_in_acl_eval (northd.c:6897): ip &amp;&amp; !ct.est, priority 1, uuid cb01d163\n    reg0[1] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6725): reg8[30..31] == 0, priority 500, uuid a19ed2d6\n    reg8[30..31] = 1;\n    next(8);\n 8. ls_in_acl_eval (northd.c:6897): ip &amp;&amp; !ct.est, priority 1, uuid cb01d163\n    reg0[1] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6725): reg8[30..31] == 1, priority 500, uuid bf86b332\n    reg8[30..31] = 2;\n    next(8);\n 8. ls_in_acl_eval (northd.c:6897): ip &amp;&amp; !ct.est, priority 1, uuid cb01d163\n    reg0[1] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6725): reg8[30..31] == 2, priority 500, uuid 674b033e\n    reg8[30..31] = 3;\n    next(8);\n 8. ls_in_acl_eval (northd.c:6897): ip &amp;&amp; !ct.est, priority 1, uuid cb01d163\n    reg0[1] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6714): 1, priority 0, uuid 7bea61b0\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n15. ls_in_pre_hairpin (northd.c:7778): ip &amp;&amp; ct.trk, priority 100, uuid 39454be7\n    reg0[6] = chk_lb_hairpin();\n    reg0[12] = chk_lb_hairpin_reply();\n    next;\n19. ls_in_acl_after_lb_action (northd.c:6725): reg8[30..31] == 0, priority 500, uuid dbb2d69a\n    reg8[30..31] = 1;\n    next(18);\n18. ls_in_acl_after_lb_eval (northd.c:6552): reg8[30..31] == 1 &amp;&amp; reg0[9] == 1 &amp;&amp; (inport == @a14645450421485494999 &amp;&amp; ((ip4.dst == $a11452480169090787059))), priority 27595, uuid 369e6009\n    log(name=\"ANP:cluster-control:Egress:5\", verdict=drop, severity=alert, meter=\"acl-logging__924b81fd-65cc-494b-9c29-0c5287d69bb1\");\n\n    LOG: ACL name=ANP:cluster-control:Egress:5, direction=IN, verdict=drop, severity=alert, packet=\"ct_state=new|trk,icmp,reg0=0x287,reg8=0x40000000,reg14=0x7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:07,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.7,nw_dst=10.244.2.8,nw_tos=0,nw_ecn=0,nw_ttl=0,nw_frag=no,icmp_type=0,icmp_code=0\"\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    reg8[17] = 1;\n    next;\n19. ls_in_acl_after_lb_action (northd.c:6691): reg8[17] == 1, priority 1000, uuid 0cd7e938\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n</code></pre> <p>The ingress rule1 in <code>cluster-control</code> ANP which allows traffic from monitoring to everywhere is only allowed at a specific port 7564. So ICMP traffic is not allowed in any way and so we hit the egress rule5 where traffic egressing from restricted namespace to monitoring namespace is blocked.</p>"},{"location":"features/network-security-controls/admin-network-policy/#metrics","title":"Metrics","text":"<p>In order to help end users know how this feature looks like in their cluster and what are the OVN objects created, we have added metrics for this feature. There is a metrics collector interface added to the admin network policy controller that periodically checks its caches and reports the type of ANP rules that are currently present on the cluster. Similarly there is another go routine invoked from metrics controller that periodically polls the libovsdb caches to query the count of OVN database objects created by the admin network policy controller. Let's look at the set of exposed metrics that can help users understand what's happening in this feature.</p> <ul> <li><code>ovnkube_controller_admin_network_policies</code>: The total number of admin   network policies in the cluster</li> <li><code>ovnkube_controller_baseline_admin_network_policies</code>: The total number   of baseline admin network policies in the cluster</li> <li><code>ovnkube_controller_admin_network_policies_db_objects</code>: The total number   of OVN NBDB objects (table_name) owned by AdminNetworkPolicy controller in   the cluster. <code>table_name</code> label here can be either <code>ACL</code> or <code>Address_Set</code>.</li> <li><code>ovnkube_controller_baseline_admin_network_policies_db_objects</code>: The total   number of OVN NBDB objects (table_name) owned by BaselineAdminNetworkPolicy   controller in the cluster. <code>table_name</code> label here can be either <code>ACL</code> or <code>Address_Set</code>.</li> <li><code>ovnkube_controller_admin_network_policies_rules</code>: The total number of rules   across all admin network policies in the cluster grouped by <code>direction</code> and <code>action</code>.   <code>direction</code> can be either <code>Ingress</code> or <code>Egress</code> and <code>action</code> can be either   <code>Pass</code> or <code>Allow</code> or <code>Deny</code>.</li> <li><code>ovnkube_controller_baseline_admin_network_policies_rules</code>: The total number of rules   across all baseline admin network policies in the cluster grouped by <code>direction</code> and <code>action</code>.   <code>direction</code> can be either <code>Ingress</code> or <code>Egress</code> and <code>action</code> can be either   <code>Allow</code> or <code>Deny</code>.</li> </ul> <p> </p>"},{"location":"features/network-security-controls/admin-network-policy/#best-practices-and-gotchas","title":"Best Practices and Gotchas!","text":"<ul> <li>Its better to create ANPs at separate priorities so that the precedence   is well defined and if you are creating two ANPs at the same priority,   then it is the responsibility of the end user to ensure the rules in   those ANPs are not overlapping. If it overlaps then its not guaranteed   which one will take effect.</li> <li>Unlike ANPs BANP is a singleton object. So there can be only 1 <code>default</code>   BANP in your cluster. Hence use this policy wisely as a wider default   guardrail.</li> <li>API semantics for admin network policies are very different from network   policies. There is no implicit deny when you create the policy. You have to   explicitly define what you need. Because of this it is not possible to   completely isolate the pod from everything using admin network policies for   ingress traffic. The only peers you can express for a <code>Deny</code> ingress rule are   pods; so atmost only \"DENY from all other pods\" can be expressed as the best   case default deny for ingress. In case of egress, one can deny to 0.0.0.0/0   and it will act like a default deny to everything.</li> <li>When using 0.0.0.0/0 to DENY everything, it is recommeded to be extra careful.   If higher priority allow rules for essential traffic like towards DNS and   KAPI-server are not defined before doing the default-deny, then you have the   potential to lock up your entire cluster leaving it in a non-functional state.</li> <li>Unlike network policies, admin network policies are cluster-scoped. So use the   empty namespace selector with care because it will select ALL namespaces in your   cluster including the infrastructure namespaces like kube-system and kapi-server   or openshift-*. Best practice to secure your workloads is to use workload   specific labels and then use that as the namespace selector instead. Using <code>{}</code>   empty namespace selector has the potential to lock up your entire cluster leaving   it in a non-functional state.</li> <li>Cluster Admins should care about interoperability with NetworkPolicies if any are   expected to be present in the cluster namespaces. In such cases the action <code>Pass</code>   can be leveraged to indicate admin doesn't want to block the traffic so that network   policies take effect. If you use <code>Allow</code> rules on admin level, then it will become   impossible for you to let finer grain controls within a namespace policy take effect.   So almost always <code>Deny</code> and <code>Pass</code> can get the job done unless you have specific   requirements for a strong <code>Allow</code>.   On the other hand, if you don't (plan to) have any network policies in your cluster   you can just use <code>Allow</code> instead to indicate you don't want to block the traffic.</li> <li>Since <code>0-100</code> is the supported priority range, it is always better when designing   your admin policies to use a middle range like <code>30-70</code> by leaving some placeholder   priorities before and after. Even in the middle range, be careful to leave gaps so   that as your infrastructure requirements evolve over time, you are able to insert   new ANPs when needed at the right priority level. If you pack your ANPs, then you   might need to recreate all of them to accommodate any changes in the future.</li> <li>Each rule can have upto 100 peers. All those peers are added to the same address-set.   It is for the best if selectors across multiple rules don't overlap so that different   address-sets don't have the same IPs in them. OVN has perf/scale limitations where   it doesn't do incremental processing at ovn-controller level in such cases. So any pod   level churn causes full processing of OVS flows. See here   for more details.</li> <li><code>networks</code> peer in egress rule is meant to select external destinations. It is recommended   to use <code>namespaces</code> and <code>pods</code> peers to select in-cluster peers. Do not use <code>networks</code> for   podIP level matching.</li> <li>Currently in order to express multi-tenant isolation, you need to do 1 ANP per tenant.   There is some future planned work upstream to make this easier for users. See multi tenant   isolation section for more details.</li> </ul>"},{"location":"features/network-security-controls/admin-network-policy/#known-limitations-and-design-choices-of-ovn-kubernetes-anp-implementation","title":"Known Limitations and Design Choices of OVN-Kubernetes ANP Implementation","text":"<ul> <li>Even if API supports upto 1000 as a value for <code>spec.priority</code>, in OVN-Kubernetes   we only support upto 99 as a max value. This is because of known limitation   in OVN level for <code>nbdb.ACL</code> priority field. See this   for more details. However the number of admin policies in a cluster are usually expected   to be of a maximum of say 30-50 and not more than that based on use cases for   which this API was defined for. Thus supported priority values in   OVN-Kubernetes are from 0 to 99. When user tries to create an ANP with priority   greater than 99 an event <code>ANPWithUnsupportedPriority</code> will be emitted. The user has   to manually update the priority field so that it get's created correctly. There are   ways around the known OVN limitation where in OVN-Kubernetes we could dynamically   allocate priorities to ACLs but that is subject to end user feedback on if support for   more priorities is desired.</li> <li>Using <code>namedPorts</code> is not very performant efficient since each name has to get   evaluated against the <code>subject</code> and <code>peer</code> pod's container ports and podIPs. Use this with caution   in larger clusters where you have many pods selected as subjects or peers for policies   matching <code>namedPorts</code> and each of these matched pod has many containers.</li> </ul>"},{"location":"features/network-security-controls/admin-network-policy/#known-limitations-and-design-choices-of-anp-api","title":"Known Limitations and Design Choices of ANP API","text":"<ul> <li>The <code>subject</code> of an admin network policy does NOT include support for   <code>host-networked</code> pods.</li> <li>The <code>namespaces</code> and <code>pods</code> peers in admin network policy does NOT include   support for <code>host-networked</code> pods.</li> <li><code>nodes</code> peer can be specified only from <code>egress</code> rule. There are no ingress use   cases yet which is why this is not supported from <code>ingress</code> rule.</li> <li><code>networks</code> peer can be specified only from <code>egress</code> rule. There are no ingress use   cases yet which is why this is not supported from <code>ingress</code> rule.</li> <li>Specifying <code>namedPorts</code> with <code>networks</code> peer is not supported.</li> </ul>"},{"location":"features/network-security-controls/admin-network-policy/#future-items","title":"Future Items","text":"<ul> <li>Support for FQDN Peers</li> <li>Support for Easier Tenant Expressions</li> </ul>"},{"location":"features/network-security-controls/admin-network-policy/#references","title":"References","text":"<ul> <li>AdminNetworkPolicy on OpenShift</li> <li>Kubernetes Enhancement Proposal</li> <li>Network Policy Enhancement Proposals</li> <li>Kubernetes AdminNetworkPolicy API reference</li> </ul>"},{"location":"features/network-security-controls/dns-name-resolution/","title":"DNS name resolover","text":""},{"location":"features/network-security-controls/dns-name-resolution/#introduction","title":"Introduction","text":"<p>The DNS name resolver feature allows a cluster administrator to enable integration of EgressFirewall with CoreDNS. Once enabled the EgressFirewall DNS rules will be resolved using CoreDNS and the underlying address set will be updated accordingly. Additionally, if any other pod does the DNS resolution of DNS names used in the EgressFirewall DNS rules, then the underlying address sets will also be updated if there is any change in IP addresses.</p> <p>The DNS name resolver feature also adds the support of using wildcard DNS names in EgressFirewall DNS name rules. The wildcard (<code>*</code>) will match only one label (subdomain). For example, <code>*.example.com</code> will match <code>sub1.example.com</code> and will not match <code>sub2.sub1.example.com</code>.</p> <p>The <code>DNSNameResolver</code> CRD will be used by ovnk to get the latest IP addresses corresponding to a DNS name when the feature is enabled. However, the cluster administrator does not need to interact with the CR. </p>"},{"location":"features/network-security-controls/dns-name-resolution/#configuring-dns-name-resolover-in-a-cluster","title":"Configuring DNS name resolover in a cluster","text":"<p>The DNS name resolver should be configured in the following components for it to work in a cluster:</p>"},{"location":"features/network-security-controls/dns-name-resolution/#dnsnameresolver-operator","title":"DNSNameResolver operator","text":"<p>The DNSNameResolver operator should also be deployed using <code>make deploy</code>. Before deploying the operator, correct values should be provided for the following arguments: - <code>coredns-namespace</code> - <code>coredns-service-name</code> - <code>coredns-port</code> - <code>dns-name-resolver-namespace</code> [this should be the namespace where ovnk is deployed]</p> <p>NOTE: The DNSNameResolver CRD should be installed using <code>make install</code> before deploying the operator.</p>"},{"location":"features/network-security-controls/dns-name-resolution/#coredns-plugin","title":"CoreDNS plugin","text":"<p>In addition to deploying the operator, the <code>ocp_dnsnameresolver</code> external plugin should be enabled on the CoreDNS pods. In the <code>Corefile</code>, the plugin should be configured to watch for the namespace where ovnk is deployed.</p> <pre><code>ocp_dnsnameresolver {\n    namespaces &lt;ovnk-namespace&gt;\n}\n</code></pre>"},{"location":"features/network-security-controls/dns-name-resolution/#ovnk","title":"OVNK","text":"<p>In ovnk, the feature is gated by config flag. In order to create a cluster with dns name resolver feature enabled, use the <code>--enable-dns-name-resolver</code> option in cluster-manager and ovnkube-controller.</p> <p>The following RBAC permissions should be added to cluster-manager:</p> <pre><code>- apiGroups: [\"network.openshift.io\"]\n  resources:\n  - dnsnameresolvers\n  verbs:\n  - create\n  - delete\n  - get\n  - list\n  - patch\n  - update\n  - watch\n</code></pre> <p>The following RBAC permissions should be added to ovnkube-node:</p> <pre><code>- apiGroups: [\"network.openshift.io\"]\n  resources:\n  - dnsnameresolvers\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>"},{"location":"features/network-security-controls/egress-firewall/","title":"EgressFirewall","text":""},{"location":"features/network-security-controls/egress-firewall/#introduction","title":"Introduction","text":"<p>The EgressFirewall feature enables a cluster administrator to limit the external hosts that a pod in a project can access. The EgressFirewall object rules apply to all pods that share the namespace with the egressfirewall object. A namespace only supports having one EgressFirewallObject.</p>"},{"location":"features/network-security-controls/egress-firewall/#example","title":"Example","text":"<p>The yaml  below is an example of a simple egressFirewall object</p> <pre><code>kind: EgressFirewall\napiVersion: k8s.ovn.org/v1\nmetadata:\n  name: default\n  namespace: default\nspec:\n  egress:\n  - type: Allow\n    to:\n      dnsName: www.openvswitch.org\n  - type: Allow\n    to:\n      cidrSelector: 1.2.3.0/24\n  - type: Allow\n    to:\n      cidrSelector: 4.5.6.0/24\n    ports:\n      - protocol: UDP\n        port: 55\n  - type: Deny\n    to:\n      cidrSelector: 0.0.0.0/0\n</code></pre> <p>This example allows Pods in the default namespace to connect to the host(s) that www.openvswitch.org translates to, any external host within the range 1.2.3.0 to 1.2.3.255, and in addtion allows traffic to 4.5.6.0 to 4.5.6.255 only for the UDP protocol on port number 55 and denies traffic to all other external hosts. The ports  section is optional and allows the user to specify specific ports  to and protocols to allow or deny traffic.</p> <p>The priority of a rule is determined by its placement in the egress array. An earlier rule is processed before a later rule. In the  previous example, if the rules are reversed, all traffic is denied, including any traffic to hosts in the 1.2.3.0/24 CIDR block.</p> <p>Using the DNS feature assumes that the nodes and masters are located in a similar location as the DNS entries that are added to the ovn database are generated by the master.</p> <p>NOTE: use Caution when using DNS names in deny rules. The DNS interceptor will never work flawlessly and could allow access to a denied host if the DNS resolution on the node is different then in the master.</p>"},{"location":"features/network-security-controls/network-policy/","title":"NetworkPolicy","text":"<p>Kubernetes NetworkPolicy documentation</p> <p>Kubernetes NetworkPolicy API reference</p> <p>By default, the network traffic from and to K8s pods is not restricted in any way. Using NetworkPolicy is a way to enforce network isolation of selected pods. When a pod is selected by a NetworkPolicy allowed traffic is specified by the <code>Ingress</code> and <code>Egress</code> sections.  </p> <p>Each NetworkPolicy object consists of four sections:</p> <ol> <li><code>podSelector</code>: a label selector that determines which pods the NetworkPolicy applies to</li> <li><code>policyTypes</code>: determines which policy types are included, if none are selected then <code>Ingress</code> will always be set and <code>Egress</code> will be set if any <code>Egress</code> rules are applied </li> <li><code>Ingress rules</code>: determines the sources that pods selected by the ingress rule can receive traffic from </li> <li><code>Egress rules</code>: determines the sinks that pods selected by the egress rule can send trafic to </li> </ol>"},{"location":"features/network-security-controls/network-policy/#networkpolicy-features","title":"NetworkPolicy features","text":"<p>These are described in order and are additive </p>"},{"location":"features/network-security-controls/network-policy/#unicast-default-deny","title":"Unicast default-deny","text":"<p>When a pod is selected by one or more NetworkPolicies, the <code>policyTypes</code> is set to both <code>Ingress</code> and <code>Egress</code>, and if no rules are specified it becomes isolated and all unicast ingress and egress traffic is blocked for pods in the same namespce as the NetworkPolicy. </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre> <p>If only ingress traffic to all pods in a namespace needs to be blocked the following can be used </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n</code></pre> <p>And finally if only Egress traffic from all pods in a Namespace needs to be blocked </p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n</code></pre> OVN-Implementation:</p> <p>Every new NetworkPolicy creates a port group named <code>FOO_bar</code> where <code>FOO</code> is the policy's Namespace and <code>bar</code> is the policy's name.  All pods that the policy's <code>podSelector</code> selects are added to the port group.</p> <p>Additionally, two global deny PortGroups are also used, specifially: <code>IngressDefaultDeny</code> and <code>EgressDefaultDeny</code>.  Any pod selected by a NetworkPolicy in any Namespace is added to these PortGroups.</p> <p>subset of <code>ovn-nbctl find port-group</code> <pre><code>    _uuid               : 1deeac49-e87e-4e05-9324-beb8ef0dcef4\n    acls                : [3f864884-cdb7-44be-a60e-e4f743afc9d0, f174fcf1-a7c2-496d-9b96-136aaccc014f]\n    external_ids        : {name=ingressDefaultDeny}\n    name                : ingressDefaultDeny\n    ports               : [ce1bc4e5-0309-463f-9fd1-80f6e487e2d4]\n\n    _uuid               : 5249b7a2-36bf-4246-98ea-13d5c5e17c68\n    acls                : [36ed4154-71ab-4b8f-8119-5c4cc92708d9, c6663e29-99d0-4410-a2ff-f694a896a035]\n    external_ids        : {name=egressDefaultDeny}\n    name                : egressDefaultDeny\n    ports               : []\n</code></pre></p> <p>Two ACLs (four total) are added to each PortGroup:</p> <ol> <li>a drop policy with <code>priority=1000</code> and <code>direction=to-lport</code></li> </ol> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>   _uuid               : f174fcf1-a7c2-496d-9b96-136aaccc014f\n    action              : drop\n    direction           : to-lport\n    external_ids        : {default-deny-policy-type=Ingress}\n    log                 : false\n    match               : \"outport == @ingressDefaultDeny\"\n    meter               : []\n    name                : []\n    priority            : 1000\n    severity            : []\n</code></pre></p> <ol> <li>an allow policy for ARP traffic with <code>priority=1001</code>, <code>direction=to-lport</code>, and <code>match=arp</code></li> </ol> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>_uuid               : 3f864884-cdb7-44be-a60e-e4f743afc9d0\naction              : allow\ndirection           : to-lport\nexternal_ids        : {default-deny-policy-type=Ingress}\nlog                 : false\nmatch               : \"outport == @ingressDefaultDeny &amp;&amp; arp\"\nmeter               : []\nname                : []\npriority            : 1001\nseverity            : []\n</code></pre></p>"},{"location":"features/network-security-controls/network-policy/#applying-the-network-policy-to-specific-pods-using-specpodselector","title":"Applying the network policy to specific pods using <code>spec.podSelector</code>","text":"<p>In some cases only certain pods in a Namespace may need to be selected by a NetworkPolicy. To handle this feature the <code>spec.podSelector</code> field can be used as follows </p> <p>The <code>spec.podSelector</code> is a label selector, which can be either a list of labels (<code>app=nginx</code>) or a match expression. Only pods in the same Namespace as the NetworkPolicy can be selected by it. The end result is a list of zero or more pods to which this NetworkPolicy's <code>Ingress</code> and <code>Egress</code> sections will be applied.</p> <p>For example, to block all traffic to and from a pod labeled with <code>app=demo</code> the following can be used </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\nspec:\n  podSelector:\n    matchLabels:\n          app: demo\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"features/network-security-controls/network-policy/#applying-ingress-and-egress-rules-using-specingress-and-specegress","title":"Applying Ingress and Egress Rules using <code>spec.Ingress</code> and <code>spec.Egress</code>","text":"<p>In some cases we need to explicilty define what sources and sinks a pod is allowed to communicate with, to handle this feature the <code>spec.Ingress</code> and <code>spec.Egress</code> fields of a NeworkPolicy can be used </p> <p>These sections contain a list of ingress or egress \"peers\" (the <code>from</code> section for <code>ingress</code> and the <code>to</code> section for <code>egress</code>) and a list of IP ports/protocols (the <code>ports</code> section) to or from which traffic should be allowed. Each list element of each section is logically OR-ed with other elements.</p> <p>Each <code>from</code>/<code>to</code> section can contain the following selectors </p> <ol> <li><code>namespaceSelector</code>: a label selector matching all pods in zero or more Namespaces</li> <li><code>podSelector</code>: a label selector matching zero or more pods in the same Namespace as the NetworkPolicy</li> <li><code>namespaceSelector</code> and <code>podSelector</code>: when both are present in an element, selects only pods matching the <code>podSelector</code> from Namespaces matching the <code>namespaceSelector</code></li> <li><code>ipBlock</code>: an IP network in CIDR notation that can be either internal or external, with optional exceptions</li> </ol>"},{"location":"features/network-security-controls/network-policy/#specingress","title":"<code>spec.ingress</code>","text":"<p>Rules defined in <code>spec.Ingress</code> can match on two main sections, 1.<code>spec.Ingress.from</code> and 2.<code>spec.Ingress.ports</code></p> <ol> <li> <p><code>spec.Ingress.from</code> </p> <p>Specifies FROM what sources a network policy will allow traffic </p> <p>It contains three selectors which are described further below </p> <ul> <li> <p><code>spec.Ingress.from.ipBlock</code> </p> <p>The ip addresses from which to allow traffic, contains fields <code>spec.Ingress.from.ipBlock.cidr</code> to specify which ip address are allowed and  <code>spec.Ingress.from.ipBlock.except</code> to specifiy which address's are not allowed </p> </li> <li> <p><code>spec.Ingress.from.namespaceSelector</code> </p> <p>The Namespaces from which to allow traffic, uses matchLabels to select much like the <code>spec.Podselector</code> field</p> </li> <li> <p><code>spec.Ingress.from.podSelector</code> </p> <p>The pods from which to allow traffic, matches the same as described above</p> </li> </ul> </li> <li> <p><code>spec.Ingress.ports</code></p> <p>The ports that the <code>Ingress</code> rule matches on, contains fields <code>spec.Ingress.ports.port</code> which can be either numerical or named, if set all port names and numbers will be matched, and <code>spec.Ingress.ports.protocol</code> matches to the protocol of the provided port </p> </li> </ol>"},{"location":"features/network-security-controls/network-policy/#specegress","title":"<code>spec.egress</code>","text":"<p>Rules defined in <code>spec.Egress</code> can match on two main sections, 1.<code>spec.Egress.to</code> and 2.<code>spec.Egress.ports</code></p> <ol> <li> <p><code>spec.Egress.to</code> </p> <p>specifies TO what destinations a network policy will allow a pod to send traffic </p> <p>It contains three selectors which are described further below </p> <ul> <li> <p><code>spec.Egress.to.ipBlock</code> </p> <p>The ip addresses which a pod can send traffic to, contains fields <code>spec.Egress.from.ipBlock.cidr</code> to specify which ip address are allowed and  <code>spec.Egress.from.ipBlock.except</code> to specifiy which address's are not allowed</p> </li> <li> <p><code>spec.Egress.to.namespaceSelector</code> </p> <p>The Namespaces allowed to receive traffic, uses matchLabels to select much like the <code>spec.Podselector</code> field</p> </li> <li> <p><code>spec.Egress.to.podSelector</code> </p> <p>The pods allowed to receive traffic, uses matchLabels to select much like described <code>spec.Podselector</code> field</p> </li> </ul> </li> <li> <p><code>spec.Egress.ports</code></p> <p>The ports that the <code>Egress</code> rule matches on, contains fields <code>spec.Egress.ports.port</code> which can be either numerical or named, if set all port names and numbers will be matched, and <code>spec.Egress.ports.protocol</code> matches to the protocol of the provided port </p> </li> </ol>"},{"location":"features/network-security-controls/network-policy/#specingress-and-specegress-ovn-implementation","title":"<code>spec.ingress</code> and <code>spec.egress</code> OVN implementation","text":"<p>Each Namespace creates an AddressSet to which the IPs of all pods in that Namespace are added. This is used in NetworkPolicy <code>Ingress</code> and <code>Egress</code> sections to implement the Namespace selector.</p> <p>Each element in the <code>from</code> or <code>to</code> list results in an AddressSet containing the IP addresses of all peer pods(i.e all pods touched by this policy) As pods are created, updated, or deleted each AddressSet is updated to add the new pod if it matches the selectors, or to remove the pod if it used to match selectors but no longer does. Namespace label changes may also result in AddressSet updates to add or remove pods if the Namespace now matches or no longer matches the <code>namespaceSelector</code>.</p> <p>If an <code>ipBlock</code> is specified, an ACL with the label <code>ipblock_cidr=\"false\"</code> is added to the policy's PortGroup with <code>priority=1001</code> that allows traffic to or from the list of CIDRs in the <code>ipBlock</code>, any exceptions are added as <code>drop</code> ACLs to the policy's PortGroup with <code>priority=1010</code>.</p> <p>Examples: </p> <p>Given two pods in Namespace <code>default</code> called  <code>client1</code> and <code>client2</code> , and one pod in Mamespace <code>demo</code>, called <code>server</code> lets make a network policy that allows ingress traffic to the server from <code>client1</code> but bocks traffic from <code>client2</code> </p> <p>Notice the pod <code>client1</code> and Namespace <code>default</code> are labeled with <code>app=demo</code></p> <pre><code>[astoycos@localhost demo]$ kubectl get pods -o wide --show-labels --all-namespaces\nNAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE     IP           NODE                NOMINATED NODE   READINESS GATES   LABELS\ndefault              client1                                     1/1     Running   0          5m5s    10.244.2.5   ovn-worker          &lt;none&gt;           &lt;none&gt;            app=demo\ndefault              client2                                     1/1     Running   0          4m59s   10.244.1.4   ovn-worker2         &lt;none&gt;           &lt;none&gt;            &lt;none&gt;\ndemo                 server                                      1/1     Running   0          42s     10.244.2.6   ovn-worker          &lt;none&gt;           &lt;none&gt;            &lt;none&gt;\n</code></pre> <pre><code>[astoycos@localhost demo]$ kubectl get namespace --show-labels\nNAME                 STATUS   AGE   LABELS\ndefault              Active   94m   ns=default\ndemo                 Active   66m   &lt;none&gt;\n</code></pre> <p>Before applying the following network policy both pods <code>client1</code> and <code>client2</code> can reach the <code>server</code> pod</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-client\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          ns: default\n      podSelector:\n        matchLabels:\n          app: demo\n</code></pre> <p>after applying this Network policy in Namespace <code>demo</code> (<code>oc create -n demo -f policy.yaml</code>) only the pod <code>client1</code> can reach the <code>server</code> pod </p> <p>NOTE: in the above definition there is only a single <code>from</code> element allowing connections from Pods labeled <code>app=demo</code> in Namespaces with the label <code>app=demo</code></p> <p>if the from section was applied as follows </p> <p><pre><code>- from:\n    - namespaceSelector:\n        matchLabels:\n          ns: demo\n    - podSelector:\n        matchLabels:\n          app: demo\n</code></pre>   Then there would be two elements in the <code>from</code> array which allows connections from Pods labeled <code>app=demo</code> OR and Pod from Namespaces with the label <code>app=demo</code></p> <p>Now let's have a look at some of the OVN resources that are created along with this Network Policy </p> <p>For all worker nodes we can see the UUIDs of the logical ports corresponding to the pods <code>client1</code>, <code>client2</code>, and <code>server</code> </p> <p>ovn-worker</p> <p><pre><code>[root@ovn-control-plane ~]# ovn-nbctl lsp-list ovn-worker\nedb290cf-b250-4699-b102-7acbb6300dc9 (default_client1)\nc754a19d-1e8c-4415-99b9-66fdcdaed196 (demo_server)\n24c789a2-fc4b-42a5-bb16-5b1c19490b50 (k8s-ovn-worker)\n484b2004-a5c1-447c-b857-eb8e524a73f3 (kube-system_coredns-f9fd979d6-qp6xd)\n45163af0-08c2-4d42-9fc7-7b0ddc935bd8 (local-path-storage_local-path-provisioner-78776bfc44-lgzdf)\n0bb378f1-4e89-46a8-a455-7a891f64c7c8 (stor-ovn-worker)\n</code></pre> ovn-worker2</p> <pre><code>[root@ovn-control-plane ~]# ovn-nbctl lsp-list ovn-worker2\nd5030b96-1163-4ed0-90f8-41b3831d2a0b (default_client2)\n4da8fb64-0a43-4d76-a7ba-18941c078862 (k8s-ovn-worker2)\n37f58eeb-8c1a-437b-8b21-bcc1337b2e3f (kube-system_coredns-f9fd979d6-628xd)\n65019041-51b2-4599-913d-7f01c8eaa394 (stor-ovn-worker2)\n</code></pre> <p>Port Groups </p> <pre><code>[root@ovn-control-plane ~]# ovn-nbctl find port-group\n_uuid               : 2b74086c-9986-4f4d-8c97-3388625230e9\nacls                : []\nexternal_ids        : {name=clusterPortGroup}\nname                : clusterPortGroup\nports               : [24c789a2-fc4b-42a5-bb16-5b1c19490b50, 4da8fb64-0a43-4d76-a7ba-18941c078862, e556e329-d624-473a-8827-f022c17a8f60]\n\n_uuid               : a132ecce-dbca-4989-87f7-96e2f0b62a2c\nacls                : [b4e57f83-8b8f-4b37-b5e5-1f82704c49c4]\nexternal_ids        : {name=demo_allow-from-client}\nname                : a13757631697825269621\nports               : [c754a19d-1e8c-4415-99b9-66fdcdaed196]\n\n_uuid               : a32d9dda-d7fb-4ae8-b6a9-3af17d62aa7f\nacls                : [510d797c-6302-4171-8a08-eeaab67063f4, f9079cce-29aa-4d1b-b36b-ca39933ad4e6]\nexternal_ids        : {name=ingressDefaultDeny}\nname                : ingressDefaultDeny\nports               : [c754a19d-1e8c-4415-99b9-66fdcdaed196]\n\n_uuid               : 896a80ff-46f7-4837-a105-7b52cee0c625\nacls                : [660b10ea-0f2e-49cb-b620-ca4218e87ac6, 9bb634ff-cb69-44b6-a64d-09147cf337b5]\nexternal_ids        : {name=egressDefaultDeny}\nname                : egressDefaultDeny\nports               : []\n</code></pre> <p>Notice that the port corresponding to the pod <code>server</code> is included in the <code>ingressDefaultDeny</code> port group </p> <p>To bypass the ingress default deny and allow traffic from pod <code>client1</code> in Namespace <code>demo</code> as specificed in the network policy, an address set is created containing the ip address for the pod <code>client1</code> </p> <p>subset of <code>ovn-nbctl find address_set</code> <pre><code>_uuid               : 7dc68ee9-9628-4a6a-83f0-a92bfa0970c6\naddresses           : [\"10.244.2.5\"]\nexternal_ids        : {name=demo.allow-from-client.ingress.0_v4}\nname                : a14783882619065065142\n</code></pre></p> <p>Finally we can see the ingress ACL that allows traffic to the <code>server</code> pod by allowing <code>ip4.src</code> traffic FROM the address's in the address set <code>a14783882619065065142</code> TO the port group <code>@a13757631697825269621</code> which contains the port <code>c754a19d-1e8c-4415-99b9-66fdcdaed196</code> (corresponding to the <code>server</code>'s logical port)</p> <p>subset of <code>ovn-nbctl find ACL</code> <pre><code>_uuid               : b4e57f83-8b8f-4b37-b5e5-1f82704c49c4\naction              : allow-related\ndirection           : to-lport\nexternal_ids        : {Ingress_num=\"0\", ipblock_cidr=\"false\", l4Match=None, namespace=demo, policy=allow-from-client, policy_type=Ingress}\nlog                 : false\nmatch               : \"ip4.src == {$a14783882619065065142} &amp;&amp; outport == @a13757631697825269621\"\nmeter               : []\nname                : []\npriority            : 1001\nseverity            : []\n</code></pre></p> <p>TODO: Add more examples(good for first PRs), specifically replicate above scenario by matching on the pod's network(<code>ip_block</code>) rather than the pod itself </p>"},{"location":"features/user-defined-networks/user-defined-networks/","title":"User Defined Networks","text":""},{"location":"features/user-defined-networks/user-defined-networks/#introduction","title":"Introduction","text":"<p>User Defined Networks (UDNs) in OVN-Kubernetes offer flexible network configurations for users, going beyond the traditional single default network model for all pods within a Kubernetes cluster. This feature addresses the diverse and advanced networking requirements of various applications and use cases.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#motivation","title":"Motivation","text":"<p>Traditional Kubernetes networking, which typically connects all pods to a default Layer3 network, lacks the necessary flexibility for many modern use cases and advanced network capabilities. UDNs provide several key advantages:</p> <ul> <li>Workload/Tenant Isolation: UDNs enable the grouping of different application types into isolated networks within the cluster, preventing communication between them.</li> <li>Flexible Network Topologies: Users can create different types of overlay networks that suits their use cases and then attach their workloads to these networks which are then isolated natively.</li> <li>Overlapping Pod IPs: UDNs allow the creation of multiple networks within a cluster that can use the same IP address ranges for pods, expanding deployment scenarios.</li> </ul> <p>See the enhancement for more details.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>See the user-stories defined in the enhancement.</p> <p>The two main user stories are:</p>"},{"location":"features/user-defined-networks/user-defined-networks/#native-namespace-isolation-using-networks","title":"Native Namespace Isolation using Networks","text":"<p> Here the blue, green, purple and yellow networks within those namespaces cannot reach other and hence provide native isolation to the workloads in those networks from workloads in other networks.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#native-tenant-isolation-using-networks","title":"Native Tenant Isolation using Networks","text":"<p> Here the tenants BERLIN and MUNICH are isolated from each other. So the workloads in namespaces belonging to BERLIN across the four namespaces - purple, yellow, green and blue can talk to each other but they can't talk to the workloads belogning to MUNICH tenant across namespaces brown, cyan, orange and violet.</p> <p>There are more user stories which will be covered in the sections below with appropriate diagrams.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>This feature is enabled by default on all OVN-Kubernetes clusters. You don't need to do anything extra to start using this feature. There is a Feature Config option <code>--enable-network-segmentation</code> under <code>OVNKubernetesFeatureConfig</code> config that can be used to disable this feature. However, note that disabling the feature will not remove existing CRs in the cluster. This feature has to be enabled along with the flag for multiple-networks <code>--enable-multi-network</code> since UDNs use Network Attachment Definitions as underlying implementation detail construct and reuse the user-defined network controllers.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#workflow-description","title":"Workflow Description","text":"<p>A tenant consists of one or more namespaces in a cluster. Network segmentation can be achieved by attaching 1 or more namespaces as part of same network which are then not reachable from other namespaces in the cluster that are not part of that network.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#implementation-details","title":"Implementation Details","text":""},{"location":"features/user-defined-networks/user-defined-networks/#user-facing-api-changes","title":"User facing API Changes","text":"<p>The implementation of UDNs introduces two new Custom Resource Definitions (CRDs) for network creation:</p> <ul> <li> <p>Namespace-scoped UserDefinedNetwork (UDN): This CRD is for tenant owners, allowing them to create networks within their namespace. This provides isolation for their namespaces from other tenants' namespaces.</p> </li> <li> <p>Cluster-scoped ClusterUserDefinedNetwork (CUDN): This CRD provides cluster administrators with the ability to allow multiple namespaces to be part of the same network that is then isolated from other networks.</p> </li> </ul> <p>NOTE: For a namespace to be considered for UDN creation, it must be labeled with <code>k8s.ovn.org/primary-user-defined-network</code> at the time of its creation. This label cannot be updated later, and if absent, the namespace will not be considered for UDN creation.</p> <p>See the api-specification-docs for information on each of the fields</p>"},{"location":"features/user-defined-networks/user-defined-networks/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p><code>UserDefinedNetworks</code> is an opinionated implementation of multi-networking in Kubernetes. There are two types of UserDefinedNetworks:</p> <ul> <li><code>Primary</code>: Also known as P-UDN -&gt; Primary UserDefinedNetwork: This means the    network will act as the primary network for the pod and all default traffic    will pass through this network except for Kubelet healthchecks which still uses    the default cluster-wide network as Kubernetes is not multi-networking aware.</li> <li><code>Secondary</code>: Also known as S-UDN -&gt; Secondary UserDefinedNetwork: This means the    network will act as only a secondary network for the pod and only pod traffic    that is part of the secondary network may be routed through this interface. These    types of networks have existed for a long time usually created using    <code>NetworkAttachmentDefinitions</code> API but are now more standardised using UDN CRs.</li> </ul> <p>OVN-Kubernetes currently doesn't support north-south traffic for secondary networks and none of the core Kubernetes features like Services will work there. Primary networks on the other hand has full support for all features as present on cluster default network.</p> <p>UDNs can have flexible virtual network topologies to suit the use cases of end users. Currently supported topology types for a given network include:</p> <p>Layer3 Networks</p> <p><code>Layer3</code>: is a topology type wherein the pods or VMs are connected to their node\u2019s local router and all these routers are then connected to the distributed switch across nodes.   * Each pod would hence get an IP from the node's subnet segment   * When in doubt which topology to use go with layer3 which is the same topology     as the cluster default network   * Can be of type <code>primary</code> or <code>secondary</code></p> <p>Let's see how a Layer3 Network looks on the OVN layer.</p> <p></p> <p>Here we can see a blue and green P-UDN. On node1, pod1 is part of green UDN and pod2 is part of blue UDN. They each have a udn-0 interface that is attached to the UDN network and a eth0 interface that is attached to the cluster default network (grey color) which is only used for kubelet healthchecks.</p> <p>Layer2 Networks</p> <p><code>Layer2</code>: is a topology type wherein the pods or VMs are all connected to the same layer2 flat switch.   * Usually used when the applications deployed expect a layer2 type network     connection (Perhaps applications want a single broadcast domain, latency sensitive, use proprietary L2 protocols)   * Common in Virtualization world for seamless migration of the VM since     persistent IPs of the VMs can be preserved across nodes in your cluster     during live migration   * Can be of type <code>primary</code> or <code>secondary</code></p> <p></p> <p>Here we can see a blue and green P-UDN. On node1, pod1 is part of green UDN and pod2 is part of blue UDN. They each have a udn-0 interface that is attached to the UDN network and a eth0 interface that is attached to the cluster default network (grey color) which is only used for kubelet healthchecks.</p> <p>Localnet Networks</p> <p><code>Localnet</code>: is a topology type wherein the pods or VMs attached to a localnet network on the overlay can egress to the provider\u2019s physical network   * without SNATing to nodeIPs\u2026 preserves the podIPs   * podIPs can be on the same subnet as the provider\u2019s VLAN   * VLAN IDs can be used to mark the traffic coming from the localnet for     isolation on provider network   * Can be of type <code>secondary</code>, it cannot be a <code>primary</code> network of a pod.   * Only <code>ClusterUserDefinedNetwork</code> supports <code>localnet</code></p> <p></p> <p>Here we can see blue and green S-UDN localnet networks.</p> <p>The ovnkube-cluster-manager component watches for these CR's and the controller reacts to it by creating NADs under the hood. The ovnkube-controller watches for the NADs and creates the required OVN logical constructs in the OVN database. The ovnkube-node also adds the required gateway plumbing such as openflows and VRF tables and routes to provide networking to these networks.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#creating-userdefinednetworks","title":"Creating UserDefinedNetworks","text":"<p>Now that we understand what a UDN is, let's get handson!</p> <p>Let's create two namespaces <code>blue</code> and <code>green</code>:</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: blue\n  labels:\n    name: blue\n    k8s.ovn.org/primary-user-defined-network: \"\"\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: green\n  labels:\n    name: green\n    k8s.ovn.org/primary-user-defined-network: \"\"\n</code></pre> <p>Sample API yaml for create two <code>UserDefinedNetworks</code> of type <code>Layer3</code> in these namespaces:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: UserDefinedNetwork\nmetadata:\n  name: blue-network\n  namespace: blue\n  labels:\n    name: blue\n    purpose: kubecon-eu-2025-demo\nspec: \n  topology: Layer3\n  layer3:\n    role: Primary\n    subnets:\n    - cidr: 103.103.0.0/16\n      hostSubnet: 24\n---\napiVersion: k8s.ovn.org/v1\nkind: UserDefinedNetwork\nmetadata:\n  name: green-network\n  namespace: green\n  labels:\n    name: green\n    purpose: kubecon-eu-2025-demo\nspec: \n  topology: Layer3\n  layer3:\n    role: Primary\n    subnets:\n    - cidr: 203.203.0.0/16\n      hostSubnet: 24\n</code></pre>"},{"location":"features/user-defined-networks/user-defined-networks/#inspecting-a-udn-pod","title":"Inspecting a UDN Pod","text":"<p>Now if you create pods on these two namespaces and try to ping one pod from the other pod, you will see that connection won't work.</p> <pre><code> $ k get pods -n blue -owide\n NAME    READY   STATUS    RESTARTS   AGE   IP           NODE          \n blue    1/1     Running   0          9h    10.244.0.7   ovn-worker  \n blue1   1/1     Running   0          8h    10.244.1.4   ovn-worker2  \n\n $ k get pods -n green -owide\n NAME    READY   STATUS    RESTARTS   AGE   IP           NODE      \n green   1/1     Running   0          9h    10.244.0.6   ovn-worker\n</code></pre> <p>NOTE: Doing kubectl get pods and describe pod will all show the default network podIP which is not to be confused with the UDN podIPs. Remember how we said Kubernetes is not multi-networking aware? Hence pod.Status.IPs will always be the IPs that kubelet is aware of for healthchecks to work.</p> <p>In order to see the real UDN PodIPs, always do a describe on the pod and see the following annotations on the pod: <pre><code>$ k get pod -n green green -oyaml                                                                                        \napiVersion: v1                                                                                                                                                               \nkind: Pod                                                                                                                                                                    \nmetadata:                                                                                                                                                                    \n  annotations:                                                                                                                                                               \n    k8s.ovn.org/pod-networks: '{\"default\":{\"ip_addresses\":[\"10.244.0.6/24\"],\n    \"mac_address\":\"0a:58:0a:f4:00:06\",\"routes\":[{\"dest\":\"10.244.0.0/16\",\n    \"nextHop\":\"10.244.0.1\"},{\"dest\":\"100.64.0.0/16\",\"nextHop\":\"10.244.0.1\"}],\n    \"ip_address\":\"10.244.0.6/24\",\"role\":\"infrastructure-locked\"},\n    \"green/green-network\":{\"ip_addresses\":[\"203.203.2.5/24\"],\n    \"mac_address\":\"0a:58:c8:0a:02:05\",\"gateway_ips\":[\"203.203.2.1\"],\n    \"routes\":[{\"dest\":\"203.203.0.0/16\",\"nextHop\":\"203.203.2.1\"},\n    {\"dest\":\"10.96.0.0/16\",\"nextHop\":\"203.203.2.1\"},{\"dest\":\"100.65.0.0/16\",\n    \"nextHop\":\"203.203.2.1\"}],\"ip_address\":\"203.203.2.5/24\",\"gateway_ip\":\"203.203.2.1\",\n    \"role\":\"primary\"},\"green/green-secondary-network\":{\"ip_addresses\":[\"100.10.1.7/24\"],\n    \"mac_address\":\"0a:58:64:0a:01:07\",\"routes\":[{\"dest\":\"100.10.0.0/16\",\n    \"nextHop\":\"100.10.1.1\"}],\"ip_address\":\"100.10.1.7/24\",\"role\":\"secondary\"}}'\n</code></pre> The above shows the OVN-Kubernetes IPAM Annotation for each type of network: * <code>default</code> which is the cluster-wide <code>infrastructure-locked</code> network only used   for Kubelet health checks and pod has IP 10.244.0.6 here * <code>primary</code> which is the primary UDN for the pod through which all traffic   passes through and pod has IP 203.203.2.5. * <code>secondary</code> which is the secondary UDN network for the pod from which pod has IP 100.10.1.7</p> <p>One can also use the multus annotation to figure out the podIPs on each interface:</p> <pre><code>$ oc get pod -n green green -oyaml                                                                                          \napiVersion: v1                                                                                                                                                               \nkind: Pod                                                                                                                                                                    \nmetadata:                                                                                                                                                                    \n  annotations:                                                                                                                                                                                                                                 \n        k8s.v1.cni.cncf.io/network-status: |-                                                                                                                                    \n      [{                                                                                                                                                                     \n          \"name\": \"ovn-kubernetes\",                                                                                                                                          \n          \"interface\": \"eth0\",                                                                                                                                               \n          \"ips\": [                                                                                                                                                           \n              \"10.244.0.6\"                                                                                                                                                   \n          ],                                                                                                                                                                 \n          \"mac\": \"0a:58:0a:f4:00:06\",                                                                                                                                        \n          \"dns\": {}                                                                                                                                                          \n      },{                                                                                                                                                                    \n          \"name\": \"ovn-kubernetes\",                                                                                                                                          \n          \"interface\": \"ovn-udn1\",                                                                                                                                           \n          \"ips\": [                                                                                                                                                           \n              \"200.203.2.5\"                                                                                                                                                   \n          ],                                                                                                                                                                 \n          \"mac\": \"0a:58:c8:0a:02:05\",                                                                                                                                        \n          \"default\": true,                                                                                                                                                   \n          \"dns\": {}                                                                                                                                                          \n      },{                                                                                                                                                                    \n          \"name\": \"green/green-secondary-network\",                                                                                                                           \n          \"interface\": \"net1\",                                                                                                                                               \n          \"ips\": [                                                                                                                                                           \n              \"100.10.1.7\"                                                                                                                                                   \n          ],                                                                                                                                                                 \n          \"mac\": \"0a:58:64:0a:01:07\",                                                                                                                                        \n          \"dns\": {}                                                                                                                                                          \n      }]\n</code></pre>"},{"location":"features/user-defined-networks/user-defined-networks/#kubelethealthchecks-for-udn-pods","title":"KubeletHealthChecks for UDN pods","text":"<p>In each of the above diagrams we saw a grey network still attached to all pods across all UDNs. This represents the cluster default network which is <code>infrastructure-locked</code> for primary-UDN pods and is only used for healthchecks.</p> <p>We add UDN Isolation ACLs and cgroups NFTable rules on these pod ports so that no traffic except healthcheck traffic from kubelet is allowed to reach these pods.</p> <p>Using OVN ACLs, we ensure only traffic from kubelet is allowed on the default <code>eth0</code> interface of the pods:</p> <pre><code>_uuid               : 1278b0f4-0a14-4637-9d05-83ba9df6ec03\naction              : allow\ndirection           : from-lport\nexternal_ids        : {direction=Egress, \"k8s.ovn.org/id\"=\"default-network-controller:UDNIsolation:AllowHostARPPrimaryUDN:Egress\", \"k8s.ovn.org/name\"=AllowHostARPPrimaryUDN, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=UDNIsolation}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a8747502060113802905 &amp;&amp; (( arp &amp;&amp; arp.tpa == 10.244.2.2 ) || ( nd &amp;&amp; nd.target == fd00:10:244:3::2 ))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n\n_uuid               : 489ae95b-ae9d-47d0-bf1d-b2477a9ed6a2\naction              : allow\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, \"k8s.ovn.org/id\"=\"default-network-controller:UDNIsolation:AllowHostARPPrimaryUDN:Ingress\", \"k8s.ovn.org/name\"=AllowHostARPPrimaryUDN, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=UDNIsolation}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a8747502060113802905 &amp;&amp; (( arp &amp;&amp; arp.spa == 10.244.2.2 ) || ( nd &amp;&amp; nd.target == fd00:10:244:3::2 ))\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1001\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n\n\n_uuid               : 980be3e4-75af-45f7-bce3-3bb08ecd8b3a\naction              : drop\ndirection           : to-lport\nexternal_ids        : {direction=Ingress, \"k8s.ovn.org/id\"=\"default-network-controller:UDNIsolation:DenyPrimaryUDN:Ingress\", \"k8s.ovn.org/name\"=DenyPrimaryUDN, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=UDNIsolation}\nlabel               : 0\nlog                 : false\nmatch               : \"outport == @a8747502060113802905\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1000\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n\n_uuid               : cca19dca-1fde-4a14-841d-7e2cce804de4\naction              : drop\ndirection           : from-lport\nexternal_ids        : {direction=Egress, \"k8s.ovn.org/id\"=\"default-network-controller:UDNIsolation:DenyPrimaryUDN:Egress\", \"k8s.ovn.org/name\"=DenyPrimaryUDN, \"k8s.ovn.org/owner-controller\"=default-network-controller, \"k8s.ovn.org/owner-type\"=UDNIsolation}\nlabel               : 0\nlog                 : false\nmatch               : \"inport == @a8747502060113802905\"\nmeter               : acl-logging\nname                : []\noptions             : {}\npriority            : 1000\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n</code></pre> <p></p> <p>As you can see here a default network pod, <code>pod2</code> can't reach the UDN pod <code>pod1</code> via its eth0 interface thanks to the ACLs in place. So no traffic from the UDN pod ever leaves via <code>eth0</code>. The only traffic that is allowed via <code>eth0</code> interface is the kubelet probe traffic.</p> <p>But given how we have allow ACLs for kubelet traffic, but this matches on management portIP which is the hostIP, any process on the host can potentially reach the UDN pods. In order to have more tighter security, we have cgroups based NFT rules on the host to prevent any non-kubelet process from being able to reach the default network <code>eth0</code> port on UDN pods.</p> <p></p> <p>These rules look like this:</p> <pre><code>    chain udn-isolation {\n        comment \"Host isolation for user defined networks\"\n        type filter hook output priority filter; policy accept;\n        ip daddr . meta l4proto . th dport @udn-open-ports-v4 accept\n        ip daddr @udn-open-ports-icmp-v4 meta l4proto icmp accept\n        socket cgroupv2 level 2 475436 ip daddr @udn-pod-default-ips-v4 accept\n        ip daddr @udn-pod-default-ips-v4 drop\n        ip6 daddr . meta l4proto . th dport @udn-open-ports-v6 accept\n        ip6 daddr @udn-open-ports-icmp-v6 meta l4proto ipv6-icmp accept\n        socket cgroupv2 level 2 475436 ip6 daddr @udn-pod-default-ips-v6 accept\n        ip6 daddr @udn-pod-default-ips-v6 drop\n    }\n\n        set udn-open-ports-v4 {\n        type ipv4_addr . inet_proto . inet_service\n        comment \"default network open ports of pods in user defined networks (IPv4)\"\n    }\n\n    set udn-open-ports-v6 {\n        type ipv6_addr . inet_proto . inet_service\n        comment \"default network open ports of pods in user defined networks (IPv6)\"\n    }\n\n    set udn-open-ports-icmp-v4 {\n        type ipv4_addr\n        comment \"default network IPs of pods in user defined networks that allow ICMP (IPv4)\"\n    }\n\n    set udn-open-ports-icmp-v6 {\n        type ipv6_addr\n        comment \"default network IPs of pods in user defined networks that allow ICMP (IPv6)\"\n    }\n\n    set udn-pod-default-ips-v4 {\n        type ipv4_addr\n        comment \"default network IPs of pods in user defined networks (IPv4)\"\n    }\n\n    set udn-pod-default-ips-v6 {\n        type ipv6_addr\n        comment \"default network IPs of pods in user defined networks (IPv6)\"\n    }\n</code></pre> <p>The only exception to this is when users annotate the UDN pod using the <code>open-default-ports</code> annotation: <pre><code>k8s.ovn.org/open-default-ports: |\n      - protocol: tcp\n        port: 80\n      - protocol: udp\n        port: 53\n</code></pre> which means we open up allow ACLs and nftrules to allow traffic to reach at those ports.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#overlapping-podips","title":"Overlapping PodIPs","text":"<p>Two networks can have the same subnet since they are completely isolated. We use a <code>masqueradeIP</code> SNAT per UDN to avoid conntrack collisions on the host. So traffic leaving each UDN is SNATed to a unique IP before being sent to the host.</p> <p></p>"},{"location":"features/user-defined-networks/user-defined-networks/#vm-livemigration-and-persistentips-over-layer2-udns","title":"VM LiveMigration and PersistentIPs over Layer2 UDNs","text":"<p>Users can use the <code>layer2</code> topology when creating virtual machines on OVN-Kubernetes and can easily live migrate the VMs across nodes along with preserving their IPs.</p> <p></p>"},{"location":"features/user-defined-networks/user-defined-networks/#services-on-udns","title":"Services on UDNs","text":"<p>Creating a service on UDNs is same as creating them on default network, no extra plumbing is required.</p> <p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: service-blue\n  namespace: blue\n  labels:\n    network: blue\nspec:\n  type: LoadBalancer\n  selector:\n    network: blue\n  ports:\n  - name: web\n    port: 80\n    targetPort: 8080\n</code></pre> <pre><code>$ k get svc -n blue\nNAME           TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice-blue   LoadBalancer   10.96.207.175   172.19.0.10     80:31372/TCP   5s\n$ k get endpointslice -n blue\nNAME                 ADDRESSTYPE   PORTS   ENDPOINTS                 AGE\nservice-blue-55d6c   IPv4          8080    103.103.1.5,103.103.0.5   65s\nservice-blue-pkll7   IPv4          8080    10.244.0.3,10.244.1.8     66s\n</code></pre> One set of endpoints show the UDN ntework IPs of the pods and the other set shows default network IPs.</p> <p>When the service is created inside the blue namespace, the clusterIPs get automatically isolated from pods in other networks. However nodeports, loadbalancerIPs and externalIPs can be reached across UDNs.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#endpointslices-mirror-controller-for-user-defined-networks","title":"EndpointSlices mirror controller for User-Defined Networks","text":"<p>Pods that use a UDN as their primary network will still have the cluster default network IP in their status. For services this results in the EndpointSlices providing the IPs of the cluster default network in the Kubernetes API. To enable services support for primary user-defined networks, the EndpointSlices mirror controller was introduced to create custom EndpointSlices with user-defined network IP addresses extracted from OVN-Kubernetes annotations.</p> <p>The introduced controller duplicates the default EndpointSlices, creating new copies that include IP addresses from primary user-defined network. It bypasses EndpointSlices in namespaces that do not have a user-defined primary network. The controller lacks specific logic for selecting endpoints, it only replicates those generated by the default controller and replaces the IP addresses. For host-networked pods, the controller retains the same IP addresses as the default controller. Custom EndpointSlices not created by the default controller are not processed.</p> <p>The default EndpointSlices controller creates objects that contain the following labels:</p> <ul> <li><code>endpointslice.kubernetes.io/managed-by:endpointslice-controller.k8s.io</code> - Indicates   that the EndpointSlice is managed by the default Kubernetes EndpointSlice controller.</li> <li><code>kubernetes.io/service-name:&lt;service-name&gt;</code> - The service that this EndpointSlice   belongs to, used by the default network service controller.</li> </ul> <p>The EndpointSlices mirror controller uses a separate set of labels:</p> <ul> <li><code>endpointslice.kubernetes.io/managed-by:endpointslice-mirror-controller.k8s.ovn.org</code> -  Indicates   that the EndpointSlice is managed by the mirror controller.</li> <li><code>k8s.ovn.org/service-name:&lt;service-name&gt;</code> - The service that this mirrored EndpointSlice   belongs to, used by the user-defined network service controller. Note that the label   key is different from the default EndpointSlice.</li> <li><code>k8s.ovn.org/source-endpointslice-version:&lt;default-endpointslice-resourceversion&gt;</code> - The   last reconciled resource version from the default EndpointSlice.</li> </ul> <p>and annotations (Label values have a length limit of 63 characters): - <code>k8s.ovn.org/endpointslice-network:&lt;udn-network-name&gt;</code> - The user-defined network   that the IP addresses in the mirrored EndpointSlice belong to. - <code>k8s.ovn.org/source-endpointslice:&lt;default-endpointslice&gt;</code> -  The name of the   default EndpointSlice that was the source of the mirrored EndpointSlice.</p> <p>Example:</p> <p>With the following NetworkAttachmentDefinition:</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l3-network\n  namespace: nad-l3\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"1.0.0\",\n            \"name\": \"l3-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer3\",\n            \"subnets\": \"10.128.0.0/16/24\",\n            \"mtu\": 1300,\n            \"netAttachDefName\": \"nad-l3/l3-network\",\n            \"role\": \"primary\"\n    }\n</code></pre> <p>We can observe the following EndpointSlices created for a one-replica deployment exposed through a <code>sample-deployment</code> service:</p> Default EndpointSlice Mirrored EndpointSlice <pre><code>kind: EndpointSlice\napiVersion: discovery.k8s.io/v1\nmetadata:\n  name: sample-deployment-rkk4n\n  generateName: sample-deployment-\n  generation: 1\n  labels:\n    app: l3pod\n    endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io\n    kubernetes.io/service-name: sample-deployment\n  name: sample-deployment-rkk4n\n  namespace: nad-l3\n  resourceVersion: \"31533\"\naddressType: IPv4\nendpoints:\n- addresses:\n  - 10.244.1.17\n  conditions:\n    ready: true\n    serving: true\n    terminating: false\n  nodeName: ovn-worker\n  targetRef:\n    kind: Pod\n    name: sample-deployment-6b64bd4868-7ftt6\n    namespace: nad-l3\n    uid: 6eb5d05c-cff4-467d-bc1b-890443750463\nports:\n- name: \"\"\n  port: 80\n  protocol: TCP\n</code></pre> <pre><code>kind: EndpointSlice\napiVersion: discovery.k8s.io/v1\nmetadata:\n  name: l3-network-sample-deployment-hgkmw\n  generateName: l3-network-sample-deployment-\n  labels:\n    endpointslice.kubernetes.io/managed-by: endpointslice-mirror-controller.k8s.ovn.org\n    k8s.ovn.org/service-name: sample-deployment\n    k8s.ovn.org/source-endpointslice-version: \"31533\"\n  annotations:\n    k8s.ovn.org/endpointslice-network: l3-network\n    k8s.ovn.org/source-endpointslice: sample-deployment-rkk4n\n  namespace: nad-l3\n  resourceVersion: \"31535\"\naddressType: IPv4\nendpoints:\n- addresses:\n  - 10.128.1.3\n  conditions:\n    ready: true\n    serving: true\n    terminating: false\n  nodeName: ovn-worker\n  targetRef:\n    kind: Pod\n    name: sample-deployment-6b64bd4868-7ftt6\n    namespace: nad-l3\n    uid: 6eb5d05c-cff4-467d-bc1b-890443750463\nports:\n- name: \"\"\n  port: 80\n  protocol: TCP\n</code></pre> <p>That's how behind the scenes services on UDNs are implemented.</p>"},{"location":"features/user-defined-networks/user-defined-networks/#references","title":"References","text":"<ul> <li>Use the workshop yamls here to play around</li> </ul>"},{"location":"getting-started/configuration/","title":"Config Variables","text":"<p>Let us look at the supported configuration variables by OVN-Kubernetes</p>"},{"location":"getting-started/configuration/#default-config","title":"Default Config","text":""},{"location":"getting-started/configuration/#gateway-config","title":"Gateway Config","text":""},{"location":"getting-started/configuration/#disable-forwarding-config","title":"Disable Forwarding Config","text":"<p>OVN-Kubernetes allows to enable or disable IP forwarding for all traffic on OVN-Kubernetes managed interfaces (such as br-ex). By default forwarding is enabled and this allows host to forward traffic across OVN-Kubernetes managed interfaces. If forwarding is disabled then Kubernetes related traffic is still forwarded appropriately, but other IP traffic will not be routed by cluster nodes.</p> <p>IP forwarding is implemented at cluster node level by modifying both iptables <code>FORWARD</code> chain and IP forwarding <code>sysctl</code> parameters. </p>"},{"location":"getting-started/configuration/#ipv4","title":"IPv4","text":"<p>If forwarding is enabled(default) and it is desired to allow forwarding for traffic on unmanaged ovn-kubernetes interfaces, then system administrators need to set the following sysctl parameters on the desired interfaces or globally. OVN-Kubernetes already sets sysctl forwarding for interfaces it manages, such as the ovn-k8s-mp0 interface and the shared gateway bridge interface. An operator can be built to manage forwarding sysctl parameters based on forwarding mode. No extra iptables rules are added by OVN-Kubernetes to FORWARD chain while using this IP forwarding mode.</p> <p><pre><code>net.ipv4.ip_forward=1\n</code></pre> IP forwarding can be disabled either by setting <code>disable-forwarding</code> command line option to <code>true</code> while starting ovnkube or by setting <code>disable-forwarding</code> to <code>true</code> in config file. If forwarding is disabled the default policy for the FORWARD iptables chain is set as DROP and system administrators can add use-case specific ACCEPT rules.</p> <p>When IP forwarding is disabled, following sysctl parameters are modified by OVN-Kubernetes to allow forwarding Kubernetes related traffic on OVN-Kubernetes managed bridge interfaces and management port interface.</p> <pre><code>net.ipv4.conf.br-ex.forwarding=1\nnet.ipv4.conf.ovn-k8s-mp0.forwarding = 1\n</code></pre>"},{"location":"getting-started/configuration/#ipv6","title":"IPv6","text":"<p>IP forwarding works differently for IPv6: <pre><code>/proc/sys/net/ipv6/* Variables:\n\nconf/all/forwarding - BOOLEAN\n  Enable global IPv6 forwarding between all interfaces.\n  IPv4 and IPv6 work differently here; e.g. netfilter must be used\n  to control which interfaces may forward packets and which not.\n\n...\n\nconf/interface/*:\n\nforwarding - INTEGER\n    Configure interface-specific Host/Router behaviour.\n\n    Note: It is recommended to have the same setting on all\n    interfaces; mixed router/host scenarios are rather uncommon.\n\n    Possible values are:\n        0 Forwarding disabled\n        1 Forwarding enabled\n\n    FALSE (0):\n\n    By default, Host behaviour is assumed.  This means:\n\n    1. IsRouter flag is not set in Neighbour Advertisements.\n    2. If accept_ra is TRUE (default), transmit Router\n       Solicitations.\n    3. If accept_ra is TRUE (default), accept Router\n       Advertisements (and do autoconfiguration).\n    4. If accept_redirects is TRUE (default), accept Redirects.\n\n    TRUE (1):\n\n    If local forwarding is enabled, Router behaviour is assumed.\n    This means exactly the reverse from the above:\n\n    1. IsRouter flag is set in Neighbour Advertisements.\n    2. Router Solicitations are not sent unless accept_ra is 2.\n    3. Router Advertisements are ignored unless accept_ra is 2.\n    4. Redirects are ignored.\n\n    Default: 0 (disabled) if global forwarding is disabled (default),\n         otherwise 1 (enabled).\n</code></pre> https://www.kernel.org/doc/Documentation/networking/ip-sysctl.txt</p> <p>It is not possible to configure the IPv6 forwarding per interface by setting the  <code>net.ipv6.conf.IFNAME.forwarding</code> sysctl (it just configures the interface-specific Host/Router  behaviour). \\ Instead, the opposite approach is required where the global forwarding is always enabled and  the traffic is restricted through iptables.</p>"},{"location":"getting-started/configuration/#forwarding-rules","title":"Forwarding rules","text":"<p>When the <code>disable-forwarding</code> parameter is configured specific iptables rules are added to the FORWARD chain to forward clusterNetwork and serviceNetwork traffic to their intended destinations. Additionally, the default policy for the FORWARD chain is set as <code>DROP</code>. Otherwise, the policy defaults to <code>ACCEPT</code> and no custom rules are added. This behavior is the same for both IPv6 and IPv4 networks:</p> <pre><code># In IPv4 with disable-forwarding=true the FORWARD policy is set to DROP\nChain FORWARD (policy DROP 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            169.254.169.1       \n    0     0 ACCEPT     0    --  *      *       169.254.169.1        0.0.0.0/0           \n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            10.96.0.0/16        \n    0     0 ACCEPT     0    --  *      *       10.96.0.0/16         0.0.0.0/0           \n    0     0 ACCEPT     0    --  *      *       0.0.0.0/0            10.244.0.0/16       \n    0     0 ACCEPT     0    --  *      *       10.244.0.0/16        0.0.0.0/0           \n    0     0 ACCEPT     0    --  ovn-k8s-mp0 *       0.0.0.0/0            0.0.0.0/0           \n    0     0 ACCEPT     0    --  *      ovn-k8s-mp0  0.0.0.0/0            0.0.0.0/0\n\n# In IPv6 with disable-forwarding=true the FORWARD policy is set to DROP\nChain FORWARD (policy DROP 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 ACCEPT     0    --  *      *       ::/0                 fd69::1             \n    0     0 ACCEPT     0    --  *      *       fd69::1              ::/0                \n    0     0 ACCEPT     0    --  *      *       ::/0                 fd00:10:96::/112    \n    0     0 ACCEPT     0    --  *      *       fd00:10:96::/112     ::/0                \n    0     0 ACCEPT     0    --  *      *       ::/0                 fd00:10:244::/48    \n    0     0 ACCEPT     0    --  *      *       fd00:10:244::/48     ::/0                \n    0     0 ACCEPT     0    --  ovn-k8s-mp0 *       ::/0                 ::/0                \n    0     0 ACCEPT     0    --  *      ovn-k8s-mp0  ::/0                 ::/0          \n</code></pre>"},{"location":"getting-started/configuration/#vlan-config","title":"VLAN Config","text":"<p>OVN-Kubernetes supports using VLAN tagging for underlay connectivity. To enable VLAN tagging, specify the <code>vlan-id</code> gateway configuration option with the desired VLAN tag. This tag will be used for traffic ingressing and egressing OVN as well as the host. When <code>vlan-id</code> is configured:</p> <ul> <li>OVN will be configured to accept the VLAN tag specified by <code>vlan-id</code></li> <li>The external gateway bridge will be configured to add/strip the VLAN tag specified by <code>vlan-id</code> for packets destined to and coming from the host</li> <li>The physical interface attached to the external gateway bridge will act as a VLAN trunk</li> </ul> <p>Note, it is not required to configure a VLAN sub-interface (802.1q interface) on the host, OVN-Kubernetes will automatically handle VLAN tagging in the OVS external bridge. It is also supported to have additional interfaces attached to the external gateway bridge that use different VLAN tags than VLANID. These interfaces will operate in their own VLAN, and share the physical interface as a trunk.</p>"},{"location":"getting-started/configuration/#logging-config","title":"Logging Config","text":""},{"location":"getting-started/configuration/#monitoring-config","title":"Monitoring Config","text":""},{"location":"getting-started/configuration/#ipfix-config","title":"IPFIX Config","text":""},{"location":"getting-started/configuration/#cni-config","title":"CNI Config","text":""},{"location":"getting-started/configuration/#kubernetes-config","title":"Kubernetes Config","text":""},{"location":"getting-started/configuration/#metrics-config","title":"Metrics Config","text":""},{"location":"getting-started/configuration/#ovn-kubernetes-feature-config","title":"OVN-Kubernetes Feature Config","text":""},{"location":"getting-started/configuration/#enable-multiple-networks","title":"Enable Multiple Networks","text":"<p>Users can create pods with multiple interfaces such that each interface is hooked to a separate network thereby enabling multiple networks for a given pod; a.k.a multi-homing. All networks that are created as additions to the primary default Kubernetes network are fondly called <code>secondary networks</code>. This feature can be enabled by using the <code>--enable-multi-network</code> flag on OVN-Kubernetes clusters.</p>"},{"location":"getting-started/configuration/#enable-network-segmentation","title":"Enable Network Segmentation","text":"<p>Users can enable the network-segmentation feature using <code>--enable-network-segmentation</code> flag on a KIND cluster. This allows users to be able to design native isolation between their tenant namespaces by coupling all namespaces that belong to the same tenant under the same secondary network and then making this network the primary network for the pod. Each network is isolated and cannot talk to other user defined network. Check out the feature docs for more information on how to segment your cluster on a network level.</p> <p>NOTE: This feature only works if <code>--enable-multi-network</code> is also enabled since it leverages the secondary networks feature.</p>"},{"location":"getting-started/configuration/#ha-config","title":"HA Config","text":""},{"location":"getting-started/configuration/#ovn-auth-config","title":"OVN Auth Config","text":""},{"location":"getting-started/configuration/#hybrid-overlay-config","title":"Hybrid Overlay Config","text":""},{"location":"getting-started/configuration/#cluster-manager-config","title":"Cluster Manager Config","text":""},{"location":"getting-started/overview/","title":"Work in progress","text":""},{"location":"governance/CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"governance/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"governance/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement directly. Maintainers are identified in the MAINTAINERS.md file and their contact information is on their GitHub profile page. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"governance/CODE_OF_CONDUCT/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"governance/CODE_OF_CONDUCT/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"governance/CODE_OF_CONDUCT/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"governance/CODE_OF_CONDUCT/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"governance/CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"governance/CONTRIBUTING/","title":"Contributing Guide","text":"<p>The contributing guide is maintained in the repository root.</p> <p>For the full guide, including ways to contribute, pull request lifecycle, and development setup, see:</p> <p>CONTRIBUTING.md in the OVN-Kubernetes repository.</p>"},{"location":"governance/GOVERNANCE/","title":"ovn-kubernetes Project Governance","text":"<p>The ovn-kubernetes project is dedicated to creating a robust Kubernetes Networking platform built from the ground up by leveraging Open vSwitch (OVS) as the data plane, and Open Virtual Network (OVN) as the SDN Controller. The project focuses strictly on enhancing networking for the Kubernetes platform and includes a wide variety of features that are critical to enterprise and telco users.</p> <p>This governance explains how the project is run.</p> <ul> <li>Values</li> <li>Maintainers</li> <li>Becoming a Maintainer</li> <li>Removing a Maintainer</li> <li>Members</li> <li>Becoming a Member</li> <li>Removing a Member</li> <li>Meetings</li> <li>Code of Conduct</li> <li>Security Response Team</li> <li>Voting</li> <li>Modifying this Charter</li> </ul>"},{"location":"governance/GOVERNANCE/#values","title":"Values","text":"<p>The ovn-kubernetes and its leadership embrace the following values:</p> <ul> <li> <p>Openness: Communication and decision-making happens in the open and is discoverable for future   reference. As much as possible, all discussions and work take place in public   forums and open repositories.</p> </li> <li> <p>Fairness: All stakeholders have the opportunity to provide feedback and submit   contributions, which will be considered on their merits.</p> </li> <li> <p>Community over Product or Company: Sustaining and growing our community takes   priority over shipping code or sponsors' organizational goals.  Each   contributor participates in the project as an individual.</p> </li> <li> <p>Inclusivity: We innovate through different perspectives and skill sets, which   can only be accomplished in a welcoming and respectful environment.</p> </li> <li> <p>Participation: Responsibilities within the project are earned through   participation, and there is a clear path up the contributor ladder into leadership   positions.</p> </li> </ul>"},{"location":"governance/GOVERNANCE/#maintainers","title":"Maintainers","text":"<p>ovn-kubernetes Maintainers have write access to the project GitHub repository. They can merge their own patches or patches from others. The current maintainers can be found in MAINTAINERS.md.  Maintainers collectively manage the project's resources and contributors.</p> <p>This privilege is granted with some expectation of responsibility: maintainers are people who care about the ovn-kubernetes project and want to help it grow and improve. A maintainer is not just someone who can make changes, but someone who has demonstrated their ability to collaborate with the team, get the most knowledgeable people to review code and docs, contribute high-quality code, and follow through to fix issues (in code or tests).</p> <p>A maintainer is a contributor to the project's success and a citizen helping the project succeed.</p> <p>The collective team of all Maintainers is known as the Maintainer Council, which is the governing body for the project.</p>"},{"location":"governance/GOVERNANCE/#becoming-a-maintainer","title":"Becoming a Maintainer","text":"<p>To become a Maintainer you need to demonstrate the following:</p> <ul> <li>commitment to the project:</li> <li>participate in discussions, contributions, code and documentation reviews     for 10 months or more,</li> <li>perform reviews for 10 non-trivial pull requests,</li> <li>contribute 15 non-trivial pull requests and have them merged,</li> <li>ability to write quality code and/or documentation,</li> <li>ability to collaborate with the team,</li> <li>understanding of how the team works (policies, processes for testing and code review, etc),</li> <li>understanding of the project's code base and coding and documentation style.</li> </ul> <p>A new Maintainer must be proposed by an existing maintainer by sending a message to the developer mailing list. A simple majority vote of existing Maintainers approves the application.  Maintainers nominations will be evaluated without prejudice to employer or demographics.</p> <p>Maintainers who are selected will be granted the necessary GitHub rights.</p>"},{"location":"governance/GOVERNANCE/#removing-a-maintainer","title":"Removing a Maintainer","text":"<p>Maintainers may resign at any time if they feel that they will not be able to continue fulfilling their project duties.</p> <p>Maintainers may also be removed after being inactive, failure to fulfill their  Maintainer responsibilities, violating the Code of Conduct, or other reasons. Inactivity is defined as a period of very low or no activity in the project  for a year or more, with no definite schedule to return to full Maintainer  activity.</p> <p>A Maintainer may be removed at any time by a 2/3 vote of the remaining maintainers.</p> <p>Depending on the reason for removal, a Maintainer may be converted to Emeritus status.  Emeritus Maintainers will still be consulted on some project matters, and can be rapidly returned to Maintainer status if their availability changes.</p>"},{"location":"governance/GOVERNANCE/#members","title":"Members","text":"<p>Members are active contributors who have shown a commitment to the project. They have privileges to review pull requests and are part of the <code>ovn-kubernetes/ovn-kubernetes-members</code> GitHub team, which makes them eligible for automatic PR review assignments. Members are not Maintainers, but they are expected to contribute to the project and collaborate with the team.</p>"},{"location":"governance/GOVERNANCE/#becoming-a-member","title":"Becoming a Member","text":"<p>To become a Member, you need to demonstrate the following: - commitment to the project:   - participate in discussions, contributions, code and documentation reviews     for 3 months or more,   - perform reviews for 5 non-trivial pull requests,   - contribute 10 non-trivial pull requests and have them merged, - ability to write quality code and/or documentation, - ability to collaborate with the team (e.g., participate in project meetings,   join discussion in the CNCF slack channel, etc.), - understanding of how the team works (policies, processes for testing and   code review, etc), - understanding of the project's code base and coding and documentation style.</p> <p>A new Member must be proposed by an existing maintainer by sending a message to the developer mailing list. The application is approved with two affirmative votes from current maintainers.</p>"},{"location":"governance/GOVERNANCE/#removing-a-member","title":"Removing a Member","text":"<p>Members may resign at any time.</p> <p>Members may also be removed after being inactive for a period of 6 months or more, for failure to fulfill their responsibilities, or for violating the Code of Conduct. A Member may be removed at any time by a simple majority vote of the maintainers.</p> <p>Members who are consistently unresponsive to assigned PR reviews may be contacted by Maintainers to discuss their availability and commitment. If the pattern of non-responsiveness continues, the Member may be removed.</p>"},{"location":"governance/GOVERNANCE/#meetings","title":"Meetings","text":"<p>Time zones permitting, Maintainers are expected to participate in the public developer meeting, details of which can be found here.  </p> <p>Maintainers will also have closed meetings in order to discuss security reports or Code of Conduct violations.  Such meetings should be scheduled by any Maintainer on receipt of a security issue or CoC report.  All current Maintainers must be invited to such closed meetings, except for any Maintainer who is accused of a CoC violation.</p>"},{"location":"governance/GOVERNANCE/#code-of-conduct","title":"Code of Conduct","text":"<p>Code of Conduct violations by community members will be discussed and resolved on the private Slack Maintainer channel.</p>"},{"location":"governance/GOVERNANCE/#security-response-team","title":"Security Response Team","text":"<p>The Maintainers will appoint a Security Response Team to handle security reports. This committee may simply consist of the Maintainer Council themselves.  If this responsibility is delegated, the Maintainers will appoint a team of at least two  contributors to handle it.  The Maintainers will review who is assigned to this at least once a year.</p> <p>The Security Response Team is responsible for handling all reports of security holes and breaches according to the security policy.</p>"},{"location":"governance/GOVERNANCE/#voting","title":"Voting","text":"<p>While most business in ovn-kubernetes is conducted by \"lazy consensus\",  periodically the Maintainers may need to vote on specific actions or changes. A vote can be taken on the developer mailing list or the private Maintainer Slack Channel for security or conduct matters. Votes may also be taken at the developer meeting.  Any Maintainer may demand a vote be taken.</p> <p>Most votes require a simple majority of all Maintainers to succeed, except where otherwise noted.  Two-thirds majority votes mean at least two-thirds of all  existing maintainers.</p>"},{"location":"governance/GOVERNANCE/#modifying-this-charter","title":"Modifying this Charter","text":"<p>Changes to this Governance and its supporting documents may be approved by  a 2/3 vote of the Maintainers.</p>"},{"location":"governance/MAINTAINERS/","title":"MAINTAINERS","text":"<p>The current Maintainers Group for the ovn-kubernetes Project consists of:</p> Name Employer Responsibilities Girish Moodalbail NVIDIA All things ovnkube Jaime Caama\u00f1o Ruiz Red Hat All things ovnkube Nadia Pinaeva NVIDIA All things ovnkube Patryk Diak Red Hat All things ovnkube Surya Seetharaman Red Hat All things ovnkube Tim Rozet NVIDIA All things ovnkube <p>See CONTRIBUTING.md for general contribution guidelines. See GOVERNANCE.md for governance guidelines and maintainer responsibilities.</p> <p>Emeritus Maintainers</p> Name Employer Responsibilities Dan Williams Independent All things ovnkube"},{"location":"governance/MEETINGS/","title":"Community Meetings","text":"<p>Meeting information is maintained in the repository root.</p> <p>For meeting times, location, and agenda, see:</p> <p>MEETINGS.md in the OVN-Kubernetes repository.</p>"},{"location":"governance/REVIEWING/","title":"Reviewing Guide","text":"<p>The reviewing guide is maintained in the repository root.</p> <p>For reviewer role, process, and community standards, see:</p> <p>REVIEWING.md in the OVN-Kubernetes repository.</p>"},{"location":"governance/SECURITY/","title":"Security Policy","text":"<p>OVN-Kubernetes repo uses the dependabot which does automatic security updates by scanning the repo and opening PRs to update the effected libraries.</p>"},{"location":"governance/SECURITY/#reporting-a-vulnerability","title":"Reporting a Vulnerability","text":"<p>To report a vulnerability, please use the Private Vulnerability Reporting Feature on GitHub. We will endevour to respond within 48hrs of reporting. If a vulnerability is reported but considered low priority it may be converted into an issue and handled on the public issue tracker. Should a vulnerability be considered severe we will endeavour to patch it within 48hrs of acceptance, and may ask for you to collaborate with us on a temporary private fork of the repository.</p>"},{"location":"installation/INSTALL.KUBEADM/","title":"INSTALL.KUBEADM","text":"<p>The following is a walkthrough for an installation in an environment with 4 virtual machines, and a cluster deployed with <code>kubeadm</code>. This shall serve as a guide for people who are curious enough to deploy OVN-Kubernetes on a manually created cluster and to play around with the components. </p> <p>Note that the resulting environment might be highly unstable.</p> <p>If your goal is to set up an environment quickly or to set up a development environment, see the kind installation documentation instead.</p>"},{"location":"installation/INSTALL.KUBEADM/#environment-setup","title":"Environment setup","text":""},{"location":"installation/INSTALL.KUBEADM/#overview","title":"Overview","text":"<p>The environment consists of 4 libvirt/qemu virtual machines, all deployed with Rocky Linux 8 or CentOS 8. <code>node1</code> will serve as the sole master node and nodes <code>node2</code> and <code>node3</code> as the worker nodes. <code>gw1</code> will be the default gateway for the cluster via the <code>Isolated Network</code>. It will also host an HTTP registry to store the OVN-Kubernetes images.</p> <pre><code>       to hypervisor         to hypervisor         to hypervisor\n             \u2502                     \u2502                     \u2502\n             \u2502                     \u2502                     \u2502\n           \u250c\u2500\u2534\u2500\u2510                 \u250c\u2500\u2534\u2500\u2510                 \u250c\u2500\u2534\u2500\u2510\n           \u2502if1\u2502                 \u2502if1\u2502                 \u2502if1\u2502\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502     node1     \u2502     \u2502     node2     \u2502     \u2502     node3     \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2502               \u2502     \u2502               \u2502     \u2502               \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502if2\u2502                 \u2502if2\u2502                 \u2502if2\u2502\n           \u2514\u2500\u252c\u2500\u2518                 \u2514\u2500\u252c\u2500\u2518                 \u2514\u2500\u252c\u2500\u2518\n             \u2502                     \u2502                     \u2502\n             \u2502                     \u2502                     \u2502\n             \u2502                    xxxxxxxx               \u2502\n             \u2502                 xxx       xxx             \u2502\n             \u2502                xx           xx            \u2502\n             \u2502               x   Isolated   x            \u2502\n             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500x     Network   x\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            xxx            x\n                              xxxxxx  xxxxx\n                                   xxxx\n                                   \u2502\n                                 \u250c\u2500\u2534\u2500\u2510\n                                 \u2502if2\u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502               \u2502\n                           \u2502               \u2502\n                           \u2502      gw1      \u2502\n                           \u2502               \u2502\n                           \u2502               \u2502\n                           \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502if1\u2502\n                                 \u2514\u2500\u252c\u2500\u2518\n                                   \u2502\n                                   \u2502\n                              to hypervisor\n</code></pre> <p>Legend: * if1 - enp1s0 | 192.168.122.0/24 * if2 - enp7s0 | 192.168.123.0/24</p> <p><code>to hypervisor</code> is libvirt's default network with full DHCP. It will be used as management access to all nodes as well as on <code>gw1</code> as the interface for outside connectivity: <pre><code>$ sudo virsh net-dumpxml default\n&lt;network connections='2'&gt;\n  &lt;name&gt;default&lt;/name&gt;\n  &lt;uuid&gt;76b7e8c1-7c2c-456b-ac10-09c98c6275a5&lt;/uuid&gt;\n  &lt;forward mode='nat'&gt;\n    &lt;nat&gt;\n      &lt;port start='1024' end='65535'/&gt;\n    &lt;/nat&gt;\n  &lt;/forward&gt;\n  &lt;bridge name='virbr0' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:4b:4d:f8'/&gt;\n  &lt;ip address='192.168.122.1' netmask='255.255.255.0'&gt;\n    &lt;dhcp&gt;\n      &lt;range start='192.168.122.2' end='192.168.122.254'/&gt;\n    &lt;/dhcp&gt;\n  &lt;/ip&gt;\n&lt;/network&gt;\n</code></pre></p> <p>And <code>Isolated Network</code> is an isolated network. <code>gw1</code> will be the default gateway for this network, and <code>node1</code> through <code>node3</code> will have their default route go through this network: <pre><code>$ sudo virsh net-dumpxml ovn\n&lt;network connections='2'&gt;\n  &lt;name&gt;ovn&lt;/name&gt;\n  &lt;uuid&gt;fecea98b-8b92-438e-a759-f6cfb366614c&lt;/uuid&gt;\n  &lt;bridge name='virbr2' stp='on' delay='0'/&gt;\n  &lt;mac address='52:54:00:d4:f2:cc'/&gt;\n  &lt;domain name='ovn'/&gt;\n&lt;/network&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#gateway-setup-gw1","title":"Gateway setup (gw1)","text":"<p>Deploy the gateway virtual machine first. Set it up as a simple gateway which will NAT everything that comes in on interface enp7s0: <pre><code>IF1=enp1s0\nIF2=enp7s0\nhostnamectl set-hostname gw1\nnmcli conn mod ${IF1} connection.autoconnect yes\nnmcli conn mod ${IF2} ipv4.address 192.168.123.254/24\nnmcli conn mod ${IF2} ipv4.method static\nnmcli conn mod ${IF2} connection.autoconnect yes\nnmcli conn reload\nsystemctl stop firewalld\ncat /proc/sys/net/ipv4/ip_forward\nsysctl -a | grep ip_forward\necho \"net.ipv4.ip_forward=1\" &gt;&gt; /etc/sysctl.d/99-sysctl.conf\nsysctl --system\nyum install iptables-services -y\nyum remove firewalld -y\nsystemctl enable --now iptables\niptables-save\niptables -t nat -I POSTROUTING --src 192.168.123.0/24  -j MASQUERADE\niptables -I FORWARD --j ACCEPT\niptables -I INPUT -p tcp --dport 5000 -j ACCEPT\niptables-save &gt; /etc/sysconfig/iptables\n</code></pre></p> <p>Also set up an HTTP registry: <pre><code>yum install podman -y\nmkdir -p /opt/registry/data\npodman run --name mirror-registry \\\n  -p 5000:5000 -v /opt/registry/data:/var/lib/registry:z      \\\n  -d docker.io/library/registry:2\npodman generate systemd --name mirror-registry &gt; /etc/systemd/system/mirror-registry-container.service\nsystemctl daemon-reload\nsystemctl enable --now mirror-registry-container\n</code></pre></p> <p>Now, reboot the gateway: <pre><code>reboot\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#node1-through-node3-base-setup","title":"node1 through node3 base setup","text":"<p>You must install Open vSwitch on <code>node1</code> through <code>node3</code>. You will then connect <code>enp7s0</code> to an OVS bridge called <code>br-ex</code>. This bridge will be used later by OVN-Kubernetes. Furthermore, you  must assign IP addresses to <code>br-ex</code> and point the nodes' default route via <code>br-ex</code> to <code>gw1</code>. </p>"},{"location":"installation/INSTALL.KUBEADM/#set-hostnames","title":"Set hostnames","text":"<p>Set the hostnames manually, even if they are set correctly by DHCP. Set them manually to: <pre><code>hostnamectl set-hostname node&lt;x&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#disable-swap","title":"Disable swap","text":"<p>Make sure to disable swap. Kubelet will not run otherwise: <pre><code>sed -i '/ swap /d' /etc/fstab\nreboot\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#remove-firewalld","title":"Remove firewalld","text":"<p>Make sure to uninstall firewalld. Otherwise, it will block the kubernetes management ports (that can easily be fixed by configuration) and it will also preempt and block the OVN-Kubernetes installed NAT and FORWARD rules (this is more difficult to remediate). The easiest fix is hence not to use firewalld at all: <pre><code>systemctl disable --now firewalld\nyum remove -y firewalld\n</code></pre></p> <p>For more details, see https://gitmemory.com/issue/firewalld/firewalld/767/790687269; this is about Calico, but it highlights the same issue.</p>"},{"location":"installation/INSTALL.KUBEADM/#install-open-vswitch","title":"Install Open vSwitch","text":"<p>Install Open vSwitch from https://wiki.centos.org/SpecialInterestGroup/NFV</p>"},{"location":"installation/INSTALL.KUBEADM/#on-centos","title":"On CentOS:","text":"<pre><code>yum install centos-release-nfv-openvswitch -y\nyum install openvswitch2.13 --nobest -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre>"},{"location":"installation/INSTALL.KUBEADM/#on-rocky-linux","title":"On Rocky Linux","text":"<p>Rocky doesn't have access to CentOS's repositories. However, you can still use the CentOS NFV repositories: <pre><code>rpm -ivh http://mirror.centos.org/centos/8-stream/extras/x86_64/os/Packages/centos-release-nfv-common-1-3.el8.noarch.rpm --nodeps\nrpm -ivh http://mirror.centos.org/centos/8-stream/extras/x86_64/os/Packages/centos-release-nfv-openvswitch-1-3.el8.noarch.rpm\nyum install openvswitch2.13 --nobest -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre></p> <p>Alternatively, on Rocky Linux, you can also build your own RPMs directly from the SRPMs, e.g.: <pre><code>yum install '@Development Tools'\nyum install desktop-file-utils libcap-ng-devel libmnl-devel numactl-devel openssl-devel python3-devel python3-pyOpenSSL python3-setuptools python3-sphinx rdma-core-devel unbound-devel -y\nrpmbuild --rebuild  http://ftp.redhat.com/pub/redhat/linux/enterprise/8Base/en/Fast-Datapath/SRPMS/openvswitch2.13-2.13.0-79.el8fdp.src.rpm\nyum install selinux-policy-devel -y\nrpmbuild --rebuild http://ftp.redhat.com/pub/redhat/linux/enterprise/8Base/en/Fast-Datapath/SRPMS/openvswitch-selinux-extra-policy-1.0-28.el8fdp.src.rpm\nyum localinstall /root/rpmbuild/RPMS/noarch/openvswitch-selinux-extra-policy-1.0-28.el8.noarch.rpm /root/rpmbuild/RPMS/x86_64/openvswitch2.13-2.13.0-79.el8.x86_64.rpm -y\nyum install NetworkManager-ovs.x86_64 -y\nsystemctl enable --now openvswitch\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#configure-networking","title":"Configure networking","text":"<p>Set up networking: <pre><code>BRIDGE_NAME=br-ex\nIF1=enp1s0\nIF2=enp7s0\nIP_ADDRESS=\"192.168.123.$(hostname | sed 's/node//')/24\"\n</code></pre></p> <p>Verify the <code>IP_ADDRESS</code> - it should be unique for every node and the last octet should be the same as the node's numeric identifier: <pre><code>echo $IP_ADDRESS\n</code></pre></p> <p>Then, continue: <pre><code>nmcli c add type ovs-bridge conn.interface ${BRIDGE_NAME} con-name ${BRIDGE_NAME}\nnmcli c add type ovs-port conn.interface ${BRIDGE_NAME} master ${BRIDGE_NAME} con-name ovs-port-${BRIDGE_NAME}\nnmcli c add type ovs-interface slave-type ovs-port conn.interface ${BRIDGE_NAME} master ovs-port-${BRIDGE_NAME}  con-name ovs-if-${BRIDGE_NAME}\nnmcli c add type ovs-port conn.interface ${IF2} master ${BRIDGE_NAME} con-name ovs-port-${IF2}\nnmcli c add type ethernet conn.interface ${IF2} master ovs-port-${IF2} con-name ovs-if-${IF2}\nnmcli conn delete ${IF2}\nnmcli conn mod ${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${IF2} connection.autoconnect yes\nnmcli conn mod ovs-port-${IF2} connection.autoconnect yes\nnmcli conn mod ovs-port-${BRIDGE_NAME} connection.autoconnect yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.address ${IP_ADDRESS}\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.method static\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.route-metric 50\n\n# move the default route to br-ex\nBRIDGE_NAME=br-ex\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.gateway \"192.168.123.254\"\nnmcli conn mod ${IF1} ipv4.never-default yes\n# Change DNS to 8.8.8.8\nnmcli conn mod ${IF1} ipv4.ignore-auto-dns yes\nnmcli conn mod ovs-if-${BRIDGE_NAME} ipv4.dns \"8.8.8.8\"\n</code></pre></p> <p>Now, reboot the node: <pre><code>reboot\n</code></pre></p> <p>After the reboot, you should see something like this, for example on node1: <pre><code>[root@node1 ~]# cat /etc/resolv.conf \n# Generated by NetworkManager\nnameserver 8.8.8.8\n[root@node1 ~]# ovs-vsctl show\nc1aee179-b425-4b48-8648-dd8746f59add\n    Bridge br-ex\n        Port enp7s0\n            Interface enp7s0\n                type: system\n        Port br-ex\n            Interface br-ex\n                type: internal\n    ovs_version: \"2.13.4\"\n[root@node1 ~]# ip r\ndefault via 192.168.123.254 dev br-ex proto static metric 800 \n192.168.122.0/24 dev enp1s0 proto kernel scope link src 192.168.122.205 metric 100 \n192.168.123.0/24 dev br-ex proto kernel scope link src 192.168.123.1 metric 800 \n[root@node1 ~]# ip a ls dev br-ex\n6: br-ex: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/ether 26:98:69:4a:d7:43 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.123.1/24 brd 192.168.123.255 scope global noprefixroute br-ex\n       valid_lft forever preferred_lft forever\n    inet6 fe80::4a1d:4d35:7c28:1ff2/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[root@node1 ~]# nmcli conn\nNAME             UUID                                  TYPE           DEVICE \novs-if-br-ex     d434980e-ea23-4ab4-8414-289b7af44c50  ovs-interface  br-ex  \nenp1s0           52060cdd-913e-4df8-9e9e-776f31647323  ethernet       enp1s0 \nbr-ex            950f405f-cd5c-4d51-b2ab-3d8e1e938c8b  ovs-bridge     br-ex  \novs-if-enp7s0    0279d1c9-212c-4be8-8dfe-88a7b0b6d623  ethernet       enp7s0 \novs-port-br-ex   3b47e5ae-a27a-4522-bea5-1fbf9c8c08eb  ovs-port       br-ex  \novs-port-enp7s0  1baea5a3-09ee-4972-8f6b-bb8195ae46c4  ovs-port       enp7s0 \n</code></pre></p> <p>And you should be able to ping outside of the cluster: <pre><code>[root@node1 ~]# ping -c1 -W1 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=112 time=18.5 ms\n\n--- 8.8.8.8 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 18.506/18.506/18.506/0.000 ms\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#install-container-runtime-engine-and-kubeadm-node1-node2-node3","title":"Install container runtime engine and kubeadm (node1, node2, node3)","text":"<p>The following will be a brief walkthrough of what's requried to install the container runtime and kubernetes. For further details, follow the <code>kubeadm</code> documentation: * https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ * https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</p>"},{"location":"installation/INSTALL.KUBEADM/#install-the-container-runtime","title":"Install the container runtime","text":"<p>See https://kubernetes.io/docs/setup/production-environment/container-runtimes/ for further details.</p> <p>Set up iptables: <pre><code># Create the .conf file to load the modules at bootup\ncat &lt;&lt;EOF | sudo tee /etc/modules-load.d/crio.conf\noverlay\nbr_netfilter\nEOF\n\nsudo modprobe overlay\nsudo modprobe br_netfilter\n\n# Set up required sysctl params, these persist across reboots.\ncat &lt;&lt;EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\n\nsudo sysctl --system\n</code></pre></p> <p>Then, install cri-o. At time of this writing, the latest version was 1.21: <pre><code>OS=CentOS_8\nVERSION=1.21\ncurl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/devel:kubic:libcontainers:stable.repo\ncurl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/devel:kubic:libcontainers:stable:cri-o:$VERSION.repo\nyum install cri-o -y\n</code></pre></p> <p>Make sure to set 192.168.123.254 (gw1) as an insecure registry: <pre><code>cat &lt;&lt;'EOF' | tee /etc/containers/registries.conf.d/999-insecure.conf\n[[registry]]\nlocation = \"192.168.123.254:5000\"\ninsecure = true\nEOF\n</code></pre></p> <p>Also, make sure to remove <code>/etc/cni/net.d/100-crio-bridge.conf</code> as we do not want to fall back to crio's default networking: <pre><code>mv /etc/cni/net.d/100-crio-bridge.conf /root/.\n</code></pre></p> <p>Note: If you forget to move or delete this file, your CoreDNS pods will come up with an IP address in the 10.0.0.0/8 range.</p> <p>Finally, start crio: <pre><code>systemctl daemon-reload\nsystemctl enable crio --now\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#install-kubelet-kubectl-kubeadm","title":"Install kubelet, kubectl, kubeadm","text":"<p>See https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl for further details.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nexclude=kubelet kubeadm kubectl\nEOF\n\n# Set SELinux in permissive mode (effectively disabling it)\nsudo setenforce 0\nsudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\nsudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\n\nsudo systemctl enable --now kubelet\n</code></pre>"},{"location":"installation/INSTALL.KUBEADM/#deploying-a-cluster-with-ovn-kubernetes","title":"Deploying a cluster with OVN-Kubernetes","text":"<p>Execute the following instructions only on the master node, <code>node1</code>.</p>"},{"location":"installation/INSTALL.KUBEADM/#install-instructions-for-kubeadm","title":"Install instructions for kubeadm","text":"<p>Deploy on the master node <code>node1</code>: <pre><code>kubeadm init --pod-network-cidr 172.16.0.0/16 --service-cidr 172.17.0.0/16 --apiserver-advertise-address 192.168.123.1\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/ovn.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre></p> <p>Write down the join command for worker nodes - you will need it later.</p> <p>You will now have a one node cluster without a CNI plugin and as such the CoreDNS pods will not start: <pre><code>[root@node1 ~]# kubectl get pods -o wide -A\nNAMESPACE     NAME                            READY   STATUS              RESTARTS   AGE   IP                NODE    NOMINATED NODE   READINESS GATES\nkube-system   coredns-78fcd69978-dvpjg        0/1     ContainerCreating   0          21s   &lt;none&gt;            node1   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-78fcd69978-mzpzr        0/1     ContainerCreating   0          21s   &lt;none&gt;            node1   &lt;none&gt;           &lt;none&gt;\nkube-system   etcd-node1                      1/1     Running             2          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-apiserver-node1            1/1     Running             2          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-controller-manager-node1   1/1     Running             3          33s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-vm44k                1/1     Running             0          22s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-scheduler-node1            1/1     Running             3          28s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Now, deploy OVN-Kubernetes - see below.</p>"},{"location":"installation/INSTALL.KUBEADM/#deploying-ovn-kubernetes-on-node1","title":"Deploying OVN-Kubernetes on node1","text":"<p>Install build dependencies and create a softlink for <code>pip</code> to <code>pip3</code>: <pre><code>yum install git python3-pip make podman buildah -y\nln -s $(which pip3) /usr/local/bin/pip\n</code></pre></p> <p>Install golang, for further details see https://golang.org/doc/install: <pre><code>curl -L -O https://golang.org/dl/go1.17.linux-amd64.tar.gz\nrm -rf /usr/local/go &amp;&amp; tar -C /usr/local -xzf go1.17.linux-amd64.tar.gz\necho \"export PATH=$PATH:/usr/local/go/bin\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\ngo version\n</code></pre></p> <p>Now, clone the OVN-Kubernetes repository: <pre><code>mkdir -p $HOME/work/src/github.com/ovn-org\ncd $HOME/work/src/github.com/ovn-org\ngit clone https://github.com/ovn-org/ovn-kubernetes\ncd $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/images\n</code></pre></p> <p>Build the latest ovn-daemonset image and push it to the registry. Prepare the binaries: <pre><code># Build ovn docker image\npushd ../../go-controller\nmake\npopd\n\n# Build ovn kube image\n# Find all built executables, but ignore the 'windows' directory if it exists\nfind ../../go-controller/_output/go/bin/ -maxdepth 1 -type f -exec cp -f {} . \\;\necho \"ref: $(git rev-parse  --symbolic-full-name HEAD)  commit: $(git rev-parse  HEAD)\" &gt; git_info\n</code></pre></p> <p>Now, build and push the image with: <pre><code>OVN_IMAGE=192.168.123.254:5000/ovn-daemonset-fedora:latest\nbuildah bud -t $OVN_IMAGE -f Dockerfile.fedora .\npodman push $OVN_IMAGE\n</code></pre></p> <p>Next, run: <pre><code>OVN_IMAGE=192.168.123.254:5000/ovn-daemonset-fedora:latest\nMASTER_IP=192.168.123.1\nNET_CIDR=\"172.16.0.0/16/24\"\nSVC_CIDR=\"172.17.0.0/16\"\n./daemonset.sh --image=${OVN_IMAGE} \\\n    --net-cidr=\"${NET_CIDR}\" --svc-cidr=\"${SVC_CIDR}\" \\\n    --gateway-mode=\"local\" \\\n    --k8s-apiserver=https://${MASTER_IP}:6443\n</code></pre></p> <p>You might also have to work around an issue where br-int is added by OVN, but the necessary files in /var/run/openvswitch are not created until Open vSwitch is restarted - see here for more details. This only happens on the master, so let's pre-create <code>br-int</code> there: <pre><code>ovs-vsctl add-br br-int\n</code></pre></p> <p>Now, set up ovnkube: <pre><code># set up the namespace\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovn-setup.yaml\n# set up the database pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-db.yaml\n# set up the master pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-master.yaml\n# set up the ovnkube-node pods - wait until the pods are up and running before progressing to the next command:\nkubectl apply -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-node.yaml\n</code></pre></p> <p>Once all OVN related pods are up, you should see that the CoreDNS pods have started as well and they should be in the correct network. <pre><code>[root@node1 images]# kubectl get pods -A -o wide | grep coredns\nkube-system      coredns-78fcd69978-ms969         1/1     Running   0          29s     172.16.0.6        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      coredns-78fcd69978-w6k2z         1/1     Running   0          36s     172.16.0.5        node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>Finally, delete the kube-proxy DaemonSet: <pre><code>kubectl delete ds -n kube-system kube-proxy\n</code></pre></p> <p>You should now see the following when listing all pods: <pre><code>[root@node1 ~]# kubectl get pods -A -o wide\nNAMESPACE        NAME                             READY   STATUS    RESTARTS   AGE     IP                NODE    NOMINATED NODE   READINESS GATES\nkube-system      coredns-78fcd69978-rhjgh         1/1     Running   0          10s     172.16.0.4        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      coredns-78fcd69978-xcxnx         1/1     Running   0          17s     172.16.0.3        node1   &lt;none&gt;           &lt;none&gt;\nkube-system      etcd-node1                       1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-apiserver-node1             1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-controller-manager-node1    1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\nkube-system      kube-scheduler-node1             1/1     Running   1          74m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-db-7767c6b7c5-25drn      2/2     Running   2          11m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-master-775d45fd5-mzkcb   3/3     Running   3          10m     192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\novn-kubernetes   ovnkube-node-xmgrj               3/3     Running   3          8m49s   192.168.122.205   node1   &lt;none&gt;           &lt;none&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#verifying-the-deployment","title":"Verifying the deployment","text":"<p>Create a test deployment to make sure that everything works as expected: <pre><code>cd ~\ncat &lt;&lt;'EOF' &gt; fedora.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fedora-deployment\n  labels:\n    app: fedora-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: fedora-pod\n  template:\n    metadata:\n      labels:\n        app: fedora-pod\n    spec:\n      tolerations:\n      - key: \"node-role.kubernetes.io/control-plane\"\n        operator: \"Exists\"\n      containers:\n      - name: fedora\n        image: fedora\n        command:\n          - sleep\n          - infinity\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n          capabilities:\n            add:\n              - \"SETFCAP\"\n              - \"CAP_NET_RAW\"\n              - \"CAP_NET_ADMIN\"\nEOF\nkubectl apply -f fedora.yaml\n</code></pre></p> <p>Make sure that the pods have a correct IP address and that they can reach the outside world, e.g. by installing some software: <pre><code>[root@node1 ~]# kubectl get pods -o wide\nNAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES\nfedora-deployment-86f7647bd6-dllbs   1/1     Running   0          58s   172.16.0.5   node1   &lt;none&gt;           &lt;none&gt;\nfedora-deployment-86f7647bd6-k42wm   1/1     Running   0          36s   172.16.0.6   node1   &lt;none&gt;           &lt;none&gt;\n[root@node1 ~]# kubectl exec -it fedora-deployment-86f7647bd6-dllbs -- /bin/bash\n[root@fedora-deployment-86f7647bd6-dllbs /]# yum install iputils -y\nFedora 34 - x86_64                                                                   4.2 MB/s |  74 MB     00:17    \nFedora 34 openh264 (From Cisco) - x86_64                                             1.7 kB/s | 2.5 kB     00:01    \nFedora Modular 34 - x86_64                                                           2.8 MB/s | 4.9 MB     00:01    \nFedora 34 - x86_64 - Updates                                                         3.7 MB/s |  25 MB     00:06    \nFedora Modular 34 - x86_64 - Updates                                                 2.0 MB/s | 4.6 MB     00:02    \nLast metadata expiration check: 0:00:01 ago on Tue Aug 24 17:04:04 2021.\nDependencies resolved.\n=====================================================================================================================\n Package                   Architecture             Version                           Repository                Size\n=====================================================================================================================\nInstalling:\n iputils                   x86_64                   20210202-2.fc34                   fedora                   170 k\n\nTransaction Summary\n=====================================================================================================================\nInstall  1 Package\n\nTotal download size: 170 k\nInstalled size: 527 k\nDownloading Packages:\niputils-20210202-2.fc34.x86_64.rpm                                                   1.2 MB/s | 170 kB     00:00    \n---------------------------------------------------------------------------------------------------------------------\nTotal                                                                                265 kB/s | 170 kB     00:00     \nRunning transaction check\nTransaction check succeeded.\nRunning transaction test\nTransaction test succeeded.\nRunning transaction\n  Preparing        :                                                                                             1/1 \n  Installing       : iputils-20210202-2.fc34.x86_64                                                              1/1 \n  Running scriptlet: iputils-20210202-2.fc34.x86_64                                                              1/1 \n  Verifying        : iputils-20210202-2.fc34.x86_64                                                              1/1 \n\nInstalled:\n  iputils-20210202-2.fc34.x86_64                                                                                     \n\nComplete!\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#uninstalling-ovn-kubernetes","title":"Uninstalling OVN-Kubernetes","text":"<p>In order to uninstall OVN kubernetes: <pre><code>kubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-node.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-master.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovnkube-db.yaml\nkubectl delete -f $HOME/work/src/github.com/ovn-org/ovn-kubernetes/dist/yaml/ovn-setup.yaml\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#issues-workarounds","title":"Issues / workarounds:","text":"<p>br-int might be added by OVN, but the files for it are not created in /var/run/openvswitch. <code>ovs-ofctl dump-flows br-int</code> fails, and one will see the following log messages among others: <pre><code>2021-08-24T12:42:43.810Z|00025|rconn|WARN|unix:/var/run/openvswitch/br-int.mgmt: connection failed (No such file or directory)\n</code></pre></p> <p>The best workaroud is to pre-create br-int before the OVN-Kubernetes installation: <pre><code>ovs-vsctl add-br br-int\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#joining-worker-nodes-to-the-environment","title":"Joining worker nodes to the environment","text":"<p>Finally, join your worker nodes. Set them up using the base setup steps for the nodes and the CRI and kubeadm installation steps. Then, use the output from the <code>kubeadm init</code> command that you ran earlier to join the node to the cluster: <pre><code>kubeadm join 192.168.123.10:6443 --token &lt;...&gt; \\\n    --discovery-token-ca-cert-hash &lt;...&gt;\n</code></pre></p>"},{"location":"installation/INSTALL.KUBEADM/#kubeadm-reset-instructions","title":"kubeadm reset instructions","text":"<p>If you must reset your master and worker nodes, the following commands can be used to reset the lab environment. Run this on each node and then ideally reboot the node right after: <pre><code>IF2=enp7s0\necho \"y\" | kubeadm reset\nrm -f /etc/cni/net.d/10-*\nrm -Rf ~/.kube\nrm -f /etc/openvswitch/conf.db\nnmcli conn del cni0\nsystemctl restart openvswitch\nsystemctl restart NetworkManager\nnmcli conn up ovs-if-${IF2}\n</code></pre></p>"},{"location":"installation/INSTALL.OPENSHIFT/","title":"OVN overlay network on Openshift","text":"<p>This section describes how an OVN overlay network is setup on Openshift 4.10 and later. It explains the various components and how they come together to establish the OVN overlay network. People that are interested in understanding how the ovn cni plugin is installed will find this useful.</p> <p>As of OCP-4.10, the Openshift overlay network is managed using daemonsets. The ovs/ovn components are built into a Docker image that has all of the needed packages. Daemonsets are deployed on the nodes in the cluster to provide the network. The daemonsets use a common Docker image when creating the needed pods.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#overview","title":"Overview","text":"<p>Ovn has a master service that runs on a compute node in the cluster and node controllers that run on all nodes in the cluster. The master service runs ovn north and ovn south daemons that work with the ovs north and south databases. All of the nodes run the ovn controller which accesses the ovn north and south databases.</p> <p>There are two daemonsets, ovnkube-master.yaml and ovnkube.yaml that mangage the parts of the ovn architecture. The ovnkube-master daemonset runs both master and node services, ovnkube daemonset just runs the node service. The daemonset uses selector labels in the cluster's nodes to specify where the daemonsets run.</p> <p>Both daemonsets use the same docker image. Configuration is passed from the daemonset to the image using environment variables and mounting directories from the host. The environment variables are set from a configmap that is created during installation. The image includes an control script that is the entrypoint for the container, ovnkube and the openvswitch and ovn rpms.</p> <p>The ovnkube program (in this repository) provides the interface between Openshift and Ovn. The arguments used when starting it determine its role in the configuration which are master or node.</p> <p>The ovn cni service is configured and installed after Openshift is installed and running. Openshift comes up with no network support. One of many networking choices, including ovn, is selected, installed, configured and started to complete Openshift installation.</p> <p>The installer creates a configmap that contains the configuration information. The two daemonsets use the configuration information.</p> <p>The configmap is mounted in the daemonsets and is used to initialize environment variables that are passed to the Docker image running in the pod.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#daemonset","title":"Daemonset","text":"<p>The daemonsets govern where the components run, what is run, and how they interface with other components. The ovnkube-master.yaml and ovnkube.yaml files bring together the needed pieces and set up the context for running the pods. Here is some of how it is done.</p> <p>The nodes are chosen based on labels:</p> <p><pre><code>spec:\n  selector:\n    matchLabels:\n      node-role.kubernetes.io/compute: \"true\"\n             or\n      node-role.kubernetes.io/control-plane: \"true\"\n  template:\n    spec:\n      nodeSelector:\n        node-role.kubernetes.io/compute: \"true\"\n             or\n        node-role.kubernetes.io/control-plane: \"true\"\n</code></pre> The daemonset will start a pod on every nodes that has the desired labels.</p> <p>NOTE: The above labels are openshift labels created when openshift is installed. As this evolves ovn specific labels will be used.</p> <p>The docker image is configured as follows: <pre><code>spec:\n  template:\n    spec:\n      containers:\n      - name: ovnkube-master\n        image: \"netdev22:5000/ovn-kube:latest\"\n</code></pre> The image is in a docker repository that all nodes can access.</p> <p>NOTE: The above uses a local development docker repository, netdev22:5000</p> <p>The openshift service account is created during installation and configured here: <pre><code>spec:\n  template:\n    spec:\n      serviceAccountName: ovn\n</code></pre></p> <p>The daemonsets use host networking and provide the cni plugin when they startup. Docker uses the ovn-cni-plugin to access networking. Docker must be able to access the cni plugin that is in the image. To do this the host /opt/cni/bin directory is mounted in the pod and the pod startup script copies the plugin to the host. Since the host operating environment and the pod are different, the installed plugin is just a shim that passes requests to the server that is running in the pod.</p> <pre><code>spec:\n  template:\n    spec:\n      hostNetwork: true\n      hostPID: true\n      containers:\n        volumeMounts:\n        - mountPath: /host/opt/cni/bin\n          name: host-opt-cni-bin\n        - mountPath: /etc/cni/net.d\n          name: host-etc-cni-netd\n        - mountPath: /var/lib/cni/networks/ovn-k8s-cni-overlay\n          name: host-var-lib-cni-networks-openshift-ovn\n      volumes:\n      - name: host-opt-cni-bin\n        hostPath:\n          path: /opt/cni/bin\n      - name: host-etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n      - name: host-var-lib-cni-networks-openshift-ovn\n        hostPath:\n          path: /var/lib/cni/networks/ovn-k8s-cni-overlay\n</code></pre>"},{"location":"installation/INSTALL.OPENSHIFT/#docker-image","title":"Docker Image","text":"<p>The Dockerfile directs construction of the image. It installs a base OS, rpms, and local scripts into the image. It also sets up directories in the image and copies files into them.  In particular it copies in the entrypoint script, ovnkube.sh.</p> <p><pre><code># docker build -t ovn-kube .\n# docker tag ovn-kube netdev22:5000/ovn-kube:latest\n# docker push netdev22:5000/ovn-kube:latest\n</code></pre> The image is tagged and pushed to a docker repository. This example uses the local development docker repo, netdev22:5000</p> <p>NOTE: At present go_controller is built and the resultant files are copied to the docker build directory. In the future, these files will be installed from openvswitch-ovn-kubernetes rpm.</p>"},{"location":"installation/INSTALL.OPENSHIFT/#configuration","title":"Configuration","text":"<p>There are three sources of configuration information in the daemonsets:</p> <ol> <li>/etc/origin/node/ files</li> </ol> <p>These files are installed by the Openshift install and are in the node daemonset which mounts them on the host. The ovn deamonsets mount them in the pod.</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        volumeMounts:\n        # Directory which contains the host configuration.\n        - mountPath: /etc/origin/node/\n          name: host-config\n          readOnly: true\n        - mountPath: /etc/sysconfig/origin-node\n          name: host-sysconfig-node\n          readOnly: true\n      volumes:\n      - name: host-config\n        hostPath:\n          path: /etc/origin/node\n      - name: host-sysconfig-node\n        hostPath:\n          path: /etc/sysconfig/origin-node\n</code></pre> <ol> <li>/etc/openvswitch/ovn_k8s.conf</li> </ol> <p>This file is used by ovn daemons and ovnkube to set needed values. This file is build into the image. It holds configuration constants.</p> <ol> <li>ovn-config configmap</li> </ol> <p>This configmap is created after Openshift is installed and running and before the network is installed. It contains information that is specific to the cluster that is needed for ovn to access the cluster apiserver.</p> <p><pre><code># oc get configmap ovn-config -o yaml\napiVersion: v1\ndata:\n  OvnNorth: tcp:10.19.188.22:6641\n  OvnSouth: tcp:10.19.188.22:6642\n  k8s_apiserver: https://wsfd-netdev22.ntdv.lab.eng.bos.redhat.com:8443\n  net_cidr: 10.128.0.0/14\n  svc_cidr: 172.30.0.0/16\nkind: ConfigMap\n</code></pre> OvnNorth and OvnSouth are used by ovn to access its daemons. They are host IP address of the daemons.</p> <p>k8s_apiserver allows access to Openshift's api server.</p> <p>net_cidr and svc_cidr are the configuration configuration cidrs</p> <p>The configmap keys become environment variable values in the daemonset yaml files.</p> <pre><code>spec:\n  template:\n    spec:\n      containers:\n        env:\n        - name: OVN_NORTH\n          valueFrom:\n            configMapKeyRef:\n              name: ovn-config\n              key: OvnNorth\n</code></pre>"},{"location":"installation/INSTALL.SSL/","title":"INSTALL.SSL","text":"<p>This document explains the way one could use SSL for connectivity between OVN components.  The details in this documentation needs a minimum version of Open vSwitch 2.7.</p>"},{"location":"installation/INSTALL.SSL/#generating-certificates","title":"Generating certificates","text":"<p>Detailed explanation of how OVS utilites and daemons use SSL certificates is explained at OVS.SSL.  The following section summarizes it for ovn-kubernetes.</p>"},{"location":"installation/INSTALL.SSL/#create-a-certificate-authority","title":"Create a certificate authority.","text":"<p>On a secure machine (separate from your k8s cluster), where you have installed the ovs-pki utility (comes with OVS installations), create a certificate authority by running</p> <pre><code>ovs-pki init --force\n</code></pre> <p>The above command creates 2 certificate authorities.  But we are concerned only with one of them, i.e the \"switch\" certificate authority.  We will use this certificate authority to sign individual certificates of all the nodes.  We will then use the same certificate authority's certificate to verify a node's connections to the master.</p> <p>Copy this certificate to the master and each of the nodes. $CENTRAL_IP is the IP address of the master.</p> <pre><code>scp /var/lib/openvswitch/pki/switchca/cacert.pem \\\n    root@$CENTRAL_IP:/etc/openvswitch/.\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-components-running-on-the-master","title":"Generate signed certificates for OVN components running on the master.","text":""},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-nb-database","title":"Generate signed certificates for OVN NB Database","text":"<p>On the master, run the following commands.</p> <pre><code>cd /etc/openvswitch\novs-pki req ovnnb\n</code></pre> <p>The above command will generate a private key \"ovnnb-privkey.pem\" and a corresponding certificate request named \"ovnnb-req.pem\". Copy over the ovnnb-req.pem to the aforementioned secure machine's /var/lib/openvswitch/pki folder to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnnb\n</code></pre> <p>The above command will generate ovnnb-cert.pem. Copy over this file back to the master's /etc/openvswitch. The ovnnb-privkey.pem and ovnnb-cert.pem will be used by the ovsdb-server that fronts the OVN NB database.</p> <p>Now run the following commands to ask ovsdb-server to use these certificates and also to open up SSL ports via which the database can be accessed.</p> <pre><code>ovn-nbctl set-ssl /etc/openvswitch/ovnnb-privkey.pem \\\n    /etc/openvswitch/ovnnb-cert.pem  /etc/openvswitch/cacert.pem\n\novn-nbctl set-connection pssl:6641\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-sb-database","title":"Generate signed certificates for OVN SB Database","text":"<pre><code>cd /etc/openvswitch\novs-pki req ovnsb\n</code></pre> <p>The above command will generate a private key \"ovnsb-privkey.pem\" and a corresponding certificate request named \"ovnsb-req.pem\". Copy over the ovnsb-req.pem to the aforementioned secure machine's /var/lib/openvswitch/pki to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnsb\n</code></pre> <p>The above command will generate ovnsb-cert.pem. Copy over this file back to the master's /etc/openvswitch. The ovnsb-privkey.pem and ovnsb-cert.pem will be used by the ovsdb-server that fronts the OVN SB database.</p> <p>Now run the following commands to ask ovsdb-server to use these certificates  and also to open up SSL ports via which the database can be accessed.</p> <pre><code>ovn-sbctl set-ssl /etc/openvswitch/ovnsb-privkey.pem \\\n    /etc/openvswitch/ovnsb-cert.pem  /etc/openvswitch/cacert.pem\n\novn-sbctl set-connection pssl:6642\n</code></pre>"},{"location":"installation/INSTALL.SSL/#generate-signed-certificates-for-ovn-northd","title":"Generate signed certificates for OVN Northd","text":"<p>If you are running ovn-northd on the same host as the OVN NB and SB database servers, then there is no need to secure the communication between ovn-northd and OVN NB/SB daemons. ovn-northd will communicate using UNIX path.</p> <p>In case you still want to secure the communication, or the daemons are running on separate hosts, then follow the instructions on this page [OVN-NORTHD.SSL.md]</p>"},{"location":"installation/INSTALL.SSL/#generate-certificates-for-the-nodes","title":"Generate certificates for the nodes","text":"<p>On each node, create a certificate request.</p> <pre><code>cd /etc/openvswitch\novs-pki req ovncontroller\n</code></pre> <p>The above command will create a new private key for the node called ovncontroller-privkey.pem and a certificate request file called ovncontroller-req.pem.  Copy this certificate request file to the secure machine where you created the certificate authority and from the directory where the copied file exists, run:</p> <pre><code>ovs-pki -b sign ovncontroller switch\n</code></pre> <p>The above will create the certificate for the node called \"ovncontroller-cert.pem\". You should copy this certificate back to the node's /etc/openvswitch directory.</p> <p>Additionally, the common name used for signing the certificates (<code>ovncontroller</code>) needs to be passed in for TLS server certificate verification using the  <code>-nb-cert-common-name</code> and the <code>-sb-cert-common-name</code> CLI options.</p>"},{"location":"installation/INSTALL.SSL/#one-time-setup","title":"One time setup.","text":"<p>As explained here, OVN architecture has a central component which stores your networking intent in a database.  You start this central component on the node where you intend to start your k8s central daemons by running:</p> <pre><code>/usr/share/openvswitch/scripts/ovn-ctl start_northd\n</code></pre> <p>You should now restart the ovn-controller on each host with the following additional options.</p> <pre><code>/usr/share/openvswitch/scripts/ovn-ctl \\\n    --ovn-controller-ssl-key=\"/etc/openvswitch/ovncontroller-privkey.pem\"  \\\n    --ovn-controller-ssl-cert=\"/etc/openvswitch/ovncontroller-cert.pem\"    \\\n    --ovn-controller-ssl-ca-cert=\"/etc/openvswitch/cacert.pem\" \\\n    restart_controller\n</code></pre> <p>To make sure that ovn-controller restarts with the above options during system reboots, you should add the above options to your startup script's defaults file.  For e.g. on Ubuntu, if you installed ovn-controller via the package 'ovn-host*.deb', write the following to your /etc/default/ovn-host file</p> <p><pre><code>OVN_CTL_OPTS=\"--ovn-controller-ssl-key=/etc/openvswitch/ovncontroller-privkey.pem  --ovn-controller-ssl-cert=/etc/openvswitch/ovncontroller-cert.pem --ovn-controller-ssl-ca-cert=/etc/openvswitch/cacert.pem\"\n</code></pre> If you start the ovnkube utility on master with \"--init-master\" or with \"--init-network-control-manager\", you should pass the SSL certificates to it. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \\\n -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n -logfile=\"/var/log/ovn-kubernetes/ovnkube.log\" \\\n -init-master=\"$NODE_NAME\" -cluster-subnets=$CLUSTER_IP_SUBNET \\\n -k8s-service-cidr=$SERVICE_IP_SUBNET \\\n -nodeport \\\n -nb-address=\"ssl:$CENTRAL_IP:6641\" \\\n -sb-address=\"ssl:$CENTRAL_IP:6642\" \\\n -nb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n -nb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n -nb-client-cacert /etc/openvswitch/cacert.pem \\\n -nb-cert-common-name ovncontroller \\\n -sb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n -sb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n -sb-client-cacert /etc/openvswitch/cacert.pem \\\n -sb-cert-common-name ovncontroller\n</code></pre> <p>If you start the ovnkube utility with \"--init-cluster-manager\", there is no need to pass the SSL certificates to it as it doesn't connect to the OVN databases. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig kubeconfig.yaml -loglevel=4 \\\n -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n -logfile=\"/var/log/ovn-kubernetes/ovnkube-cluster-manager.log\" \\\n -init-cluster-manager=\"$NODE_NAME\" -cluster-subnets=$CLUSTER_IP_SUBNET \\\n -k8s-service-cidr=$SERVICE_IP_SUBNET\n</code></pre> <p>And when you start your ovnkube utility on nodes, you should pass the SSL certificates to it. For e.g:</p> <pre><code>sudo ovnkube -k8s-kubeconfig $HOME/kubeconfig.yaml -loglevel=4 \\\n    -k8s-apiserver=\"http://$CENTRAL_IP:8080\" \\\n    -init-node=\"$NODE_NAME\"  \\\n    -nodeport \\\n    -nb-address=\"ssl:$CENTRAL_IP:6641\" \\\n    -sb-address=\"ssl:$CENTRAL_IP:6642\" -k8s-token=$TOKEN \\\n    -nb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n    -nb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n    -nb-client-cacert /etc/openvswitch/cacert.pem \\\n    -nb-cert-common-name ovncontroller \\\n    -sb-client-privkey /etc/openvswitch/ovncontroller-privkey.pem \\\n    -sb-client-cert /etc/openvswitch/ovncontroller-cert.pem \\\n    -sb-client-cacert /etc/openvswitch/cacert.pem \\\n    -sb-cert-common-name ovncontroller \\\n    -init-gateways \\\n    -k8s-service-cidr=$SERVICE_IP_SUBNET \\\n    -cluster-subnets=$CLUSTER_IP_SUBNET\n</code></pre>"},{"location":"installation/INSTALL.UBUNTU/","title":"Installing OVS and OVN on Ubuntu","text":""},{"location":"installation/INSTALL.UBUNTU/#installing-ovs-and-ovn-from-packages","title":"Installing OVS and OVN from packages","text":"<p>To install OVS bits on all nodes, run:</p> <pre><code>sudo apt-get install python-six openssl -y\n\nsudo apt-get install openvswitch-switch openvswitch-common -y\n</code></pre> <p>On the master, where you intend to start OVN's central components, run:</p> <pre><code>sudo apt-get install ovn-central ovn-common ovn-host -y\n</code></pre> <p>On the agent nodes, run:</p> <pre><code>sudo apt-get install ovn-host ovn-common -y\n</code></pre>"},{"location":"installation/INSTALL.UBUNTU/#installing-ovs-and-ovn-from-sources","title":"Installing OVS and OVN from sources","text":"<p>Install a few pre-requisite packages.</p> <pre><code>apt-get update\napt-get install -y build-essential fakeroot debhelper \\\n                    autoconf automake libssl-dev \\\n                    openssl python-all \\\n                    python-setuptools \\\n                    python-six \\\n                    libtool git dh-autoreconf \\\n                    linux-headers-$(uname -r)\n</code></pre> <p>Clone the OVS repo.</p> <pre><code>git clone https://github.com/openvswitch/ovs.git\ncd ovs\n</code></pre> <p>Configure and compile the sources</p> <pre><code>./boot.sh\n./configure --prefix=/usr --localstatedir=/var  --sysconfdir=/etc --enable-ssl --with-linux=/lib/modules/`uname -r`/build\nmake -j3\n</code></pre> <p>Install the executables</p> <pre><code>make install\nmake modules_install\n</code></pre> <p>Create a depmod.d file to use OVS kernel modules from this repo instead of upstream linux.</p> <pre><code>cat &gt; /etc/depmod.d/openvswitch.conf &lt;&lt; EOF\noverride openvswitch * extra\noverride vport-geneve * extra\noverride vport-stt * extra\noverride vport-* * extra\nEOF\n</code></pre> <p>Copy a startup script and start OVS</p> <pre><code>depmod -a\ncp debian/openvswitch-switch.init /etc/init.d/openvswitch-switch\n/etc/init.d/openvswitch-switch force-reload-kmod\n</code></pre>"},{"location":"installation/OVN-NORTHD.SSL/","title":"OVN NORTHD.SSL","text":"<p>If the ovn-northd instance is not running on the same node as OVN NB and OVN SB database, then you will need to follow the steps below to secure the communication between ovn-northd and NB/SB databases.</p>"},{"location":"installation/OVN-NORTHD.SSL/#generate-signed-certificates-for-ovn-northd","title":"Generate signed certificates for OVN Northd","text":"<p>On the node running ovn-northd daemon, run the following commands. <pre><code>cd /etc/openvswitch\novs-pki req ovnnorthd\n</code></pre></p> <p>The above command will generate a private key \"ovnnorthd-privkey.pem\" and a corresponding certificate request named \"ovnnorthd-req.pem\". Copy over the ovnnorthd-req.pem to the secure machine's (where you have installed the ovs-pki utility) /var/lib/openvswitch/pki folder to sign it using the command below.</p> <pre><code>ovs-pki -b sign ovnnorthd\n</code></pre> <p>The above command will generate ovnnorthd-cert.pem. Copy over this file back to the ovn-northd node's /etc/openvswitch foler. The ovnnorthd-privkey.pem and ovnnorthd-cert.pem will be used by the ovn-northd to communicate with OVN NB  and SB database.</p> <p>Now run the following command to ask ovn-northd to use these certificates.</p> <pre><code>cat &gt; /etc/openvswitch/ovn-northd-db-params.conf &lt;&lt; EOF\n--ovnnb-db=ssl:$CENTRAL_IP:6641 --ovnsb-db=ssl:$CENTRAL_IP:6642 \\\n-p /etc/openvswitch/ovnnorthd-privkey.pem -c /etc/openvswitch/ovnnorthd-cert.pem \\\n-C /etc/openvswitch/cacert.pem\nEOF\n\n/usr/share/openvswitch/scripts/ovn-ctl restart_northd\n</code></pre>"},{"location":"installation/ha/","title":"OVN central database High-availability","text":"<p>OVN architecture has two central databases that can be clustered. The databases are OVN_Northbound and OVN_Southbound.  This document explains how to cluster them and start various daemons for the ovn-kubernetes integration.  You will ideally need at least 3 masters for a HA cluster. (You will need a miniumum of OVS/OVN 2.9.2 for clustering.)</p>"},{"location":"installation/ha/#master1-initialization","title":"Master1 initialization","text":"<p>To bootstrap your cluster, you need to start on one master. For a lack of better name, let's call it MASTER1 with an IP address of $MASTER1</p> <p>On MASTER1, delete any stale OVN databases and stop any ovn-northd running. e.g:</p> <pre><code>sudo /usr/share/openvswitch/scripts/ovn-ctl stop_nb_ovsdb\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_sb_ovsdb\nsudo rm /etc/openvswitch/ovn*.db\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_northd\n</code></pre> <p>Start the two databases on that host with:</p> <pre><code>LOCAL_IP=$MASTER1\nsudo /usr/share/openvswitch/scripts/ovn-ctl \\\n    --db-nb-cluster-local-addr=$LOCAL_IP start_nb_ovsdb\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl \\\n    --db-sb-cluster-local-addr=$LOCAL_IP start_sb_ovsdb\n</code></pre>"},{"location":"installation/ha/#master2-master3-initialization","title":"Master2, Master3... initialization","text":"<p>Delete any stale databases and stop any running ovn-northd daemons. e.g:</p> <pre><code>sudo /usr/share/openvswitch/scripts/ovn-ctl stop_nb_ovsdb\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_sb_ovsdb\nsudo rm /etc/openvswitch/ovn*.db\nsudo /usr/share/openvswitch/scripts/ovn-ctl stop_northd\n</code></pre> <p>On master with a IP of $LOCAL_IP, start the databases and ask it to join $MASTER1</p> <pre><code>LOCAL_IP=$LOCAL_IP\nMASTER_IP=$MASTER1\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl  \\\n    --db-nb-cluster-local-addr=$LOCAL_IP \\\n    --db-nb-cluster-remote-addr=$MASTER_IP start_nb_ovsdb\n\nsudo /usr/share/openvswitch/scripts/ovn-ctl  \\\n    --db-sb-cluster-local-addr=$LOCAL_IP \\\n    --db-sb-cluster-remote-addr=$MASTER_IP start_sb_ovsdb\n</code></pre> <p>This should get your cluster up and running. You can verify the status of your cluster with:</p> <pre><code>sudo ovs-appctl -t /var/run/openvswitch/ovnnb_db.ctl \\\n    cluster/status OVN_Northbound\n\nsudo ovs-appctl -t /var/run/openvswitch/ovnsb_db.ctl \\\n    cluster/status OVN_Southbound\n</code></pre>"},{"location":"installation/ha/#ovnkube-master-ha-setup","title":"ovnkube master HA setup","text":"<p>ovnkube master has 2 main components - cluster-manager and ovnkube-controller.</p> <p>Starting ovnkube with '-init-master', runs both the components.  It is also possible to run these components individually by starting 2 ovnkube's one with '-init-cluster-manager' and the other with '-init-ovnkube-controller'.</p> <p>On the master nodes, we can either    * start ovnkube with '-init-master'      This should be a deployment running on master nodes. Eg.</p> <p>IP1=\"$MASTER1\" IP2=\"$MASTER2\" IP3=\"$MASTER3\"</p> <p>ovn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\" ovn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"</p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-master=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport \\  -nb-address=\"${ovn_nb}\" \\  -sb-address=\"${ovn_sb}\"  2&gt;&amp;1 &amp;</p> <ul> <li>start 'ovnkube -init-cluster-manager' and 'ovnkube -init-ovnkube-controller'    This should be a deployment with these 2 as containers</li> </ul> <p>Eg.</p> <p>ovnkube master supports running in 3 modes. init-master mode, init-cluster-manager mode or init-ovnkube-controller mode.  If ovnkube is run with \"-init-master\" mode, then there is no need to run the other modes because master mode enables both cluster-manager and ovnkube-controller.  If the user desires to run cluster-manager and ovnkube-controller separately, then it is possible to do so by running </p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-ovnkube-controller=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport \\  -nb-address=\"${ovn_nb}\" \\  -sb-address=\"${ovn_sb}\"  2&gt;&amp;1 &amp;</p> <p>nohup sudo ovnkube -k8s-kubeconfig kubeconfig.yaml \\  -loglevel=4 \\  -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\  -logfile=\"/var/log/openvswitch/ovnkube.log\" \\  -init-cluster-manager=\"$NODENAME\" -cluster-subnets=\"$CLUSTER_IP_SUBNET\" \\  -init-node=\"$NODENAME\" \\  -k8s-service-cidr=\"$SERVICE_IP_SUBNET\" \\  -k8s-token=\"$TOKEN\" \\  -nodeport  2&gt;&amp;1 &amp;</p>"},{"location":"installation/ha/#start-ovn-northd","title":"start ovn-northd","text":"<p>On any one of the masters (ideally via a daemonset with replica count as 1), start ovn-northd. Let the 3 master IPs be $IP1, $IP2 and $IP3.</p> <pre><code>IP1=\"$MASTER1\"\nIP2=\"$MASTER2\"\nIP3=\"$MASTER3\"\n\nexport ovn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\"\nexport ovn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"\n\nsudo ovn-northd -vconsole:emer -vsyslog:err -vfile:info \\\n    --ovnnb-db=\"$ovn_nb\" --ovnsb-db=\"$ovn_sb\" --no-chdir \\\n    --log-file=/var/log/openvswitch/ovn-northd.log \\\n    --pidfile=/var/run/openvswitch/ovn-northd.pid --detach --monitor\n</code></pre>"},{"location":"installation/ha/#start-ovn-kube-init-node","title":"Start 'ovn-kube -init-node'","text":"<p>On all nodes (and if needed on other masters), start ovnkube with '-init-node'. For e.g:</p> <pre><code>IP1=\"$MASTER1\"\nIP2=\"$MASTER2\"\nIP3=\"$MASTER3\"\n\novn_nb=\"tcp:$IP1:6641,tcp:$IP2:6641,tcp:$IP3:6641\"\novn_sb=\"tcp:$IP1:6642,tcp:$IP2:6642,tcp:$IP3:6642\"\n\nnohup sudo ovnkube -k8s-kubeconfig $HOME/kubeconfig.yaml -loglevel=4 \\\n    -logfile=\"/var/log/openvswitch/ovnkube.log\" \\\n    -k8s-apiserver=\"http://$K8S_APISERVER_IP:8080\" \\\n    -init-node=\"$NODE_NAME\"  \\\n    -nb-address=\"${ovn_nb}\" \\\n    -sb-address=\"${ovn_sb}\" \\\n    -k8s-token=\"$TOKEN\" \\\n    -init-gateways \\\n    -k8s-service-cidr= \\\n    -cluster-subnets=\"$SERVICE_IP_SUBNET\" 2&gt;&amp;1 &amp;\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/","title":"OVN-Kubernetes KIND Setup","text":"<p>KIND (Kubernetes in Docker) deployment of OVN kubernetes is a fast and easy means to quickly install and test kubernetes with OVN kubernetes CNI. The value proposition is really for developers who want to reproduce an issue or test a fix in an environment that can be brought up locally and within a few minutes.</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#prerequisites","title":"Prerequisites","text":"<ul> <li>20 GB of free space in root file system</li> <li>Docker run time or podman</li> <li>KIND</li> <li>Installation instructions can be found at https://github.com/kubernetes-sigs/kind#installation-and-usage. </li> <li> <p>NOTE: The OVN-Kubernetes ovn-kubernetes/contrib/kind.sh and ovn-kubernetes/contrib/kind.yaml files provision port 11337. If firewalld is enabled, this port will need to be unblocked:</p> <p><pre><code>sudo firewall-cmd --permanent --add-port=11337/tcp; sudo firewall-cmd --reload\n</code></pre> - kubectl - Python 3 and pipx - jq - openssl - openvswitch - Go 1.23.0 or above - For podman users: skopeo</p> </li> </ul> <p>For OVN kubernetes KIND deployment, use the <code>kind.sh</code> script.</p> <p>First Download and build the OVN-Kubernetes repo: </p> <p><pre><code>git clone https://github.com/ovn-kubernetes/ovn-kubernetes.git \ncd ovn-kubernetes\n</code></pre> The <code>kind.sh</code> script builds OVN-Kubernetes into a container image. To verify local changes before building in KIND, run the following:</p> <pre><code>$ pushd go-controller\n$ make\n$ popd\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#run-the-kind-deployment-with-docker","title":"Run the KIND deployment with docker","text":"<p>Build the image for fedora and launch the KIND Deployment</p> <pre><code>$ pushd dist/images\n$ make fedora-image\n$ popd\n\n$ pushd contrib\n$ export KUBECONFIG=${HOME}/ovn.conf\n$ ./kind.sh\n$ popd\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#run-the-kind-deployment-with-podman","title":"Run the KIND deployment with podman","text":"<p>To verify local changes, the steps are mostly the same as with docker, except the <code>fedora</code> make target:</p> <pre><code>$ pushd dist/images\n</code></pre> <p>Then Edit the makefile, changing </p> <pre><code>$ OCI_BIN=podman\n</code></pre> <p>Then build,</p> <pre><code>$ make fedora-image\n$ popd\n</code></pre> <p>To deploy KIND however, you need to start it as root and then copy root's kube config to use it as non-root:</p> <pre><code>$ pushd contrib\n$ sudo ./kind.sh -ep podman\n$ mkdir -p ~/.kube\n$ sudo cp /root/ovn.conf ~/.kube/kind-config\n$ sudo chown $(id -u):$(id -g) ~/.kube/kind-config\n$ export KUBECONFIG=~/.kube/kind-config\n$ popd\n</code></pre> <p>NOTE: If you installed go via the official path on Linux and have encountered the \"go: command not found\" issue, you can preserve your environment when doing sudo: <code>sudo --preserve-env=PATH ./kind.sh -ep podman</code></p> <p>This will launch a KIND deployment. By default, the cluster is named <code>ovn</code>.</p> <pre><code>$ kubectl get nodes\nNAME                STATUS   ROLES    AGE     VERSION\novn-control-plane   Ready    master   5h13m   v1.16.4\novn-worker          Ready    &lt;none&gt;   5h12m   v1.16.4\novn-worker2         Ready    &lt;none&gt;   5h12m   v1.16.4\n\n$ kubectl get pods --all-namespaces\nNAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE\nkube-system          coredns-5644d7b6d9-kw2xc                    1/1     Running   0          5h13m\nkube-system          coredns-5644d7b6d9-sd9wh                    1/1     Running   0          5h13m\nkube-system          etcd-ovn-control-plane                      1/1     Running   0          5h11m\nkube-system          kube-apiserver-ovn-control-plane            1/1     Running   0          5h12m\nkube-system          kube-controller-manager-ovn-control-plane   1/1     Running   0          5h12m\nkube-system          kube-scheduler-ovn-control-plane            1/1     Running   0          5h11m\nlocal-path-storage   local-path-provisioner-7745554f7f-9r8dz     1/1     Running   0          5h13m\novn-kubernetes       ovnkube-db-5588bd699c-kb8h7                 2/2     Running   0          5h11m\novn-kubernetes       ovnkube-master-6f44d456df-bv2x8             2/2     Running   0          5h11m\novn-kubernetes       ovnkube-node-2t6m2                          3/3     Running   0          5h11m\novn-kubernetes       ovnkube-node-hhsmk                          3/3     Running   0          5h11m\novn-kubernetes       ovnkube-node-xvqh4                          3/3     Running   0          5h11m\n</code></pre> <p>The <code>kind.sh</code> script defaults the cluster to HA disabled. There are numerous configuration options when deploying. Use <code>./kind.sh -h</code> to see the latest options.</p> <pre><code>[root@ovnkubernetes contrib]# ./kind.sh --help\nusage: kind.sh [[[-cf |--config-file &lt;file&gt;] [-kt|keep-taint] [-ha|--ha-enabled]\n                 [-ho |--hybrid-enabled] [-ii|--install-ingress] [-n4|--no-ipv4]\n                 [-i6 |--ipv6] [-wk|--num-workers &lt;num&gt;] [-ds|--disable-snat-multiple-gws]\n                 [-dp |--disable-pkt-mtu-check] [-df|--disable-forwarding]\n                 [-nf |--netflow-targets &lt;targets&gt;] [sf|--sflow-targets &lt;targets&gt;]\n                 [-if |--ipfix-targets &lt;targets&gt;] [-ifs|--ipfix-sampling &lt;num&gt;]\n                 [-ifm|--ipfix-cache-max-flows &lt;num&gt;] [-ifa|--ipfix-cache-active-timeout &lt;num&gt;]\n                 [-sw |--allow-system-writes] [-gm|--gateway-mode &lt;mode&gt;]\n                 [-nl |--node-loglevel &lt;num&gt;] [-ml|--master-loglevel &lt;num&gt;]\n                 [-dbl|--dbchecker-loglevel &lt;num&gt;] [-ndl|--ovn-loglevel-northd &lt;loglevel&gt;]\n                 [-nbl|--ovn-loglevel-nb &lt;loglevel&gt;] [-sbl|--ovn-loglevel-sb &lt;loglevel&gt;]\n                 [-cl |--ovn-loglevel-controller &lt;loglevel&gt;] [-me|--multicast-enabled]\n                 [-ep |--experimental-provider &lt;name&gt;] |\n                 [-eb |--egress-gw-separate-bridge]\n                 [-nqe|--network-qos-enable]\n                 [-h]]\n\n-cf  | --config-file                Name of the KIND J2 configuration file.\n                                    DEFAULT: ./kind.yaml.j2\n-kt  | --keep-taint                 Do not remove taint components.\n                                    DEFAULT: Remove taint components.\n-ha  | --ha-enabled                 Enable high availability. DEFAULT: HA Disabled.\n-me  | --multicast-enabled          Enable multicast. DEFAULT: Disabled.\n-scm | --separate-cluster-manager   Separate cluster manager from ovnkube-master and run as a separate container within ovnkube-master deployment.\n-ho  | --hybrid-enabled             Enable hybrid overlay. DEFAULT: Disabled.\n-ds  | --disable-snat-multiple-gws  Disable SNAT for multiple gws. DEFAULT: Disabled.\n-dp  | --disable-pkt-mtu-check      Disable checking packet size greater than MTU. Default: Disabled\n-df  | --disable-forwarding         Disable forwarding on OVNK controlled interfaces. Default: Enabled\n-nf  | --netflow-targets            Comma delimited list of ip:port or :port (using node IP) netflow collectors. DEFAULT: Disabled.\n-sf  | --sflow-targets              Comma delimited list of ip:port or :port (using node IP) sflow collectors. DEFAULT: Disabled.\n-if  | --ipfix-targets              Comma delimited list of ip:port or :port (using node IP) ipfix collectors. DEFAULT: Disabled.\n-ifs | --ipfix-sampling             Fraction of packets that are sampled and sent to each target collector: 1 packet out of every &lt;num&gt;. DEFAULT: 400 (1 out of 400 packets).\n-ifm | --ipfix-cache-max-flows      Maximum number of IPFIX flow records that can be cached at a time. If 0, caching is disabled. DEFAULT: Disabled.\n-ifa | --ipfix-cache-active-timeout Maximum period in seconds for which an IPFIX flow record is cached and aggregated before being sent. If 0, caching is disabled. DEFAULT: 60.\n-el  | --ovn-empty-lb-events        Enable empty-lb-events generation for LB without backends. DEFAULT: Disabled\n-ii  | --install-ingress            Flag to install Ingress Components.\n                                    DEFAULT: Don't install ingress components.\n-n4  | --no-ipv4                    Disable IPv4. DEFAULT: IPv4 Enabled.\n-i6  | --ipv6                       Enable IPv6. DEFAULT: IPv6 Disabled.\n-wk  | --num-workers                Number of worker nodes. DEFAULT: HA - 2 worker\n                                    nodes and no HA - 0 worker nodes.\n-sw  | --allow-system-writes        Allow script to update system. Intended to allow\n                                    github CI to be updated with IPv6 settings.\n                                    DEFAULT: Don't allow.\n-gm  | --gateway-mode               Enable 'shared' or 'local' gateway mode.\n                                    DEFAULT: shared.\n-ov  | --ovn-image                  Use the specified docker image instead of building locally. DEFAULT: local build.\n-ml  | --master-loglevel            Log level for ovnkube (master), DEFAULT: 5.\n-nl  | --node-loglevel              Log level for ovnkube (node), DEFAULT: 5\n-dbl | --dbchecker-loglevel         Log level for ovn-dbchecker (ovnkube-db), DEFAULT: 5.\n-ndl | --ovn-loglevel-northd        Log config for ovn northd, DEFAULT: '-vconsole:info -vfile:info'.\n-nbl | --ovn-loglevel-nb            Log config for northbound DB DEFAULT: '-vconsole:info -vfile:info'.\n-sbl | --ovn-loglevel-sb            Log config for southboudn DB DEFAULT: '-vconsole:info -vfile:info'.\n-cl  | --ovn-loglevel-controller    Log config for ovn-controller DEFAULT: '-vconsole:info'.\n-ep  | --experimental-provider      Use an experimental OCI provider such as podman, instead of docker. DEFAULT: Disabled.\n-eb  | --egress-gw-separate-bridge  The external gateway traffic uses a separate bridge.\n-nqe | --network-qos-enable         Enable network QoS. DEFAULT: Disabled.\n-lr  |--local-kind-registry         Will start and connect a kind local registry to push/retrieve images\n--delete                            Delete current cluster\n--deploy                            Deploy ovn-kubernetes without restarting kind\n</code></pre> <p>As seen above, if you do not specify any options the script will assume the default values.</p> <p>Notes / troubleshooting:</p> <ul> <li>Issue with /dev/dma_heap: if you get the error <code>kind \"Error: open /dev/dma_heap: permission denied\"</code>, there's a known issue about it (directory mislabelled with selinux). Workaround:</li> </ul> <pre><code>sudo setenforce 0\nsudo chcon system\\_u:object\\_r:device\\_t:s0 /dev/dma\\_heap/\nsudo setenforce 1\n</code></pre> <ul> <li>If you see errors related to go, you may not have go <code>$PATH</code> configured as root. Make sure it is configured, or define it while running <code>kind.sh</code>:</li> </ul> <pre><code>sudo PATH=$PATH:/usr/local/go/bin ./kind.sh -ep podman\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#usage-notes","title":"Usage Notes","text":"<ul> <li> <p>You can create your own KIND J2 configuration file if the default one is not sufficient</p> </li> <li> <p>You can also specify these values as environment variables. Command line parameters will override the environment variables.</p> </li> <li> <p>To tear down the KIND cluster when finished simply run </p> </li> </ul> <pre><code>$ ./kind.sh --delete\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#running-ovn-kubernetes-with-ipv6-or-dual-stack-in-kind","title":"Running OVN-Kubernetes with IPv6 or Dual-stack In KIND","text":"<p>This section describes the configuration needed for IPv6 and dual-stack environments.</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#kind-with-ipv6","title":"KIND with IPv6","text":""},{"location":"installation/launching-ovn-kubernetes-on-kind/#docker-changes-for-ipv6","title":"Docker Changes For IPv6","text":"<p>For KIND clusters using KIND v0.7.0 or older (CI currently is using v0.8.1), to use IPv6, IPv6 needs to be enable in Docker on the host:</p> <pre><code>$ sudo vi /etc/docker/daemon.json\n{\n  \"ipv6\": true\n}\n\n$ sudo systemctl reload docker\n</code></pre> <p>On a CentOS host running Docker version 19.03.6, the above configuration worked. After the host was rebooted, Docker failed to start. To fix, change <code>daemon.json</code> as follows:</p> <pre><code>$ sudo vi /etc/docker/daemon.json\n{\n  \"ipv6\": true,\n  \"fixed-cidr-v6\": \"2001:db8:1::/64\"\n}\n\n$ sudo systemctl reload docker\n</code></pre> <p>IPv6 from Docker repo provided the fix. Newer documentation does not include this change, so change may be dependent on Docker version.</p> <p>To verify IPv6 is enabled in Docker, run:</p> <pre><code>$ docker run --rm busybox ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n341: eth0@if342: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue\n    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 2001:db8:1::242:ac11:2/64 scope global flags 02\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe11:2/64 scope link tentative\n       valid_lft forever preferred_lft forever\n</code></pre> <p>For the eth0 vEth-pair, there should be the two IPv6 entries (global and link addresses).</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#disable-firewalld","title":"Disable firewalld","text":"<p>Currently, to run OVN-Kubernetes with IPv6 only in a KIND deployment, firewalld needs to be disabled. To disable:</p> <pre><code>sudo systemctl stop firewalld\n</code></pre> <p>NOTE: To run with IPv4, firewalld needs to be enabled, so to reenable:</p> <pre><code>sudo systemctl start firewalld\n</code></pre> <p>If firewalld is enabled during a IPv6 deployment, additional nodes fail to join the cluster:</p> <pre><code>:\nCreating cluster \"ovn\" ...\n \u2713 Ensuring node image (kindest/node:v1.18.2) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6 \ud83d\udce6\n \u2713 Writing configuration \ud83d\udcdc\n \u2713 Starting control-plane \ud83d\udd79\ufe0f\n \u2713 Installing StorageClass \ud83d\udcbe\n \u2717 Joining worker nodes \ud83d\ude9c\nERROR: failed to create cluster: failed to join node with kubeadm: command \"docker exec --privileged ovn-worker kubeadm join --config /kind/kubeadm.conf --ignore-preflight-errors=all --v=6\" failed with error: exit status 1\n</code></pre> <p>And logs show:</p> <pre><code>I0430 16:40:44.590181     579 token.go:215] [discovery] Failed to request cluster-info, will try again: Get https://[2001:db8:1::242:ac11:3]:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s: dial tcp [2001:db8:1::242:ac11:3]:6443: connect: permission denied\nGet https://[2001:db8:1::242:ac11:3]:6443/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s: dial tcp [2001:db8:1::242:ac11:3]:6443: connect: permission denied\n</code></pre> <p>This issue was reported upstream in KIND 1257 and blamed on firewalld.</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#ovn-kubernetes-with-ipv6","title":"OVN-Kubernetes With IPv6","text":"<p>To run OVN-Kubernetes with IPv6 in a KIND deployment, run:</p> <pre><code>$ go get github.com/ovn-org/ovn-kubernetes; cd $GOPATH/src/github.com/ovn-org/ovn-kubernetes\n\n$ cd go-controller/\n$ make\n\n$ cd ../dist/images/\n$ make fedora-image\n\n$ cd ../../contrib/\n$ PLATFORM_IPV4_SUPPORT=false PLATFORM_IPV6_SUPPORT=true ./kind.sh\n</code></pre> <p>Once <code>kind.sh</code> completes, setup kube config file:</p> <pre><code>$ cp ~/ovn.conf ~/.kube/config\n-- OR --\n$ KUBECONFIG=~/ovn.conf\n</code></pre> <p>Once testing is complete, to tear down the KIND deployment:</p> <pre><code>$ kind delete cluster --name ovn\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#kind-with-dual-stack","title":"KIND with Dual-stack","text":"<p>Currently, IP dual-stack is not fully supported in: * Kubernetes * KIND * OVN-Kubernetes</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#kubernetes-and-docker-with-ip-dual-stack","title":"Kubernetes And Docker With IP Dual-stack","text":""},{"location":"installation/launching-ovn-kubernetes-on-kind/#update-kubectl","title":"Update kubectl","text":"<p>Kubernetes has some IP dual-stack support but the feature is not complete. Additional changes are constantly being added. This setup is using the latest Kubernetes release to test against. Kubernetes is being installed below using OVN-Kubernetes KIND script, however to test, an equivalent version of <code>kubectl</code> needs to be installed.</p> <p>First determine what version of <code>kubectl</code> is currently being used and save it:</p> <pre><code>$ which kubectl\n/usr/bin/kubectl\n$ kubectl version --client\nClient Version: version.Info{Major:\"1\", Minor:\"28\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-11T18:14:22Z\", GoVersion:\"go1.13.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nsudo mv /usr/bin/kubectl /usr/bin/kubectl-v1.17.3\nsudo ln -s /usr/bin/kubectl-v1.17.3 /usr/bin/kubectl\n</code></pre> <p>Download and install latest version of <code>kubectl</code>:</p> <pre><code>$ K8S_VERSION=v1.34.0\n$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$K8S_VERSION/bin/linux/amd64/kubectl\n$ chmod +x kubectl\n$ sudo mv kubectl /usr/bin/kubectl-$K8S_VERSION\n$ sudo rm /usr/bin/kubectl\n$ sudo ln -s /usr/bin/kubectl-$K8S_VERSION /usr/bin/kubectl\n$ kubectl version --client\nClient Version: v1.32.3\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#docker-changes-for-dual-stack","title":"Docker Changes For Dual-stack","text":"<p>For dual-stack, IPv6 needs to be enable in Docker on the host same as for IPv6 only. See above: Docker Changes For IPv6</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#kind-with-ip-dual-stack","title":"KIND With IP Dual-stack","text":"<p>IP dual-stack is not currently supported in KIND. There is a PR (692) with IP dual-stack changes. Currently using this to test with.</p> <p>Optionally, save previous version of KIND (if it exists):</p> <pre><code>cp $GOPATH/bin/kind $GOPATH/bin/kind.orig\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#build-kind-with-dual-stack-locally","title":"Build KIND With Dual-stack Locally","text":"<p>To build locally (if additional needed):</p> <pre><code>go get github.com/kubernetes-sigs/kind; cd $GOPATH/src/github.com/kubernetes-sigs/kind\ngit pull --no-edit --strategy=ours origin pull/692/head\nmake clean\nmake install INSTALL_DIR=$GOPATH/bin\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#ovn-kubernetes-with-ip-dual-stack","title":"OVN-Kubernetes With IP Dual-stack","text":"<p>For status of IP dual-stack in OVN-Kubernetes, see 1142.</p> <p>To run OVN-Kubernetes with IP dual-stack in a KIND deployment, run:</p> <pre><code>$ go get github.com/ovn-org/ovn-kubernetes; cd $GOPATH/src/github.com/ovn-org/ovn-kubernetes\n\n$ cd go-controller/\n$ make\n\n$ cd ../dist/images/\n$ make fedora-image\n\n$ cd ../../contrib/\n$ PLATFORM_IPV4_SUPPORT=true PLATFORM_IPV6_SUPPORT=true K8S_VERSION=v1.34.0 ./kind.sh\n</code></pre> <p>Once <code>kind.sh</code> completes, setup kube config file:</p> <pre><code>$ cp ~/ovn.conf ~/.kube/config\n-- OR --\n$ KUBECONFIG=~/ovn.conf\n</code></pre> <p>Once testing is complete, to tear down the KIND deployment:</p> <pre><code>$ kind delete cluster --name ovn\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#using-specific-kind-container-image-and-tag","title":"Using specific Kind container image and tag","text":"<p> Use with caution, as kind expects this image to have all it needs.</p> <p>In order to use an image/tag other than the default hardcoded in kind.sh, specify one (or both of) the following variables:</p> <pre><code>$ cd ../../contrib/\n$ KIND_IMAGE=example.com/kindest/node K8S_VERSION=v1.34.0 ./kind.sh\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#using-kind-local-registry-to-deploy-non-ovn-k-containers","title":"Using kind local registry to deploy non ovn-k containers","text":"<p>A local registry can be made available to the cluster if started with: <pre><code>./kind.sh --local-kind-registry\n</code></pre> This is useful if you want to make your own local images available to the  cluster. These images can be pushed, fetched or used  in manifests using the prefix <code>localhost:5000</code>.</p>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#loading-ovn-kubernetes-changes-without-restarting-kind","title":"Loading ovn-kubernetes changes without restarting kind","text":"<p>Sometimes it is useful to update ovn-kubernetes without redeploying the whole  cluster all over again. For example, when testing the update itself.  This can be achieve with the \"--deploy\" flag:</p> <pre><code># Default options will use kind mechanism to push images directly to the\n./kind.sh --deploy\n\n# Using a local registry is an alternative to deploy ovn-kubernetes updates \n# while also being useful to deploy other local images\n./kind.sh --deploy --local-kind-registry\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#current-status","title":"Current Status","text":"<p>This is subject to change because code is being updated constantly. But this is more a cautionary note that this feature is not completely working at the moment.</p> <p>The nodes do not go to ready because the OVN-Kubernetes hasn't setup the network completely:</p> <pre><code>$ kubectl get nodes\nNAME                STATUS     ROLES    AGE   VERSION\novn-control-plane   NotReady   master   94s   v1.18.0\novn-worker          NotReady   &lt;none&gt;   61s   v1.18.0\novn-worker2         NotReady   &lt;none&gt;   62s   v1.18.0\n\n$ kubectl get pods -o wide --all-namespaces\nNAMESPACE          NAME                                      READY STATUS   RESTARTS AGE    IP          NODE\nkube-system        coredns-66bff467f8-hh4c9                  0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\nkube-system        coredns-66bff467f8-vwbcj                  0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\nkube-system        etcd-ovn-control-plane                    1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-apiserver-ovn-control-plane          1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-controller-manager-ovn-control-plane 1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nkube-system        kube-scheduler-ovn-control-plane          1/1   Running  0        2m56s  172.17.0.2  ovn-control-plane\nlocal-path-storage local-path-provisioner-774f7f8fdb-msmd2   0/1   Pending  0        2m45s  &lt;none&gt;      &lt;none&gt;\novn-kubernetes     ovnkube-db-cf4cc89b7-8d4xq                2/2   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-master-87fb56d6d-7qmnb            2/2   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-node-278l9                        2/3   Running  0        107s   172.17.0.3  ovn-worker2\novn-kubernetes     ovnkube-node-bm7v6                        2/3   Running  0        107s   172.17.0.2  ovn-control-plane\novn-kubernetes     ovnkube-node-p4k4t                        2/3   Running  0        107s   172.17.0.4  ovn-worker\n</code></pre>"},{"location":"installation/launching-ovn-kubernetes-on-kind/#known-issues","title":"Known issues","text":"<p>Some environments (Fedora32,31 on desktop), have problems when the cluster is deleted directly with kind <code>kind delete cluster --name ovn</code>, it restarts the host. The root cause is unknown, this also can not be reproduced in Ubuntu 20.04 or with Fedora32 Cloud, but it does not happen if we clean first the ovn-kubernetes resources.</p> <p>You can use the following command to delete the cluster:</p> <p><code>contrib/kind.sh --delete</code></p>"},{"location":"installation/launching-ovn-kubernetes-with-helm/","title":"Launching OVN-Kubernetes using Helm Charts","text":""},{"location":"installation/launching-ovn-kubernetes-with-helm/#introduction","title":"Introduction","text":"<p>This helm chart supports deploying OVN K8s CNI in a K8s cluster.</p> <p>Open Virtual Networking (OVN) Kubernetes CNI is an open source networking and network security solution for Kubernetes workloads. It leverages a distributed OVN SDN control plane and per-node Open vSwitch (OVS) to provide network virtualization and network connectivity to K8s Pods. It does so by creating a logical network topology using logical constructs such as logical switches (Layer 2) and logical routers (Layer 3). The Pod interfaces are represented by logical ports on the logical switches. On these logical switch ports, one can specify IP network information (IP address and MAC address), anti-spoofing rules (MAC and IP), Security Groups, QoS configuration, and so on.</p> <p>A port, either physical SR-IOV VF or virtual VETH, assigned to a Pod will be associated with a corresponding logical port, this will result in applying all the logical port configuration onto the physical port. The logical port becomes the API for configuring the physical port.</p> <p>In addition to providing overlay network connectivity for Pods in the K8s cluster, OVN K8s CNI supports a plethora of advanced networking features, such as</p> <pre><code>- Optimized and Accelerated K8s Network Policy on Pod's traffic\n- Optimized and Accelerated K8s Service Implementation (aka Load Balancers and NAT)\n- Optimized and Accelerated Policy Based Routing\n- Multi-Home Pods with an option for Secondary networks to be on a Layer-2\n  Overlay (flat network), Layer-2 Underlay (VLAN-based) on private or public\n  subnets.\n- Optimized and Accelerated K8s Network Policy on Pod's secondary networks\n</code></pre> <p>Most of these services are distributed and implemented via a pipeline (series of OpenFlow tables with OpenFlow flows) on local OVS switches. These OVS pipelines are very amenable to offloading to NIC hardware, which should result in the best possible networking performance and CPU savings on the host.</p> <p>The OVN K8s CNI architecture is a layered architecture with OVS at the bottom, followed by OVN, and finally OVN K8s CNI at the top. Each layer has several K8s components - deployments, daemonsets, and statefulsets. Each component at every layer is a subchart by itself. Based on the deployment needs, all or some of these subcharts are installed to provide the aforementioned OVN K8s CNI features, this can be done by editing <code>tags</code> section in values.yaml file.</p>"},{"location":"installation/launching-ovn-kubernetes-with-helm/#quickstart","title":"Quickstart:","text":"<p>Run script <code>helm/basic-deploy.sh</code> to set up a basic OVN/Kubernetes cluster.</p>"},{"location":"installation/launching-ovn-kubernetes-with-helm/#manual-steps","title":"Manual steps:","text":"<ul> <li> <p>Launch a Kind cluster without CNI and kubeproxy (additional controle-plane or worker nodes can be added) <pre><code>kind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: worker\nnetworking:\n  disableDefaultCNI: true\n  kubeProxyMode: none\n</code></pre></p> </li> <li> <p>Optional: build local image and load it into Kind nodes <pre><code># cd dist/images\n# make ubuntu-image\n# docker tag ovn-kube-ubuntu:latest ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu:master\n# kind load docker-image ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu:master\n</code></pre></p> </li> </ul> <p>This chart does not use a <code>values.yaml</code> by default. You must specify a values file during installation.</p> <ul> <li>Run <code>helm install</code> with propery <code>k8sAPIServer</code> image repo and tag <pre><code># cd helm/ovn-kubernetes\n# helm install ovn-kubernetes . -f values-no-ic.yaml --set k8sAPIServer=\"https://$(kubectl get pods -n kube-system -l component=kube-apiserver -o jsonpath='{.items[0].status.hostIP}'):6443\" --set global.image.repository=ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu --set global.image.tag=master\n</code></pre></li> </ul>"},{"location":"installation/launching-ovn-kubernetes-with-helm/#alternative-configurations","title":"Alternative Configurations","text":"<p>To deploy ovn-kubernetes in interconnect mode, use <code>-f values-single-node-zone.yaml</code> instead of <code>-f values-no-ic.yaml</code>. Additionally, you must set the zone annotation on each node before deploying.</p> <p>Here is an example of how to set the annotation in a Kind cluster:</p> <pre><code>for n in $(kind get nodes --name \"${kind_cluster_name}\"); do\n  kubectl label node \"${n}\" k8s.ovn.org/zone-name=${n} --overwrite\ndone\n</code></pre> <p>Following section describes the meaning of the values.</p>"},{"location":"installation/launching-ovn-kubernetes-with-helm/#values","title":"Values","text":"Key Type Default Description global.aclLoggingRateLimit int <pre>\n20\n</pre> The largest number of messages per second that gets logged before drop @default 20 global.disableForwarding string <pre>\nfalse\n</pre> Controls if forwarding is allowed on OVNK controlled interfaces global.disablePacketMtuCheck string <pre>\n\"\"\n</pre> Disables adding openflow flows to check packets too large to be delivered to OVN due to pod MTU being lower than NIC MTU global.disableSnatMultipleGws string <pre>\n\"\"\n</pre> Whether to disable SNAT of egress traffic in namespaces annotated with routing-external-gws global.egressIpHealthCheckPort string <pre>\n\"\"\n</pre> Configure EgressIP node reachability using gRPC on this TCP port global.emptyLbEvents string <pre>\n\"\"\n</pre> If set, then load balancers do not get deleted when all backends are removed global.enableAdminNetworkPolicy string <pre>\n\"\"\n</pre> Whether or not to use Admin Network Policy CRD feature with ovn-kubernetes global.enableCompactMode bool <pre>\nfalse\n</pre> Indicate if ovnkube run master and node in one process global.enableConfigDuration string <pre>\n\"\"\n</pre> Enables monitoring OVN-Kubernetes master and OVN configuration duration global.enableEgressFirewall string <pre>\n\"\"\n</pre> Configure to use EgressFirewall CRD feature with ovn-kubernetes global.enableEgressIp string <pre>\n\"\"\n</pre> Configure to use EgressIP CRD feature with ovn-kubernetes global.enableEgressQos string <pre>\n\"\"\n</pre> Configure to use EgressQoS CRD feature with ovn-kubernetes global.enableEgressService string <pre>\n\"\"\n</pre> Configure to use EgressService CRD feature with ovn-kubernetes global.enableHybridOverlay string <pre>\n\"\"\n</pre> Whether or not to enable hybrid overlay functionality global.enableInterconnect bool <pre>\nfalse\n</pre> Configure to enable interconnecting multiple zones global.enableIpsec bool <pre>\nfalse\n</pre> Configure to enable IPsec global.enableLFlowCache bool <pre>\ntrue\n</pre> Indicates if ovn-controller should enable/disable the logical flow in-memory cache when processing Southbound database logical flow changes global.enableMetricsScale string <pre>\n\"\"\n</pre> Enables metrics related to scaling global.enableMultiExternalGateway bool <pre>\nfalse\n</pre> Configure to use AdminPolicyBasedExternalRoute CRD feature with ovn-kubernetes global.enableMultiNetwork bool <pre>\nfalse\n</pre> Configure to use multiple NetworkAttachmentDefinition CRD feature with ovn-kubernetes global.enableNetworkQos string <pre>\n\"\"\n</pre> Enables network QoS support from/to pods global.enableMulticast string <pre>\n\"\"\n</pre> Enables multicast support between the pods within the same namespace global.enableOvnKubeIdentity bool <pre>\ntrue\n</pre> Whether or not enable ovnkube identity webhook global.enableSsl bool <pre>\nfalse\n</pre> Use SSL transport to NB/SB db and northd global.enableStatelessNetworkPolicy bool <pre>\nfalse\n</pre> Configure to use stateless network policy feature with ovn-kubernetes global.enableSvcTemplate bool <pre>\ntrue\n</pre> Configure to use service template feature with ovn-kubernetes global.encapPort int <pre>\n6081\n</pre> GENEVE UDP port (default 6081) global.extGatewayNetworkInterface string <pre>\n\"\"\n</pre> The interface on nodes that will be used for external gateway network traffic global.gatewayMode string <pre>\n\"shared\"\n</pre> The gateway mode (shared or local), if not given, gateway functionality is disabled global.gatewayOpts string <pre>\n\"\"\n</pre> Optional extra gateway options global.hybridOverlayNetCidr string <pre>\n\"\"\n</pre> A comma separated set of IP subnets and the associated hostsubnetlengths (eg, \\\"10.128.0.0/14/23,10.0.0.0/14/23\\\") to use with the extended hybrid network global.image.pullPolicy string <pre>\n\"IfNotPresent\"\n</pre> Image pull policy global.image.repository string <pre>\n\"ghcr.io/ovn-kubernetes/ovn-kubernetes/ovn-kube-ubuntu\"\n</pre> Image repository for ovn-kubernetes components global.image.tag string <pre>\n\"master\"\n</pre> Specify image tag to run global.ipfixCacheActiveTimeout string <pre>\n\"\"\n</pre> Maximum period in seconds for which an IPFIX flow record is cached and aggregated before being sent @default 60 global.ipfixCacheMaxFlows string <pre>\n\"\"\n</pre> Maximum number of IPFIX flow records that can be cached at a time @default 0, meaning disabled global.ipfixSampling string <pre>\n\"\"\n</pre> Rate at which packets should be sampled and sent to each target collector @default 400 global.ipfixTargets string <pre>\n\"\"\n</pre> A comma separated set of IPFIX collectors to export flow data global.lFlowCacheLimit string <pre>\nunlimited\n</pre> Maximum number of logical flow cache entries ovn-controller may create when the logical flow cache is enabled global.lFlowCacheLimitKb string <pre>\n\"\"\n</pre> Maximum size of the logical flow cache (in KB) ovn-controller may create when the logical flow cache is enabled global.libovsdbClientLogFile string <pre>\n\"\"\n</pre> Separate log file for libovsdb client  global.monitorAll string <pre>\n\"\"\n</pre> Enable monitoring all data from SB DB instead of conditionally monitoring the data relevant to this node only @default true global.nbPort int <pre>\n6641\n</pre> Port of north bound ovsdb global.netFlowTargets string <pre>\n\"\"\n</pre> A comma separated set of NetFlow collectors to export flow data global.nodeMgmtPortNetdev string <pre>\n\"\"\n</pre> The net device to be used for management port, will be renamed to ovn-k8s-mp0 and used to allow host network services and pods to access k8s pod and service networks global.ofctrlWaitBeforeClear string <pre>\n\"\"\n</pre> ovn-controller wait time in ms before clearing OpenFlow rules during start up @default 0 global.remoteProbeInterval int <pre>\n100000\n</pre> OVN remote probe interval in ms  @default 100000 global.sbPort int <pre>\n6642\n</pre> Port of south bound ovsdb global.sflowTargets string <pre>\n\"\"\n</pre> A comma separated set of SFlow collectors to export flow data global.unprivilegedMode bool <pre>\nfalse\n</pre> This allows ovnkube-node to run without SYS_ADMIN capability, by performing interface setup in the CNI plugin global.v4JoinSubnet string <pre>\n\"\"\n</pre> The v4 join subnet used for assigning join switch IPv4 addresses global.v4MasqueradeSubnet string <pre>\n\"\"\n</pre> The v4 masquerade subnet used for assigning masquerade IPv4 addresses global.v6JoinSubnet string <pre>\n\"\"\n</pre> The v6 join subnet used for assigning join switch IPv6 addresses global.v6MasqueradeSubnet string <pre>\n\"\"\n</pre> The v6 masquerade subnet used for assigning masquerade IPv6 addresses k8sAPIServer string <pre>\n\"https://172.25.0.2:6443\"\n</pre> Endpoint of Kubernetes api server mtu int <pre>\n1400\n</pre> MTU of network interface in a Kubernetes pod podNetwork string <pre>\n\"10.128.0.0/14/23\"\n</pre> IP range for Kubernetes pods, /14 is the top level range, under which each /23 range will be assigned to a node serviceNetwork string <pre>\n\"172.30.0.0/16\"\n</pre> A comma-separated set of CIDR notation IP ranges from which k8s assigns service cluster IPs. This should be the same as the value provided for kube-apiserver \"--service-cluster-ip-range\" option skipCallToK8s bool <pre>\nfalse\n</pre> Whether or not call `lookup` Helm function, set it to `true` if you want to run `helm dry-run/template/lint` tags object <pre>\n{\n  \"ovn-ipsec\": false,\n  \"ovnkube-control-plane\": false,\n  \"ovnkube-db-raft\": false,\n  \"ovnkube-node-dpu\": false,\n  \"ovnkube-node-dpu-host\": false,\n  \"ovnkube-single-node-zone\": false,\n  \"ovnkube-zone-controller\": false\n}\n</pre> list of dependent subcharts that need to be installed for the given deployment mode, these subcharts haven't been tested yet."},{"location":"observability/metrics/","title":"Metrics","text":""},{"location":"observability/metrics/#ovn-kubernetes-master","title":"OVN-Kubernetes master","text":"<p>This includes a description of a selective set of metrics and to explore the exhausted set, see <code>go-controller/pkg/metrics/master.go</code></p>"},{"location":"observability/metrics/#configuration-duration-recorder","title":"Configuration duration recorder","text":""},{"location":"observability/metrics/#setup","title":"Setup","text":"<p>Enabled by default with the <code>kind.sh</code> (in directory <code>$ROOT/contrib</code>) Kind setup script. Disabled by default for binary ovnkube-master and enabled with flag <code>--metrics-enable-config-duration</code>.</p>"},{"location":"observability/metrics/#high-level-description","title":"High-level description","text":"<p>This set of metrics gives a result for the upper bound duration which means, it has taken at most this amount of seconds to apply the configuration to all nodes. It does not represent the exact accurate time to apply only this configuration. Measurement accuracy can be impacted by other parallel processing that might be occurring while the measurement is in progress therefore, the accuracy of the measurements should only indicate upper bound duration to roll out configuration changes.</p>"},{"location":"observability/metrics/#metrics_1","title":"Metrics","text":"Name Prometheus type Description ovnkube_master_network_programming_duration_seconds Histogram The duration to apply network configuration for a kind (e.g. pod, service, networkpolicy). Configuration includes add, update and delete events for kinds. This includes OVN-Kubernetes master and OVN duration. ovnkube_master_network_programming_ovn_duration_seconds Histogram The duration for OVN to apply network configuration for a kind (e.g. pod, service, networkpolicy)."},{"location":"observability/metrics/#change-log","title":"Change log","text":"<p>This list is to help notify if there are additions, changes or removals to metrics. Latest changes are at the top of this list.</p> <ul> <li>Add metrics to track logfile size for ovnkube processes - ovnkube_node_logfile_size_bytes and ovnkube_controller_logfile_size_bytes</li> <li>Remove ovnkube_controller_ovn_cli_latency_seconds metrics since we have moved most of the OVN DB operations to libovsdb.</li> <li>Effect of OVN IC architecture:</li> <li>Move all the metrics from subsystem \"ovnkube-master\" to subsystem \"ovnkube-controller\". The non-IC and IC deployments will each continue to have their ovnkube-master and ovnkube-controller containers running inside the ovnkube-master and ovnkube-controller pods. The metrics scraping should work seemlessly. See https://github.com/ovn-org/ovn-kubernetes/pull/3723 for details</li> <li>Move the following metrics from subsystem \"master\" to subsystem \"clustermanager\". Therefore, the follow metrics are renamed.<ul> <li><code>ovnkube_master_num_v4_host_subnets</code> -&gt; <code>ovnkube_clustermanager_num_v4_host_subnets</code></li> <li><code>ovnkube_master_num_v6_host_subnets</code> -&gt; <code>ovnkube_clustermanager_num_v6_host_subnets</code></li> <li><code>ovnkube_master_allocated_v4_host_subnets</code> -&gt; <code>ovnkube_clustermanager_allocated_v4_host_subnets</code></li> <li><code>ovnkube_master_allocated_v6_host_subnets</code> -&gt; <code>ovnkube_clustermanager_allocated_v6_host_subnets</code></li> <li><code>ovnkube_master_num_egress_ips</code> -&gt; <code>ovnkube_clustermanager_num_egress_ips</code></li> <li><code>ovnkube_master_egress_ips_node_unreachable_total</code> -&gt; <code>ovnkube_clustermanager_egress_ips_node_unreachable_total</code></li> <li><code>ovnkube_master_egress_ips_rebalance_total</code> -&gt; <code>ovnkube_clustermanager_egress_ips_rebalance_total</code></li> </ul> </li> <li>Update description of ovnkube_master_pod_creation_latency_seconds</li> <li>Add libovsdb metrics - ovnkube_master_libovsdb_disconnects_total and ovnkube_master_libovsdb_monitors.</li> <li>Add ovn_controller_southbound_database_connected metric (https://github.com/ovn-org/ovn-kubernetes/pull/3117).</li> <li>Stopwatch metrics now report in seconds instead of milliseconds.</li> <li>Rename (https://github.com/ovn-org/ovn-kubernetes/pull/3022):</li> <li><code>ovs_vswitchd_interface_link_resets</code> -&gt; <code>ovs_vswitchd_interface_resets_total</code></li> <li><code>ovs_vswitchd_interface_rx_dropped</code> -&gt; <code>ovs_vswitchd_interface_rx_dropped_total</code></li> <li><code>ovs_vswitchd_interface_tx_dropped</code> -&gt; <code>ovs_vswitchd_interface_tx_dropped_total</code></li> <li><code>ovs_vswitchd_interface_rx_errors</code> -&gt; <code>ovs_vswitchd_interface_rx_errors_total</code></li> <li><code>ovs_vswitchd_interface_tx_errors</code> -&gt; <code>ovs_vswitchd_interface_tx_errors_total</code></li> <li><code>ovs_vswitchd_interface_collisions</code> -&gt; <code>ovs_vswitchd_interface_collisions_total</code></li> <li>Remove (https://github.com/ovn-org/ovn-kubernetes/pull/3022):</li> <li><code>ovs_vswitchd_dp_if</code></li> <li><code>ovs_vswitchd_interface_driver_name</code></li> <li><code>ovs_vswitchd_interface_driver_version</code></li> <li><code>ovs_vswitchd_interface_firmware_version</code></li> <li><code>ovs_vswitchd_interface_rx_packets</code></li> <li><code>ovs_vswitchd_interface_tx_packets</code></li> <li><code>ovs_vswitchd_interface_rx_bytes</code></li> <li><code>ovs_vswitchd_interface_tx_bytes</code></li> <li><code>ovs_vswitchd_interface_rx_frame_err</code></li> <li><code>ovs_vswitchd_interface_rx_over_err</code></li> <li><code>ovs_vswitchd_interface_rx_crc_err</code></li> <li><code>ovs_vswitchd_interface_name</code></li> <li><code>ovs_vswitchd_interface_duplex</code></li> <li><code>ovs_vswitchd_interface_type</code></li> <li><code>ovs_vswitchd_interface_admin_state</code></li> <li><code>ovs_vswitchd_interface_link_state</code></li> <li><code>ovs_vswitchd_interface_ifindex</code></li> <li><code>ovs_vswitchd_interface_link_speed</code></li> <li><code>ovs_vswitchd_interface_mtu</code></li> <li><code>ovs_vswitchd_interface_ofport</code></li> <li><code>ovs_vswitchd_interface_ingress_policing_burst</code></li> <li><code>ovs_vswitchd_interface_ingress_policing_rate</code></li> <li>Add <code>ovnkube_master_network_programming_duration_seconds</code> and <code>ovnkube_master_network_programming_ovn_duration_seconds</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2878)</li> <li>Remove <code>ovnkube_master_skipped_nbctl_daemon_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2707)</li> <li>Add <code>ovnkube_master_egress_routing_via_host</code> (https://github.com/ovn-org/ovn-kubernetes/pull/2833)</li> <li>Add <code>ovnkube_resource_retry_failures_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/3314)</li> <li>Add <code>ovs_vswitchd_interfaces_total</code> and <code>ovs_vswitchd_interface_up_wait_seconds_total</code> (https://github.com/ovn-org/ovn-kubernetes/pull/3391)</li> <li>Add <code>ovnkube_controller_admin_network_policies</code> and <code>ovnkube_controller_baseline_admin_network_policies</code> (https://github.com/ovn-org/ovn-kubernetes/pull/4239)</li> <li>Add <code>ovnkube_controller_admin_network_policies_db_objects</code> and <code>ovnkube_controller_baseline_admin_network_policies_db_objects</code> (https://github.com/ovn-org/ovn-kubernetes/pull/4254)</li> </ul>"},{"location":"observability/ovn-observability/","title":"Observability","text":""},{"location":"observability/ovn-observability/#introduction","title":"Introduction","text":"<p>Observability feature uses OVN sampling functionality to generate samples with requested metadata when specific OVS flows are matched. To see the generated samples, a binary called <code>ovnkube-observ</code> is used. This binary allows printing the samples to stdout or writing them to a file.</p> <p>Currently, supports observability for: - Network Policy - (Baseline) Admin Network Policy - Egress firewall - UDN isolation - Multicast ACLs</p> <p>More features are planned to be added in the future. </p>"},{"location":"observability/ovn-observability/#motivation","title":"Motivation","text":"<p>Networking observability is an important feature to verify the expected networking behavior in a cluster and to debug existing problems. Ovn-kubernetes makes use of many abstraction layers (through NBDB, logical flows, openflow flows and datapath flows)  that translate kubernetes feature into very specific rules that apply  to each packet in the network. Therefore, even though there are ways to see what OVS/OVN is doing with a particular packet,  there is no way to know why.</p> <p>We aim to solve this problem by providing a way for ovn-kubernetes to generate packet samples enriched with metadata  that can be easily correlated back to kubernetes objects or other human-readable pieces of information that provide  insights of what ovn-kubernetes is doing with a packet and why.</p>"},{"location":"observability/ovn-observability/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<ul> <li>As a user I want to make sure that the network policies/egress firewalls/etc. are correctly enforced in my cluster.</li> <li>As a cluster admin I want to check why some traffic is allowed or dropped.</li> </ul>"},{"location":"observability/ovn-observability/#how-to-enable-this-feature-on-an-ovn-kubernetes-cluster","title":"How to enable this feature on an OVN-Kubernetes cluster?","text":"<p>To enable this feature, use <code>--observability</code> flag with <code>kind.sh</code> script or <code>--enable-observability</code> flag with <code>ovnkube</code> binary.</p> <p>To see the samples, use <code>ovnkube-observ</code> binary, use <code>-h</code> to see allowed flags.</p> <p>This feature requires OVS 3.4 and linux kernel 6.11.</p> <p>As of Aug 2024, the kernel need to be built from the source, therefore to try this feature you need to: - rebuild the kernel with the current master branch from Linus' tree   - to rebuild on fedora: https://docs.fedoraproject.org/en-US/quick-docs/kernel-build-custom/#_building_a_vanilla_upstream_kernel - Build an ovn-kubernetes image that uses the latest OVS/OVN code: <code>OVS_BRANCH=main make -C dist/images fedora-dev-local-gw-deployment</code> - Start kind with that image, use <code>-ov localhost/ovn-daemonset-fedora:latest</code> flag with <code>kind.sh</code> script.</p>"},{"location":"observability/ovn-observability/#workflow-description","title":"Workflow Description","text":"<ul> <li>Observability is enabled by setting the <code>--enable-observability</code> flag in the <code>ovnkube</code> binary.</li> <li>For now all mentioned features are enabled by this flag at the same time.</li> <li><code>ovnkube-observ</code> binary is used to see the samples. Samples are only generated when the real traffic matching the ACLs is sent through the OVS. An example output is: <pre><code>OVN-K message: Allowed by default allow from local node policy, direction ingress\nsrc=10.129.2.2, dst=10.129.2.5\n</code></pre></li> </ul>"},{"location":"observability/ovn-observability/#implementation-details","title":"Implementation Details","text":""},{"location":"observability/ovn-observability/#user-facing-api-changes","title":"User facing API Changes","text":"<p>No API changes were done.</p>"},{"location":"observability/ovn-observability/#ovn-sampling-details","title":"OVN sampling details","text":"<p>OVN has 3 main db tables that are used for sampling: - <code>Sample_collector</code>: This table is used to define the sampling collector. It defines the sampling rate via <code>Probability</code> field and collectorID via <code>SetID</code> field, which is used to set up collectors in the OVS.  - <code>Sampling_app</code>: This table is used to set <code>ID</code>s for existing OVN sampling applications, that are sent together with the samples. There is a supported set of <code>Sampling_app</code> types, for example <code>acl-new</code> app is used to sample new connections matched by an ACL. <code>Sampling_app.ID</code> is a way to identify the application that generated the sample. - <code>Sample</code>: This table is used to define required samples and point to the collectors.  Every sample has <code>Metadata</code> that is sent together with the sample.</p> <p>Samples are attached to the other db tables, for now only to ACLs. A sample is generated when a packet matches the ACL. Every Sample contains <code>Sampling_app.ID</code> and <code>Sample.Metadata</code>, that is decoded by <code>go-controller/observability-lib</code>.</p>"},{"location":"observability/ovn-observability/#ovn-kubernetes-implementation-details","title":"OVN-Kubernetes Implementation Details","text":"<p><code>Sample_collector</code> and <code>Sampling_app</code> are created or cleaned up when the observability is enabled/disabled on startup. When one of the supported objects (for example, network policy) is created, ovn-kuberentes generates an nbdb <code>Sample</code> for it.</p> <p>To decode the samples into human-readable information, <code>go-controller/observability-lib</code> is used. It finds <code>Sample</code> by the attached <code>Sample.Metadata</code> and then gets corresponding db object (e.g. ACL) based on <code>Sampling_app.ID</code> and <code>Sample.UUID</code>. The message is then constructed using db object (e.g. ACL) <code>external_ids</code>.</p> <p></p> <p>The diagram shows how all involved components (kernel, OVS, OVN, ovn-kubernetes) are connected.</p>"},{"location":"observability/ovn-observability/#enabling-collectors","title":"Enabling collectors","text":"<p>Currently, we have only 1 default collector with hard-coded ID, which is set via the <code>Sample_collector.SetID</code> field. To make OVS start sending samples for an existing <code>Sample_collector</code>, a new OVSDB <code>Flow_Sample_Collector_Set</code> entry needs to be created with <code>Flow_Sample_Collector_Set.ID</code> value of <code>Sample_collector.SetID</code>.  This is done by the <code>go-controller/observability-lib</code> and it is important to note that only one <code>Flow_Sample_Collector_Set</code> should be created for a given <code>Sample_collector.SetID</code> value at a time. But if such entry already exists, it can be reused.</p>"},{"location":"observability/ovn-observability/#best-practices","title":"Best Practices","text":"<p>TDB</p>"},{"location":"observability/ovn-observability/#future-items","title":"Future Items","text":"<p>Add more features support, for example, egress IP or load balancing.</p>"},{"location":"observability/ovn-observability/#known-limitations","title":"Known Limitations","text":"<p>Current version of <code>ovnkube-observ</code> only works in OVN-IC mode, as it requires <code>nbdb</code> to be available locally via unix socket. In the future non-IC will also be supported with provided <code>nbdb</code> address and certificates.</p> <p>Only default network observability is supported for now, secondary-network observability will be added later.</p> <p>Sample ID for ACL is stored in conntrack when the new session is established and is never updated until the session is closed. That means, some samples may be removed from nbdb, but still be present in the generated samples. It implies: - ACL-based sampling only affects newly established connections: if a session was already established before the sampling was enabled, the session will not be sampled. - If a session is established with enabled sampling, disabling sampling won't affect that session, and it will continue generating samples until the session is closed. - If the sample was removed from nbdb (e.g. when sampling is disabled for a given connection or when ACL is updated on network policy update or delete) generated samples won't be decoded, as required data is not present in nbdb anymore.</p> <p>Due to OVN limitations, some samples can only be generated on the first packet of a connection. This applies to  - egress firewall, as it doesn't submit a flow to conntrack. - multiple ACLs on the same direction, as only last-tier ACL will be submitted to conntrack. For now this applies to    - ANP + network policy   - ANP + BANP </p> <p>in both cases ANP will have only first-packet sample.</p> <p>Use caution when running the <code>ovnkube-observe</code> tool. Currently it has poor resource management and consumes a lot of  CPU when many packets are sent. Tracked here https://github.com/ovn-kubernetes/ovn-kubernetes/issues/5203</p>"},{"location":"observability/ovn-observability/#references","title":"References","text":"<p>NONE</p>"},{"location":"observability/sdn-dashboard/","title":"SDN Dashboards","text":"<p>This helm chart installs the dashboards for ovn-kubernetes. The dashboards are installed for the following SDN components:   - OVN Central / North Daemon   - OVN Central / Northbound DB   - OVN Central / Southbound DB   - OVN Host / Controller   - Host / OVS   - OVN K8S / Node Agent   - OVN K8S / Cluster Manager</p> <pre><code>helm install sdn-dashboard sdn-dashboard/\n</code></pre>"},{"location":"observability/sdn-dashboard/#values","title":"Values","text":"Key Type Default Description global.enableDPUDashboards bool <pre>\nfalse\n</pre> Displays DPU panels in the dashboards if set to true global.namespace string <pre>\n\"monitoring\"\n</pre> Namespace where the dashboards are installed. Same as the namespace where prometheus and grafana are installed. <p>Before, installing this helm chart, prometheus and grafana must be pre-installed.</p>"},{"location":"observability/sdn-dashboard/#installing-prometheus-grafana-helm-chart","title":"Installing prometheus-grafana helm chart","text":"<p>The kube-prometheus-stack helm chart installs prometheus and grafana.</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n &lt;desired_namespace&gt; --set prometheusOperator.tls.enabled=false --set prometheusOperator.admissionWebhooks.enabled=false --set prometheusOperator.admissionWebhooks.patch.enabled=false\n</code></pre> <p>By default this chart installs additional, dependent charts: - prometheus-community/kube-state-metrics - prometheus-community/prometheus-node-exporter - grafana/grafana</p> <p>After configuring Prometheus and Grafana and installing sdn-dashboard helm chart, ovn-kubernetes can be installed and the metrics will be scraped.</p>"},{"location":"observability/sdn-dashboard/#port-forwarding-to-access-grafana-and-prometheus-ui","title":"Port Forwarding to access Grafana and Prometheus UI","text":"<pre><code>kubectl -n &lt;desired_namespace&gt; port-forward deployment/kube-prometheus-stack-grafana 3000:3000 --address 0.0.0.0\n\nkubectl -n &lt;desired_namespace&gt; port-forward prometheus-kube-prometheus-stack-prometheus-0 9090:9090 --address 0.0.0.0\n</code></pre>"},{"location":"observability/sdn-dashboard/#credentials-to-access-the-dashboard","title":"Credentials to access the dashboard","text":"<p>Username: <code>admin</code></p> <p>Password: <code>prom-operator</code></p>"},{"location":"okeps/okep-4368-template/","title":"OKEP-4368: OKEP Template","text":"<ul> <li>Issue: #4368 (Every OKEP must have an associated enhancement tracking issue that is added to the ovn-kubernetes repo, so please open one if you don't have one yet. Then use the Issue number xxxx as the unique number for the OKEP-xxxx number as well as use the same file naming convention as used in this template. So the OKEP file name must be <code>okep-xxxx-title.md</code> - The goal is that that github issue will have disucssion details about this feature - using meeting notes; slack threads for discussions is not desired as preserving history is hard)</li> </ul>"},{"location":"okeps/okep-4368-template/#problem-statement","title":"Problem Statement","text":"<p>(1-2 sentence summary of the problem we are trying to solve here)</p>"},{"location":"okeps/okep-4368-template/#goals","title":"Goals","text":"<p>(Bullet list of Primary goals of this proposal.)</p>"},{"location":"okeps/okep-4368-template/#non-goals","title":"Non-Goals","text":"<p>(Bullet list of What is explicitly out of scope for this proposal.)</p>"},{"location":"okeps/okep-4368-template/#introduction","title":"Introduction","text":"<p>(Can link to external doc -- but we should bias towards copying the content into the OKEP as online documents are easier to lose -- e.g. owner messes up the permissions, accidental deletion) Give a good detailed introduction to the problem including the ecosystem information</p>"},{"location":"okeps/okep-4368-template/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>(What new user-stories/use-cases does this OKEP introduce?)</p> <p>A user story should typically have a summary structured this way:</p> <ol> <li>As a [user concerned by the story]</li> <li>I want [goal of the story]</li> <li>so that [reason for the story]</li> </ol> <p>The \u201cso that\u201d part is optional if more details are provided in the description. A story can also be supplemented with examples, diagrams, or additional notes.</p> <p>e.g</p> <p>Story 1: Deny traffic at a cluster level</p> <p>As a cluster admin, I want to apply non-overridable deny rules to certain pod(s) and(or) Namespace(s) that isolate the selected resources from all other cluster internal traffic.</p> <p>For Example: The admin wishes to protect a sensitive namespace by applying an AdminNetworkPolicy which denies ingress from all other in-cluster resources for all ports and protocols.</p>"},{"location":"okeps/okep-4368-template/#proposed-solution","title":"Proposed Solution","text":"<p>What is the proposed solution to solve the problem statement?</p>"},{"location":"okeps/okep-4368-template/#api-details","title":"API Details","text":"<p>(... details, can point to PR PoC with changes but this section has to be explained in depth including details about each API field and validation details)</p> <ul> <li>add details if ovnkube API is changing</li> </ul>"},{"location":"okeps/okep-4368-template/#implementation-details","title":"Implementation Details","text":"<p>(... details on what changes will be made to ovnkube to achieve the proposal; go as deep as possible; use diagrams wherever it makes sense)</p> <ul> <li>add details for differences between default mode and interconnect mode if any</li> <li>add details for differences between lgw and sgw modes if any</li> <li>add config knob details if any</li> </ul>"},{"location":"okeps/okep-4368-template/#testing-details","title":"Testing Details","text":"<ul> <li>Unit Testing details</li> <li>E2E Testing details</li> <li>API Testing details</li> <li>Scale Testing details</li> <li>Cross Feature Testing details - coverage for interaction with other features</li> </ul>"},{"location":"okeps/okep-4368-template/#documentation-details","title":"Documentation Details","text":"<ul> <li>New proposed additions to ovn-kubernetes.io for end users to get started with this feature</li> <li>when you open an OKEP PR; you must also edit https://github.com/ovn-org/ovn-kubernetes/blob/13c333afc21e89aec3cfcaa89260f72383497707/mkdocs.yml#L135 to include the path to your new OKEP (i.e Feature Title: okeps/)"},{"location":"okeps/okep-4368-template/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-4368-template/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":"<p>which version is this feature planned to be introduced in? check repo milestones/releases to get this information for when the next release is planned for</p>"},{"location":"okeps/okep-4368-template/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>(Describe any backwards compatibility considerations for this feature. This should include any changes to the API, datapath, or other components that may be backwards incompatible.)</p>"},{"location":"okeps/okep-4368-template/#alternatives","title":"Alternatives","text":"<p>(List other design alternatives and why we did not go in that direction)</p>"},{"location":"okeps/okep-4368-template/#references","title":"References","text":"<p>(Add any additional document links. Again, we should try to avoid too much content not in version control to avoid broken links)</p>"},{"location":"okeps/okep-4380-network-qos/","title":"OKEP-4380: Network QoS Support","text":"<ul> <li>Issue: #4380</li> </ul>"},{"location":"okeps/okep-4380-network-qos/#problem-statement","title":"Problem Statement","text":"<p>The workloads running in Kubernetes, using OVN-Kubernetes (OVN-K8s) as a network plugin, might have requirements in how their network traffic must be handled/differentiated compared to other workloads within the same namespace or different namespaces. For example video streaming application needs low latency and jitter whereas storage applications can tolerate packet loss. Hence, enforcing fair share use of NIC's bandwidth on a K8s Node is essential in meeting these SLAs to provide better service quality.</p> <p>Furthermore, some services in-zone (Physical Gateway to Internet) or in-cluster (Internet Gateway Pods) would like to identify network traffic to provide differentiated services. To achieve this, it is necessary to mark packets on the wire, enabling these services to apply differential treatment.  OVN natively supports DSCP (Differentiated Services Code Point), 6-bit field in IP header, marking on IP packets based on arbitrary match criteria for a logical switch.</p>"},{"location":"okeps/okep-4380-network-qos/#goals","title":"Goals","text":"<ol> <li>Provide a mechanism for users to set DSCP marking on egress east/west (pod to pod overlay)    traffic and egress north/south (pod to external underlay) traffic.</li> <li>Provide a mechanism for users to set Metering on egress east/west and egress north/south    traffic on the NIC on the K8s Node.</li> <li>Provide above mechanisms on all networks attached to K8s Pods.</li> </ol>"},{"location":"okeps/okep-4380-network-qos/#non-goals","title":"Non-Goals","text":"<ol> <li>Ingress Network QoS.</li> <li>Consolidating with current <code>kubernetes.io/egress-bandwidth</code> and <code>kubernetes.io/ingress-bandwidth</code>    annotations. Nonetheless, the work done here does not interfere with the current bandwidth      enforcement mechanisms.</li> <li>How the DSCP marking is handled by the physical network fabric is out-of-scope. It could be that    the fabric could completely ignore the marking.</li> </ol>"},{"location":"okeps/okep-4380-network-qos/#introduction","title":"Introduction","text":"<p>There are several techniques to ensure Quality of Service (QoS) for workloads running in a Kubernetes (K8s) cluster. One method involves traffic policing or metering, where traffic is regulated on the NIC based on a configured rate and burst limit. Any traffic exceeding the limit is dropped. This metering capability is natively supported by OVN through OVS Meters. Another method is traffic shaping, where excess traffic is buffered and transmitted later when bandwidth becomes available, at the cost of higher latency. However, this traffic shaping technique is not supported by OVN and, consequently, cannot be implemented in the OVN-K8s Network plugin. Since network virtualization in OVN occurs on the K8s node (also known as the OVN chassis), bandwidth enforcement for matched traffic occurs on the node itself. This allows OVN-K8s to provide API to regulate NIC's bandwidth between the workloads running on the same or different namespaces within a K8s node, even before the traffic reaches network fabric through the overlay. Additionally, the network fabric can do its own regulation of network bandwidth, however how it is done is outside the scope of this proposal.</p> <p>Another strategy for providing differential treatment to workload network traffic involves marking packets using DSCP (a 6-bit field in the IP header). These marked packets can then be handled differently by in-zone and in-cluster services. OVN supports this packet marking capability through OVS, allowing traffic to be classified based on specific match criteria. OVN marks the inner packet's IP header. So, the marking appears inside the GENEVE tunnel. There are ways to transfer this marking to outer header and influence how the underlay network fabric should handle such packets, however that is outside the scope of this proposal.</p> <p>Kubernetes offers partial support for QoS features through annotations such as <code>kubernetes.io/egress-bandwidth</code> and <code>kubernetes.io/ingress-bandwidth</code> at the Pod interface level. However, these annotations lack fine-grained control, as they cannot target specific types of traffic (e.g., video streaming) on an interface. The Network Plumbing Working Group (NPWG) has extended these annotations to secondary networks, but they remain limited to interface-level configurations without options for selecting a particular traffic flow. Additionally, Kubernetes currently lacks an API for DSCP packet marking.</p> <p>To address these limitations, this proposal introduces a NetworkQoS API that enables fine-grained bandwidth enforcement and packet marking across all interfaces within a Pod.</p> <p>The proposed solution works out-of-the box for the case where a node belongs to a single tenant and the tenant's namespace admin sets the NetworkQos for all the Pods landing on that Node. Say, a node is shared by more than one tenant (not a common scenario) and two tenant namespace admins compete with each other on setting the egress bandwidth limit. In this case, the K8s provider will have to resort to AdmissionWebhooks to either restrict the values that the tenant namespace admin can use or inject a default NetworkQos object in the respective namespaces with predefined values.</p>"},{"location":"okeps/okep-4380-network-qos/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":""},{"location":"okeps/okep-4380-network-qos/#story-1","title":"Story 1","text":"<pre><code>+---------------------+                                                                                                    \n|NS1/Pod1 (paid user) +--DSCP:20                                                                                           \n+---------------------+     |         .-----.----------------------------.-----.    +--------------------+      .-------.  \n                            +------&gt; ;       :     Overlay Traffic      ;       :   |NS3/Internet Gateway|     /         \\ \n                                     :       ;  Various DSCP marking    :       ---&gt;|   Forward + SNAT   +---&gt;( Internet  )\n                         DSCP:11---&gt;  \\     /                            \\     /    |    to Underlay     |     `.       ,' \n+---------------------+     |          `---'------------------------------`---'     +--------------------+       `-----'   \n|NS1/Pod2 (free user) +-----+                   .-------------------.                                                      \n+---------------------+                   _.---'                     `----.                                                \n                                         /        Physical Underlay        \\                                               \n                                        (     (unaware of DSCP marking)     )                                              \n                                         `.                               ,'                                               \n                                           `----.                   _.---'                                                 \n                                                 `-----------------'                                                                                                                                                    \n</code></pre> <p>As a K8s Namespace Admin, I want to configure DSCP marking for egress east-west overlay traffic so that the packet marking is carried from the source overlay pod to destination overlay pod, so that on the destination pod can treat the incoming traffic differently.</p> <p>For example: In the diagram above, Say Pod1 is a paid cloud gaming user and Pod2 is a free cloud gaming user.  I want these two Pods to be treated differently by the InternetGateway application Pod. The packets leaving Pods Pod1 and Pod2 will be marked with DSCP value of 20 and 11 respectively. This marking will be retained on the overlay across the fabric and arrive at the InternetGateway pod where the packets from the free user will be subjected queueing during peak times as compared to the paid user who will not be subjected to any sort of queueing.</p> <p>The namespace admin have the flexibility to define how they utilize the 6-bit DSCP field to meet their specific needs for client/server traffic. They own both the client and server applications.</p> <p>The end user is an individual who wants to play games in the cloud. The individual is a consumer of gaming services.</p>"},{"location":"okeps/okep-4380-network-qos/#story-2","title":"Story 2","text":"<p>As a K8s Namespace Admin, I want to enforce egress bandwidth limit (rate and burst) on the east/west and north/south traffic emanating from the Pods on the same K8s node so that they use the underlying NIC fairly.</p> <p>In the same diagram above, I want to limit the egress bandwidth from Pod2 where a free-user is present to not exceed 1Mbps rate and 1Mbps burst. However, the paid-user might not have any such limitations.</p>"},{"location":"okeps/okep-4380-network-qos/#story-3","title":"Story 3","text":"<p>As a K8s Namespace Admin, I want to define a catch-all NetworkQoS for all my Pods, and then have a more specific NetworkQos for few Pods. As such, I need priorities to define this.</p> <p>In the above diagram, I want all the Internet bound traffic from NS1/Pod1 and NS1/Pod2 to be bandwidth limited to 10Mbps. However, from the same set of Pods I want all the AWS S3 related traffic to be bandwidth limited to 100Mbps.</p> <p>So, the namespace admin can create a catch-all NetworkQoS at priority 1 for all the Pods in NS1 heading towards the Internet and create another NetworkQos at priority 2 to increase the egress bandwidth limit to AWS S3 IPs.</p>"},{"location":"okeps/okep-4380-network-qos/#proposed-solution","title":"Proposed Solution","text":"<p>The current EgressQoS is a namespace-scoped feature that enables DSCP marking for pod's egress traffic directed towards dstCIDR. A namespace supports only one EgressQoS resource, named default (any additional EgressQoS resources will be ignored). This enhancement proposes a replacement for EgressQoS. By introducing a new CRD <code>NetworkQoS</code>, users could specify a DSCP value for packets originating from pods on a given namespace heading to a specified Namespace Selector, Pod Selector, CIDR, Protocol and Port. This also supports metering for the packets by specifying bandwidth parameters <code>rate</code> and/or <code>burst</code>. The <code>priority</code> field enables one to define overlapping rules such that the rule with higher priority (match could be generic) will override the rule with lower priority (match will be specific. See: Story-3).</p> <p>The CRD will be Namespaced, with multiple resources allowed per namespace. The resources will be watched by OVN-K8s, which in turn will configure OVN's QoS Table. The <code>NetworkQoS</code> also has <code>status</code> field which is populated by OVN-K8s which helps users to identify whether NetworkQoS rules are configured correctly in OVN or not.</p>"},{"location":"okeps/okep-4380-network-qos/#api-details","title":"API Details","text":"<ul> <li>A new API <code>NetworkQoS</code> under the <code>k8s.ovn.org/v1alpha1</code> group will be added to <code>go-controller/pkg/crd/networkqos/v1alpha1</code>. This would be a namespace-scoped CRD:</li> </ul> <pre><code>import (\n    networkingv1 \"k8s.io/api/networking/v1\"\n    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\n    crdtypes \"github.com/ovn-org/ovn-kubernetes/go-controller/pkg/crd/types\"\n)\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// +kubebuilder:resource:path=networkqoses\n// +kubebuilder::singular=networkqos\n// +kubebuilder:object:root=true\n// +kubebuilder:printcolumn:name=\"Status\",type=string,JSONPath=\".status.status\"\n// +kubebuilder:subresource:status\n// NetworkQoS is a CRD that allows the user to define a DSCP marking and metering\n// for pods ingress/egress traffic on its namespace to specified CIDRs,\n// protocol and port. Traffic belong these pods will be checked against\n// each Rule in the namespace's NetworkQoS, and if there is a match the traffic\n// is marked with relevant DSCP value and enforcing specified policing\n// parameters.\ntype NetworkQoS struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   Spec   `json:\"spec,omitempty\"`\n    Status Status `json:\"status,omitempty\"`\n}\n\n// Spec defines the desired state of NetworkQoS\ntype Spec struct {\n    // networkSelector selects the networks on which the pod IPs need to be added to the source address set.\n    // NetworkQoS controller currently supports `NetworkAttachmentDefinitions` type only.\n    // +optional\n    // +kubebuilder:validation:XValidation:rule=\"self == oldSelf\", message=\"networkSelector is immutable\"\n    NetworkSelectors crdtypes.NetworkSelectors `json:\"networkSelectors,omitempty\"`\n\n    // podSelector applies the NetworkQoS rule only to the pods in the namespace whose label\n    // matches this definition. This field is optional, and in case it is not set\n    // results in the rule being applied to all pods in the namespace.\n    // +optional\n    PodSelector metav1.LabelSelector `json:\"podSelector,omitempty\"`\n\n    // priority is a value from 0 to 100 and represents the NetworkQoS' priority.\n    // QoSes with numerically higher priority takes precedence over those with lower.\n    // +kubebuilder:validation:Maximum:=100\n    // +kubebuilder:validation:Minimum:=0\n    Priority int `json:\"priority\"`\n\n    // egress a collection of Egress NetworkQoS rule objects. A total of 20 rules will\n    // be allowed in each NetworkQoS instance. The relative precedence of egress rules\n    // within a single NetworkQos object (all of which share the priority) will be\n    // determined by the order in which the rule is written. Thus, a rule that appears\n    // first in the list of egress rules would take the lower precedence.\n    // +kubebuilder:validation:MaxItems=20\n    Egress []Rule `json:\"egress\"`\n}\n\ntype Rule struct {\n    // dscp marking value for matching pods' traffic.\n    // +kubebuilder:validation:Maximum:=63\n    // +kubebuilder:validation:Minimum:=0\n    DSCP int `json:\"dscp\"`\n\n    // classifier The classifier on which packets should match\n    // to apply the NetworkQoS Rule.\n    // This field is optional, and in case it is not set the rule is applied\n    // to all egress traffic regardless of the destination.\n    // +optional\n    Classifier Classifier `json:\"classifier\"`\n\n    // +optional\n    Bandwidth Bandwidth `json:\"bandwidth\"`\n}\n\ntype Classifier struct {\n    // +optional\n    To []Destination `json:\"to\"`\n\n    // +optional\n    Ports []*Port `json:\"ports\"`\n}\n\n// Bandwidth controls the maximum of rate traffic that can be sent\n// or received on the matching packets.\ntype Bandwidth struct {\n    // rate The value of rate limit in kbps. Traffic over the limit\n    // will be dropped.\n    // +kubebuilder:validation:Minimum:=1\n    // +kubebuilder:validation:Maximum:=4294967295\n    // +optional\n    Rate uint32 `json:\"rate\"`\n\n    // burst The value of burst rate limit in kilobits.\n    // This also needs rate to be specified.\n    // +kubebuilder:validation:Minimum:=1\n    // +kubebuilder:validation:Maximum:=4294967295\n    // +optional\n    Burst uint32 `json:\"burst\"`\n}\n\n// Port specifies destination protocol and port on which NetworkQoS\n// rule is applied\ntype Port struct {\n    // protocol (tcp, udp, sctp) that the traffic must match.\n    // +kubebuilder:validation:Pattern=^TCP|UDP|SCTP$\n    // +optional\n    Protocol string `json:\"protocol\"`\n\n    // port that the traffic must match\n    // +kubebuilder:validation:Minimum:=1\n    // +kubebuilder:validation:Maximum:=65535\n    // +optional\n    Port *int32 `json:\"port\"`\n}\n\n// Destination describes a peer to apply NetworkQoS configuration for the outgoing traffic.\n// Only certain combinations of fields are allowed.\n// +kubebuilder:validation:XValidation:rule=\"!(has(self.ipBlock) &amp;&amp; (has(self.podSelector) || has(self.namespaceSelector)))\",message=\"Can't specify both podSelector/namespaceSelector and ipBlock\"\ntype Destination struct {\n    // podSelector is a label selector which selects pods. This field follows standard label\n    // selector semantics; if present but empty, it selects all pods.\n    //\n    // If namespaceSelector is also set, then the NetworkQoS as a whole selects\n    // the pods matching podSelector in the Namespaces selected by NamespaceSelector.\n    // Otherwise it selects the pods matching podSelector in the NetworkQoS's own namespace.\n    // +optional\n    PodSelector *metav1.LabelSelector `json:\"podSelector,omitempty\" protobuf:\"bytes,1,opt,name=podSelector\"`\n\n    // namespaceSelector selects namespaces using cluster-scoped labels. This field follows\n    // standard label selector semantics; if present but empty, it selects all namespaces.\n    //\n    // If podSelector is also set, then the NetworkQoS as a whole selects\n    // the pods matching podSelector in the namespaces selected by namespaceSelector.\n    // Otherwise it selects all pods in the namespaces selected by namespaceSelector.\n    // +optional\n    NamespaceSelector *metav1.LabelSelector `json:\"namespaceSelector,omitempty\" protobuf:\"bytes,2,opt,name=namespaceSelector\"`\n\n    // ipBlock defines policy on a particular IPBlock. If this field is set then\n    // neither of the other fields can be.\n    // +optional\n    IPBlock *networkingv1.IPBlock `json:\"ipBlock,omitempty\" protobuf:\"bytes,3,rep,name=ipBlock\"`\n}\n\n// Status defines the observed state of NetworkQoS\ntype Status struct {\n    // A concise indication of whether the NetworkQoS resource is applied with success.\n    // +optional\n    Status string `json:\"status,omitempty\"`\n\n    // An array of condition objects indicating details about status of NetworkQoS object.\n    // +optional\n    // +patchMergeKey=type\n    // +patchStrategy=merge\n    // +listType=map\n    // +listMapKey=type\n    Conditions []metav1.Condition `json:\"conditions,omitempty\" patchStrategy:\"merge\" patchMergeKey:\"type\"`\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// +kubebuilder:resource:path=networkqoses\n// +kubebuilder::singular=networkqos\n// NetworkQoSList contains a list of NetworkQoS\ntype NetworkQoSList struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ListMeta `json:\"metadata,omitempty\"`\n    Items           []NetworkQoS `json:\"items\"`\n}\n</code></pre>"},{"location":"okeps/okep-4380-network-qos/#implementation-details","title":"Implementation Details","text":"<p>The new controller is introduced in OVN-Kubernetes which would watch <code>NetworkQoS</code>, <code>Pod</code> and <code>Node</code> objects, which will create the relevant NetworkQoS objects and attach them to all the node local switches in the cluster in OVN - resulting in the necessary flows to be programmed in OVS.</p> <p>In order to not create an OVN NetworkQoS object per pod in the namespace, the controller will also manage AddressSets. For each QoS rule specified in a given <code>NetworkQoS</code> it'll create an AddressSet, adding only the pods whose label matches the PodSelector to it, making sure that new/updated/deleted matching pods are also added/updated/deleted accordingly. Rules that do not have a PodSelector will leverage the namespace's AddressSet.</p> <p>Similarly, when <code>NetworkQoS</code> is created for Pods secondary network, OVN-K8s must create a new AddressSet for every QoS rule. When no pod selector is specified, then it must contain all the pod's IP addresses that belong to the namespace and selected network. If only a set of pods are chosen via podSelector, then it must have IP addresses only for chosen pod(s).</p> <p>For example, assuming there's a single node <code>node1</code> and the following <code>NetworkQoS</code> (maps to the Story-1 above) is created:</p> <pre><code>kind: NetworkQoS\napiVersion: k8s.ovn.org/v1alpha1\nmetadata:\n  name: qos-external-paid\n  namespace: games\nspec:\n  podSelector:\n    matchLabels:\n      user-type: paid\n  priority: 1\n  egress:\n    - dscp: 20\n      classifier:\n        to:\n        - ipBlock:\n            cidr: 0.0.0.0/0\n            except:\n            - 10.0.0.0/8\n            - 172.16.0.0/12\n            - 192.168.0.0/16\n\n---\nkind: NetworkQoS\napiVersion: k8s.ovn.org/v1alpha1\nmetadata:\n  name: qos-external-free\n  namespace: games\nspec:\n  podSelector:\n    matchLabels:\n      user-type: free\n  priority: 2\n  egress:\n    - dscp: 11\n      classifier:\n        to:\n          - ipBlock:\n              cidr: 0.0.0.0/0\n              except:\n                - 10.0.0.0/8\n                - 172.16.0.0/12\n                - 192.168.0.0/16\n</code></pre> <p>the equivalent of:</p> <pre><code>ovn-nbctl qos-add node1 to-lport 10020 \"ip4.src == &lt;games-qos-external-paid address set&gt; &amp;&amp; ip4.dst != {10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16}\" dscp=20\novn-nbctl qos-add node1 to-lport 10040 \"ip4.src == &lt;games-qos-external-free address set&gt; &amp;&amp; ip4.dst != {10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16}\" dscp=11\n</code></pre> <p>will be executed. The math for the priority is as described below:</p> <ol> <li>we want to save the first 10K OVN priorities for future use.</li> <li>we evaluate the priority based on the fact that we allow only 20 rules per QoS object, and we use the index of the rule within the object    So, 10020 was derived like so =&gt; 10000 + NetworkQoS.priority * 20 + index(rule) =&gt; 10000 + 1 * 20 + 0 =&gt; 10020    S0, 10040 was derived like so =&gt; 10000 + 2 * 20 + 0</li> </ol> <p>Creating a new Pod in games namespace that matches the podSelector in either <code>qos-external-paid</code> or <code>qos-external-free</code> results in its IPs being added to the corresponding Address Set.</p> <p>Following example maps to the Story-2 above. It updates the above NetworkQoS objects to include the bandwidth fields.  <pre><code>kind: NetworkQoS\napiVersion: k8s.ovn.org/v1alpha1\nmetadata:\n  name: qos-external-free\n  namespace: games\nspec:\n  podSelector:\n    matchLabels:\n      user-type: free\n  priority: 2\n  egress:\n    - dscp: 11\n      bandwidth:\n        burst: 1000000 # in kbps\n        rate: 1000000  # in kbps      \n      classifier:\n        to:\n          - ipBlock:\n              cidr: 0.0.0.0/0\n              except:\n                - 10.0.0.0/8\n                - 172.16.0.0/12\n                - 192.168.0.0/16\n</code></pre></p> <p>In the above <code>qos-external-free</code> NetworkQoS example, all the pods in games namespace with <code>user-type: free</code> label will have bandwidth limited to specified burst/rate towards the Internet. Such traffic will also have DSCP marking of 11. The equivalent of:</p> <pre><code>ovn-nbctl qos-add node1 to-lport 10040 \"ip4.src == &lt;games-qos-external-free address set&gt; &amp;&amp; ip4.dst != {10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16}\" rate=20000 burst=100 dscp=11\n</code></pre> <p>will be executed.</p> <p>In addition, the controller will watch nodes to decide if further updates are needed, for example: when another node <code>node2</code> joins the cluster, the controller will attach the existing <code>NetworkQoS</code> object to its node local switch.</p> <p>The <code>NetworkQoS</code> is supported on pod's secondary networks. That may also be a User Defined Network. Consider the following example:</p> <pre><code>kind: NetworkQoS\napiVersion: k8s.ovn.org/v1alpha1\nmetadata:\n  name: qos-external-free\n  namespace: games\nspec:\n  networkSelectors:\n    - networkSelectionType: NetworkAttachmentDefinitions\n      networkAttachmentDefinitionSelector:\n        namespaceSelector:\n          matchLabels: {}   # Empty selector will select all namespaces\n        networkSelector:\n          matchLabels:\n            name: ovn-storage\n  priority: 2\n  egress:\n    - dscp: 11\n      classifier:\n        to:\n          - ipBlock:\n              cidr: 0.0.0.0/0\n        ports:\n        - protocol: TCP\n          port: 80\n        - protocol: TCP\n          port: 443\n</code></pre> <p>This creates a new AddressSet adding default namespace pod(s) IP associated with ovn-storage secondary network, using NAD. The equivalent of:</p> <pre><code>ovn-nbctl qos-add node1 to-lport 10040 \"ip4.src == &lt;games_ovn-storage_network address set&gt; &amp;&amp; ip4.dst == 0.0.0.0/0\" dscp=11\n</code></pre> <p>will be executed.</p> <p>IPv6 will also be supported, given the following <code>NetworkQoS</code>:</p> <pre><code>apiVersion: k8s.ovn.org/v1alpha1\nkind: NetworkQoS\nmetadata:\n  name: default\n  namespace: default\nspec:\n  priority: 3\n  egress:\n    - dscp: 48\n      classifier:\n        to:\n          - ipBlock:\n              cidr: 2001:0db8:85a3:0000:0000:8a2e:0370:7330/124\n</code></pre> <p>and a single pod with the IP <code>fd00:10:244:2::3</code> in the namespace, the controller will create the relevant NetworkQoS object that will result in a similar flow to this on the pod's node:</p> <pre><code> cookie=0x6d99cb18, duration=63.310s, table=18, n_packets=0, n_bytes=0, idle_age=63, priority=555,ipv6,metadata=0x4,ipv6_src=fd00:10:244:2::3,ipv6_dst=2001:db8:85a3::8a2e:370:7330/124 actions=mod_nw_tos:192,resubmit(,19)\n</code></pre>"},{"location":"okeps/okep-4380-network-qos/#testing-details","title":"Testing Details","text":"<ul> <li> <p>Unit tests coverage</p> </li> <li> <p>Validate NetworkQoS <code>status</code> fields are populated correctly.</p> </li> <li> <p>IPv4/IPv6 E2E that validates egress traffic from a namespace is marked with the correct DSCP value   by creating and deleting <code>NetworkQoS</code>, setting up src pods and destination pods.</p> </li> <li>Traffic to the all targeted pod IPs should be marked.</li> <li>Traffic to the targeted pod IPs, Protocol should be marked.</li> <li>Traffic to the targeted pod IPs, Protocol and Port should be marked.</li> <li> <p>Traffic to an pod IP address not contained in the destination pod selector, Protocol and Port     should not be marked.</p> </li> <li> <p>IPv4/IPv6 E2E that validates egress traffic from a namespace is marked with the correct DSCP value   by creating and deleting <code>NetworkQoS</code>, setting up src pods and host-networked destination pods.</p> </li> <li>Traffic to the specified CIDR should be marked.</li> <li>Traffic to the specified CIDR, Protocol should be marked.</li> <li>Traffic to the specified CIDR, Protocol and Port should be marked.</li> <li> <p>Traffic to an address not contained in the CIDR, Protocol and Port should not be marked.</p> </li> <li> <p>IPv4/IPv6 E2E that validates egress traffic from a namespace is enforced with bandwidth limit by   creating and deleting <code>NetworkQoS</code>, setting up src pods and destination pods.</p> </li> <li>Traffic to the all targeted pod IPs should be rate limited with specified bandwidth     parameters.</li> <li>Traffic to the targeted pod IPs, Protocol should be rate limited with specified bandwidth     parameters.</li> <li>Traffic to the targeted pod IPs, Protocol and Port should be rate limited with specified     bandwidth parameters.</li> <li>Traffic to an pod IP address not contained in the destination pod selector, Protocol and Port     should not be rate limited with specified bandwidth parameters.</li> </ul>"},{"location":"okeps/okep-4380-network-qos/#documentation-details","title":"Documentation Details","text":"<p>To be discussed.</p>"},{"location":"okeps/okep-4380-network-qos/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-4380-network-qos/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":"<p>To be discussed.</p>"},{"location":"okeps/okep-4380-network-qos/#alternatives","title":"Alternatives","text":"<p>N/A</p>"},{"location":"okeps/okep-4380-network-qos/#references","title":"References","text":""},{"location":"okeps/okep-5085-localnet-api/","title":"OKEP-5085: Localnet API","text":""},{"location":"okeps/okep-5085-localnet-api/#problem-statement","title":"Problem Statement","text":"<p>As of today one can create a user-defined network over localnet topology using NetworkAttachmentDefinition (NAD). Using NAD for localnet has some pitfalls due to the fact it is not managed and not validated on creation. Misconfigurations are detected too late causing bad UX and frustration for users.</p> <p>Configuring localnet topology networks requires changes to cluster nodes network stack and involves some risk and knowledge that require cluster-admin intervention, such as: configuring the OVS switch to which the localnet network connects,  aligning MTU across the stack and configuring the right VLANs that fit the provider network.</p>"},{"location":"okeps/okep-5085-localnet-api/#goals","title":"Goals","text":"<ul> <li>Enable creating user-defined-networks over localnet topology using OVN-K CUDN CRD.</li> <li>Streamline localnet UX: detect misconfigurations early, provide indications about issues or success.</li> </ul>"},{"location":"okeps/okep-5085-localnet-api/#non-goals","title":"Non-Goals","text":""},{"location":"okeps/okep-5085-localnet-api/#introduction","title":"Introduction","text":"<p>As of today OVN-Kubernetes multi-homing feature  supports creating localnet topology networks and enables connecting workloads to the host network using <code>NetworkAttachmentDefinition</code> (NAD).</p> <p>This proposal introduces a well-formed API on top of the <code>ClusterUserDefinedNetwork</code> CRD.</p> <p>Managing localnet topology networks using a well-formed API could improve UX as it is managed by a controller,  perform validations and reflect the state via status.</p>"},{"location":"okeps/okep-5085-localnet-api/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":""},{"location":"okeps/okep-5085-localnet-api/#definition-of-personas","title":"Definition of personas:","text":"<p>Admin - is the cluster admin. User - non cluster-admin user, project manager. Workloads - pod or KubeVirt VMs.</p> <ul> <li>As an admin I want to create a user-defined network over localnet topology using CUDN CRD.<ul> <li>In case the network configuration is bad I want to get an informative message saying what went wrong.</li> </ul> </li> <li>As an admin I want to enable users to connect workloads in project/namespaces they have permission to, to the localnet network I created for them.</li> <li>As a user I want to be able to connect my workloads (pod/VMs) to the localnet the admin created in my namespace.</li> <li>As a user I want my workloads to be able to communicate with each other over the localnet network.</li> <li>As a user I want my VMs connected to the localnet network to be able to migrate from one node to another, without any changes on the IP address of their localnet network interface.</li> </ul>"},{"location":"okeps/okep-5085-localnet-api/#proposed-solution","title":"Proposed Solution","text":""},{"location":"okeps/okep-5085-localnet-api/#summary","title":"Summary","text":"<p>Extend the CUDN CRD to enable creating user-defined networks over localnet topology. Since the CUDN CRD is targeted for cluster-admin users, it prevents non-admin users from performing changes that could disrupt the cluster or impact the physical network to which the workloads would connect.</p>"},{"location":"okeps/okep-5085-localnet-api/#localnet-using-networkattachmentdefinition","title":"Localnet using <code>NetworkAttachmentDefinition</code>","text":"<p>As of today OVN-K enables multi-homing including localnet topology networks using NADs. The following NAD YAML describes localnet topology configuration and options:</p> <p><pre><code>---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: tenantblue\n  namespace: blue\nspec:\n    config: &gt; '{\n        \"cniVersion\": \"0.3.1\",\n        \"type\": \"ovn-k8s-cni-overlay\"\n        \"netAttachDefName\": \"blue/tenantblue\",\n        \"topology\": \"localnet\",\n        \"name\": \"tenantblue\",#1\n        \"physicalNetworkName\": \"mylocalnet1\", #2\n        \"subnets\": \"10.100.0.0/24\", #3\n        \"excludeSubnets\": \"10.100.50.0/32\", #4\n        \"vlanID\": \"200\", #5\n        \"mtu\": \"1500\", #6\n        \"allowPersistentIPs\": true #7\n    }'\n</code></pre> 1. <code>name</code>    The underlying network name:     - Should match the node OVS bridge-mapping network-name.     - In case Kubernetes-nmstate is used, should match the <code>NodeNetworkConfigurationPolicy</code> (NNCP) <code>spec.desiredState.ovn.bridge-mappings</code> item's <code>localnet</code> attribute value. 2. <code>physicalNetworkName</code>    Points to the node OVS bridge-mapping network-name - the network-name mapped to the node OVS bridge that provides access to that network.    (Can be defined using Kubernetes-nmstate NNCP - <code>spec.desiredState.ovn.bridge-mappings</code>)     - Overrides the <code>name</code> attribute, defined in (1).     - Allows multiple localnet topology NADs to refer to the same bridge-mapping (thus simplifying the admin\u2019s life -       fewer manifests to provision and keep synced). 3. <code>subnets</code>    Subnets to use for the network across the cluster. 4. <code>excludeSubnets</code>    IP addresses ranges to exclude from the assignable IP address pool specified by the <code>subnets</code> field. 5. <code>vlanID</code> - VLAN tag assigned to traffic. 6. <code>mtu</code> - maximum transmission unit for a network 7. <code>allowPersistentIPs</code>    persist the OVN-Kubernetes assigned IP addresses in a <code>ipamclaims.k8s.cni.cncf.io</code> object. These IP addresses will be    reused by other pods if requested. Useful for KubeVirt VMs.</p>"},{"location":"okeps/okep-5085-localnet-api/#extend-clusteruserdefinednetwork-crd","title":"Extend ClusterUserDefinedNetwork CRD","text":"<p>Given the CUDN CRD is targeted at cluster-admin users, it is a good fit for operations that require cluster-admin intervention, such as localnet topology.</p> <p>The suggested solution is to extend the CUDN CRD to enable the creating of localnet topology networks.</p>"},{"location":"okeps/okep-5085-localnet-api/#underlying-network-name","title":"Underlying network name","text":"<p>The underlying network-name represented by the network-config name attribute, the NAD <code>spec.config.name</code> (net-conf-network-name).</p> <p>Given the CUDN API doesn\u2019t expose the net-conf-network-name by design, the localnet topology configuration require the net-conf-network-name  to match an existing OVS bridge-mapping on the node. </p> <p>In case Kubernetes-nmstate is used, the NAD <code>spec.config.name</code> has to match the <code>NodeNetworkConfigurationPolicy</code> <code>spec.desiredState.ovn.bridge-mappings</code> name: <pre><code>spec:\n  desiredState:\n      ovn:\n        bridge-mappings:\n        - localnet: physnet  &lt;---- has to match the NAD `config.spec.name` OR \n                                   the NAD `spec.config.physicalNetworkName`\n          bridge: br-ex &lt;--------- OVS switch\n</code></pre> * To overcome this and to avoid exposing the net-conf-network-name in the CUDN CRD spec, a new field should be introduced.   The new field allows users to point to the bridge-mapping network-name they defined in the node.   The field should be translated to the CNI <code>physicalNetworkName</code> field.</p>"},{"location":"okeps/okep-5085-localnet-api/#workflow-description","title":"Workflow Description","text":"<p>The CUDN CRD controller should be changed accordingly to support localnet topology. It should validate localnet topology configuration and generate corresponding NADs for localnet as with other topologies (Layer2 &amp; Layer3) in the selected namespaces.</p>"},{"location":"okeps/okep-5085-localnet-api/#generating-the-nad","title":"Generating the NAD","text":""},{"location":"okeps/okep-5085-localnet-api/#ovs-bridge-mappings-network-name","title":"OVS bridge-mapping\u2019s network-name","text":"<p>Introduces an attribute that points to the OVS bridge bridge-mapping network-name. This attribute name should be translated to the CNI \u201cphysicalNetworkName\u201d attribute.</p> <p>Proposal for the CUDN spec field name:  \u201cphysicalNetworkName\u201d.</p>"},{"location":"okeps/okep-5085-localnet-api/#mtu","title":"MTU","text":"<p>Should be translated to the CNI \u201cmtu\u201d attribute.</p> <p>By default, OVN-K sets the MTU of the UDN to be 100 bytes less than the physical MTU of the underlay network. For the localnet topology this is not optimal because localnet does not use a Geneve overlay and is directly connected to the underlay. This results in a loss in throughput and potential MTU mismatch issues.</p> <p>The MTU value may be set by the user, and if not set then OVN-Kubernetes will determine the default value to use - 1500 for localnet topology.</p>"},{"location":"okeps/okep-5085-localnet-api/#vlan","title":"VLAN","text":"<p>Should be translated to the CNI \u201cvlanID\u201d attribute. If not specified it should not be present in NAD spec.config.</p>"},{"location":"okeps/okep-5085-localnet-api/#subnets-excludesubnets","title":"Subnets, ExcludeSubnets","text":"<p>The subnets and exclude-subnets should be in CIDR form, similar to Layer2 topology subnets.</p>"},{"location":"okeps/okep-5085-localnet-api/#persistent-ips","title":"Persistent IPs","text":"<p>In a scenario of VMs, migrated VMs should have a persistent IP address to prevent workload disruption.. Localnet topology should allow using persistent IP allowing setting the CNI allowPersistentIPs.</p> <p>As of today, the Layer2 topology configuration API consists of the following stanza, allowing to use persistent IPs, and the localnet topology spec should have the same options: <pre><code>ipam:\n  lifecycle: Persistent\n</code></pre></p> <p>By default, persistent IP is turned off. Should be enabled by setting <code>ipam.lifecycle=Persistent</code>, similar to Layer2 topology.</p>"},{"location":"okeps/okep-5085-localnet-api/#api-details","title":"API Details","text":"<p>The ClusterUserDefinedNetwork CRD should be extended to support localnet topology.</p>"},{"location":"okeps/okep-5085-localnet-api/#cudn-spec","title":"CUDN spec","text":"<p>The CUDN <code>spec.network</code> follows the discriminated union convention. The <code>spec.network.topology</code> serves as the union discriminator, it should accept <code>Localnet</code> option.</p> <p>The API should have validation that ensures <code>spec.network.topology</code> matches the topology configuration, similar to existing validation for other topologies.</p>"},{"location":"okeps/okep-5085-localnet-api/#localnet-topology-spec","title":"Localnet topology spec","text":"Field name Description optional Role Select the network role in the pod. No PhysicalNetworkName The OVN bridge mapping network name is configured on the node. No MTU The maximum transmission unit (MTU). Yes VLAN Discriminated union for VLAN configurations for the network. <code>mode</code>: When set to <code>Access</code>, OVN-Kubernetes applies the VLAN configuration to the network logical switch port in access mode, according to the config.<code>access</code> is the access VLAN configuration.<code>access.id</code> is the VLAN ID (VID) to be set on the logical network switch. Yes Subnets List of CIDRs used for the pod network across the cluster. Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed. The format should match standard CIDR notation (for example, \"10.128.0.0/16\"). This field must be omitted if <code>ipam.mode</code> is <code>Disabled</code>. Yes ExcludeSubnets List of CIDRs removed from the specified CIDRs in <code>subnets</code>.The format should match standard CIDR notation (for example, \"10.128.0.0/16\"). This field must be omitted if <code>subnets</code> is unset or <code>ipam.mode</code> is <code>Disabled</code>. Yes IPAM Contains IPAM-related configuration, similar to Layer2 &amp; Layer3 topologies.Consists of the following fields:<code>mode</code>: When <code>Enabled</code>, OVN-Kubernetes will apply IP configuration to the SDN infra and assign IPs from the selected subnet to the pods.When <code>Disabled</code>, OVN-Kubernetes only assign MAC addresses and provides layer2 communication, enable users configure IP addresses to the pods.<code>lifecycle</code>: When <code>Persistent</code> enable workloads have persistent IP addresses. For example: Virtual Machines will have the same IP addresses along their lifecycle (stop, start migration, reboots)."},{"location":"okeps/okep-5085-localnet-api/#suggested-api-validations","title":"Suggested API validations","text":"<ul> <li><code>Role</code>:<ul> <li>Required.</li> <li>When <code>topology=Localnet</code>, the only allowed value is <code>Secondary</code>.<ul> <li>Having Role explicitly makes the API predictable and consistent with other topologies. In addition, it enables extending localnet to support future role options</li> </ul> </li> </ul> </li> <li><code>PhysicalNetworkName</code>:<ul> <li>Required.</li> <li>Max length 253.</li> <li>Cannot contain <code>,</code> or <code>:</code> characters.</li> </ul> </li> <li><code>MTU</code>:<ul> <li>Minimum 576 (minimal for IPv4). Maximum: 65536.</li> <li>When Subnets consist of IPv6 CIDR, minimum MTU should be 1280.</li> </ul> </li> <li><code>VLAN</code>:</li> <li><code>Mode</code>: Allowed valued is \"Access\".</li> <li><code>ID</code> (<code>access.id</code>)     According to dot1q (IEEE 802.1Q),     VID (VLAN ID) is 12-bits field, providing 4096 values; 0 - 4095.      The VLAN IDs <code>0</code>, and <code>4095</code> are reserved.      Suggested validations:<ul> <li>Minimum: 1, Maximum: 4094.</li> </ul> </li> <li><code>Subnets</code>:<ul> <li>Minimum items 1, Maximum items 2.</li> <li>Items are valid CIDR (e.g.: \"10.128.0.0/16\")</li> <li>When 2 items are specified they must be of different IP families.</li> </ul> </li> <li><code>ExcludeSubnets</code>:<ul> <li>Minimum items 1, Maximum items 25.</li> <li>Items are valid CIDR (e.g.: \"10.128.0.0/16\")</li> <li>Cannot be set when Subnet is unset or <code>ipam.mode=Disabled</code>.</li> <li>Ensure excluded subnet in range of at least one subnet in <code>spec.network.localnet.subnets</code>.</li> <li>Due to a bug in Kubernetes CEL validation IP/CIDR operations this validation can be implemented once the following issue is resolved     https://github.com/kubernetes/kubernetes/issues/130441      The CRD controller should validate excludeSubnets items are in range of specified subnets.      In a case of an invalid request raise an error in the status.</li> </ul> </li> </ul>"},{"location":"okeps/okep-5085-localnet-api/#yaml-examples","title":"YAML examples","text":"<p>Assuming the node has OVS bridge-mapping defined by Kubernetes-nmstate  using the following <code>NodeNetworkConfigurationPolicy</code> (NNCP): <pre><code>...\ndesiredState:\n    ovn:\n      bridge-mappings:\n      - localnet: tenantblue \n        bridge: br-ex\n</code></pre> Example 1: <pre><code>---\napiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: test-net\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"red\", \"blue\"]\n  network:\n    topology: Localnet\n    localnet:\n      role: Secondary\n      physicalNetworkName: tenantblue\n      subnets: [\"192.168.100.0/24\", \"2001:dbb::/64\"]\n      excludeSubnets: [\"192.168.100.1/32\", \"2001:dbb::0/128\"]\n</code></pre> The above CR will make the controller create NAD in each selected namespace: \"red\" and \"blue\". NAD in namespace <code>blue</code>: <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: test-net\n  namespace: blue\nfinalizers:\n - k8s.ovn.org/user-defined-network-protection\nlabels:\n  k8s.ovn.org/user-defined-network: \"\"\nownerReferences:\n- apiVersion: k8s.ovn.org/v1\n  blockOwnerDeletion: true\n  controller: true\n  kind: ClusterUserDefinedNetwork\n  name: test-net\n  uid: 293098c2-0b7e-4216-a3c6-7f8362c7aa61\nspec:\n    config: &gt; '{\n        \"cniVersion\": \"1.0.0\",\n        \"type\": \"ovn-k8s-cni-overlay\"\n        \"netAttachDefName\": \"blue/test-net\",\n        \"role\": \"secondary\",\n        \"topology\": \"localnet\",\n        \"name\": \"cluster.udn.test-net\",\n        \"physicalNetworkName: \"tenantblue\",\n        \"mtu\": 1500,\n        \"subnets\": \"192.168.100.0/24,2001:dbb::/64\",\n        \"excludeSubnets\": \"192.168.100.1/32,2001:dbb::0/128\"\n    }'\n</code></pre></p> <p>Example 2 (custom MTU, VLAN and sticky IPs): <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: test-net\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"red\", \"blue\"]\n  network:\n    topology: Localnet\n    localnet:\n      role: Secondary\n      physicalNetworkName: tenantblue\n      subnets: [\"192.168.0.0/16\", \"2001:dbb::/64\"]\n      excludeSubnets: [\"192.168.50.0/24\"]\n      mtu: 9000\n      vlan:\n        mode: Access\n        access:\n          id: 200\n      ipam:\n        lifecycle: Persistent\n</code></pre> The above CR will make the controller create NAD in each selected namespace: \"red\" and \"blue\". NAD in namespace <code>red</code>: <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: test-net\n  namespace: red\nfinalizers:\n - k8s.ovn.org/user-defined-network-protection\nlabels:\n  k8s.ovn.org/user-defined-network: \"\"\nownerReferences:\n- apiVersion: k8s.ovn.org/v1\n  blockOwnerDeletion: true\n  controller: true\n  kind: ClusterUserDefinedNetwork\n  name: test-net\nspec:\n    config: &gt; '{\n        \"cniVersion\": \"1.0.0\",\n        \"type\": \"ovn-k8s-cni-overlay\"\n        \"netAttachDefName\": \"blue/test-net\",\n        \"role\": \"secondary\",\n        \"topology\": \"localnet\",\n        \"name\": \"cluster.udn.test-net\",\n        \"physicalNetworkName: \"tenantblue\",\n        \"subnets\": \"192.168.0.0/16,2001:dbb::/64\",\n        \"allowPersistentIPs\": true,\n        \"excludesubnets: \"10.100.50.0/24\",\n        \"mtu\": 9000,\n        \"vlanID\": 200\n    }'\n</code></pre></p>"},{"location":"okeps/okep-5085-localnet-api/#implementation-details","title":"Implementation Details","text":"<p>The CUDN <code>spec.network.topology</code> field should be extended to accept <code>Localnet</code> string. And the <code>spec.network</code> struct <code>NetworkSpec</code> should have an additional field for localnet topology configuration: <pre><code>const NetworkTopologyLocalnet NetworkTopology = \"Localnet\"\n\n...\n\n// NetworkSpec defines the desired state of UserDefinedNetworkSpec.\n// +union\ntype NetworkSpec struct {\n    // Topology describes network configuration.\n    //\n    // Allowed values are \"Layer3\", \"Layer2\" and \"Localnet\".\n    // Layer3 topology creates a layer 2 segment per node, each with a different subnet. Layer 3 routing is used to interconnect node subnets.\n    // Layer2 topology creates one logical switch shared by all nodes.\n    // Localnet topology attach to the nodes physical network. Enables egress to the provider's physical network. \n    //\n    // +kubebuilder:validation:Required\n    // +required\n    // +unionDiscriminator\n    Topology NetworkTopology `json:\"topology\"`\n    ...\n    // Localnet is the Localnet topology configuration.\n    // +optional\n    Localnet *LocalnetConfig `json:\"localnet,omitempty\"`\n}\n</code></pre></p> <p>The CUDN spec should have additional validation rule for <code>spec.network.topology</code> field: <pre><code>// ClusterUserDefinedNetworkSpec defines the desired state of ClusterUserDefinedNetwork.\ntype ClusterUserDefinedNetworkSpec struct {\n    ...\n    // +required\n    Network NetworkSpec `json:\"network\"`\n}\n</code></pre></p>"},{"location":"okeps/okep-5085-localnet-api/#localnet-topology-configuration-type","title":"Localnet topology configuration type","text":"<p>Introduce new topology configuration type for localnet - <code>LocalnetConfig</code>. The CUDN CRD <code>spec.network</code> should feature proposed localnet topology configuration.</p> <p>The Layer2 and Layer3 configuration types (<code>Layer2Config</code> &amp; <code>Layer3Config</code>) <code>role</code> field is defined with <code>NetworkRole</code> type. The <code>NetworkRole</code> type has the following enum validation, allowing <code>Secondary</code> and <code>Primary</code> values: <pre><code>// +kubebuilder:validation:Enum=Primary;Secondary\n</code></pre> The proposed localnet config type (<code>LocalnetConfig</code>) <code>role</code> field would have to accept <code>Secondary</code> value only. In order to avoid misleading CRD scheme, the enum validation should be moved closer to each <code>NetworkRole</code> usage: 1. Remove the existing enum validation from <code>NetworkRole</code> definition 2. At the <code>Layer2Config</code> definition, add enum validation to <code>role</code> field allowing: <code>Primary</code> or <code>Secondary</code>. 3. At the <code>Layer3Config</code> definition, add enum validation to <code>role</code> field allowing: <code>Primary</code> or <code>Secondary</code>. 4. At the proposed <code>LocalnetConfig</code> definition, add enum validation to <code>role</code> field allowing <code>Secondary</code> only.</p>"},{"location":"okeps/okep-5085-localnet-api/#vlan-configuration-type","title":"VLAN configuration type","text":"<p>As of today OVN-Kubernetes CNI allows to set the access VLAN of a localnet topology (using NADs).</p> <p>There are two options to expose the VLAN attribute: 1. Single integer filed, living at the same level as other fields, for example:    <pre><code> localnet:\n    mtu: 1500\n    vlan: 100\n</code></pre> 2. Discriminated union for accommodating VLAN related configurations, for example:     <pre><code> localnet:\n    mtu: 1500\n    vlan: \n      mode: Access\n      access:\n        id: 100 \n</code></pre></p> <p>Although option (1) is pretty straightforward, if additional VLAN configurations need to be supported, requiring the exposure of additional fields, it will enforce having VLAN-related fields right next to non-VLAN-related ones, making the API look awkward.</p> <p>In addition, in case future VLAN additions introduce mutual exclusive relation between VLAN config related fields, the flat structure makes it harder to maintain the related API validations markers. (it is not centralized, lives alongside other validations and makes them harder to follow).</p> <p>Option (2) enables all related VLAN configurations to live under the same roof, allowing the API to evolve smoothly while introducing less noise compared to (1), and enable having all related API validations centralized. For example kubernetes-nmstate (utilize https://nmstate.io/)  follows the same convention, that is having complex filed for VLAN configurations. </p> <p>The proposed options for VLAN configurations is (2)</p> <pre><code>// VLANID is a VLAN ID (VID), should be greater than 0, and lower than 4095.\n// +kubebuilder:validation:Minimum=1\n// +kubebuilder:validation:Maximum=4094\ntype VLANID int32\n// AccessVLANConfig describes an access VLAN configuration.\ntype AccessVLANConfig struct {\n    // id is the VLAN ID (VID) to be set on the network logical switch port.\n    ID VLANID `json:\"id\"`\n}\n// VLANConfig describes the network VLAN configuration.\n// +union\ntype VLANConfig struct {\n    // mode describe the network VLAN mode.\n    // Allowed value is \"Access\".\n    // Access sets the network logical switch port in access mode, according to the config.\n    // +required\n    // +unionDiscriminator\n    // +kubebuilder:validation:Enum=Access\n    Mode string `json:\"mode\"`  \n\n    // Access is the access VLAN configuration \n    // +optional\n    Access *AccessVlanConfig `json:\"access\"`\n}\n</code></pre> <pre><code>// +kubebuilder:validation:XValidation:rule=\"!has(self.ipam) || !has(self.ipam.mode) || self.ipam.mode == 'Enabled' ? has(self.subnets) : !has(self.subnets)\", message=\"Subnets is required with ipam.mode is Enabled or unset, and forbidden otherwise\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.excludeSubnets) || has(self.excludeSubnets) &amp;&amp; has(self.subnets)\", message=\"excludeSubnets must be unset when subnets is unset\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.subnets) || !has(self.mtu) || !self.subnets.exists_one(i, isCIDR(i) &amp;&amp; cidr(i).ip().family() == 6) || self.mtu &gt;= 1280\", message=\"MTU should be greater than or equal to 1280 when IPv6 subent is used\"\n// +kubebuilder:validation:XValidation:rule=\"has(self.vlan) &amp;&amp; has(self.vlan.mode) &amp;&amp; self.vlan.mode == 'Access' ? has(self.vlan.access): !has(self.vlan.access)\", message=\"vlan.access is required when vlan.mode is 'Access', and forbidden otherwise\"\n// + ---\n// + TODO: enable the below validation once the following issue is resolved https://github.com/kubernetes/kubernetes/issues/130441\n// + kubebuilder:validation:XValidation:rule=\"!has(self.excludeSubnets) || self.subnets.map(s, self.excludeSubnets.map(e, cidr(s).containCIDR(e)))\", message=\"excludeSubnets should be in range of CIDRs specified in subnets\"\n// + kubebuilder:validation:XValidation:rule=\"!has(self.excludeSubnets) || self.excludeSubnets.all(e, self.subnets.exists(s, cidr(s).containsCIDR(cidr(e))))\",message=\"excludeSubnets must be subnetworks of the networks specified in the subnets field\",fieldPath=\".excludeSubnets\"\ntype LocalnetConfig struct {\n    // role describes the network role in the pod, required.\n    // Whether the pod interface will act as primary or secondary.\n    // For Localnet topology only `Secondary` is allowed.\n    // Secondary network is only assigned to pods that use `k8s.v1.cni.cncf.io/networks` annotation to select given network.\n    // +kubebuilder:validation:Enum=Secondary\n    // +required\n    Role NetworkRole `json:\"role\"`\n\n    // physicalNetworkName points to the OVS bridge-mapping's network-name configured in the nodes, required.\n    // In case OVS bridge-mapping is defined by Kubernetes-nmstate with `NodeNetworkConfigurationPolicy` (NNCP),\n    // this field should point to the value of the NNCP spec.desiredState.ovn.bridge-mappings.\n    // Min length is 1, max length is 253, cannot contain `,` or `:` characters.\n    // +kubebuilder:validation:MinLength=1\n    // +kubebuilder:validation:MaxLength=253\n    // +kubebuilder:validation:XValidation:rule=\"self.matches('^[^,:]+$')\", message=\"physicalNetworkName cannot contain `,` or `:` characters\"\n    // +required\n    PhysicalNetworkName string `json:\"physicalNetworkName\"`\n\n    // subnets are used for the pod network across the cluster.\n    // When set, OVN-Kubernetes assign IP address of the specified CIDRs to the connected pod,\n    // saving manual IP assigning or relaying on external IPAM service (DHCP server).\n    // subnets is optional, when omitted OVN-Kubernetes won't assign IP address automatically.\n    // Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.\n    // The format should match standard CIDR notation (for example, \"10.128.0.0/16\").\n    // This field must be omitted if `ipam.mode` is `Disabled`.\n    // In a scenario `physicalNetworkName` points to OVS bridge mapping of a network who provide IPAM services (e.g.: DHCP server),\n    // `ipam.mode` should set with `Disabled, turning off OVN-Kubernetes IPAM and avoid  conflicts with the existing IPAM services on the subject network.\n    // +optional\n    Subnets DualStackCIDRs `json:\"subnets,omitempty\"`\n\n    // excludeSubnets list of CIDRs removed from the specified CIDRs in `subnets`.\n    // excludeSubnets is optional. When omitted no IP address is excluded and all IP address specified by `subnets` subject to be assigned.\n    // Each item should be in range of the specified CIDR(s) in `subnets`.\n    // The maximal exceptions allowed is 25.\n    // The format should match standard CIDR notation (for example, \"10.128.0.0/16\").\n    // This field must be omitted if `subnets` is unset or `ipam.mode` is `Disabled`.\n    // In a scenario `physicalNetworkName` points to OVS bridge mapping of a network who has reserved IP addresses\n    // that shouldn't be assigned by OVN-Kubernetes, the specified CIDRs will not be assigned. For example:\n    // Given: `subnets: \"10.0.0.0/24\"`, `excludeSubnets: \"10.0.0.200/30\", the following addresses will not be assigned to pods: `10.0.0.201`, `10.0.0.202`.\n    // +optional\n    // +kubebuilder:validation:MinItems=1\n    // +kubebuilder:validation:MaxItems=25\n    ExcludeSubnets []CIDR `json:\"excludeSubnets,omitempty\"`\n\n    // ipam configurations for the network (optional).\n    // IPAM is optional, when omitted, `subnets` should be specified.\n    // When `ipam.mode` is `Disabled`, `subnets` should be omitted.\n    // `ipam.mode` controls how much of the IP configuration will be managed by OVN.\n    //    When `Enabled`, OVN-Kubernetes will apply IP configuration to the SDN infra and assign IPs from the selected subnet to the pods.\n    //    When `Disabled`, OVN-Kubernetes only assign MAC addresses and provides layer2 communication, enable users configure IP addresses to the pods.\n    // `ipam.lifecycle` controls IP addresses management lifecycle.\n    //    When set with 'Persistent', the assigned IP addresses will be persisted in `ipamclaims.k8s.cni.cncf.io` object.\n    //    Useful for VMs, IP address will be persistent after restarts and migrations. Supported when `ipam.mode` is `Enabled`.\n    // +optional\n    IPAM *IPAMConfig `json:\"ipam,omitempty\"`\n\n    // mtu is the maximum transmission unit for a network.\n    // MTU is optional, if not provided, the default MTU (1500) is used for the network.\n    // Minimum value for IPv4 subnet is 576, and for IPv6 subnet is 1280. Maximum value is 65536.\n    // In a scenario `physicalNetworkName` points to OVS bridge mapping of a network configured with certain MTU settings,\n    // this field enable configuring the same MTU the pod interface, having the pod MTU aligned with the network.\n    // Misaligned MTU across the stack (e.g.: pod has MTU X, node NIC has MTU Y), could result in network disruptions and bad performance.\n    // +kubebuilder:validation:Minimum=576\n    // +kubebuilder:validation:Maximum=65536\n    // +optional\n    MTU int32 `json:\"mtu,omitempty\"`\n\n    // vlan configuration for the network.\n    // vlan.mode is the VLAN mode.\n    //   When \"Access\" is set, OVN-Kuberentes configures the network logical switch port in access mode.\n    // vlan.access is the access VLAN configuration. \n    // vlan.access.id is the VLAN ID (VID) to be set on the network logical switch port.\n    // vlan is optional, when omitted the underlying network default VLAN will be used (usually `1`).\n    // When set, OVN-Kubernetes will apply VLAN configuration to the SDN infra and to the connected pods.\n    // +optional\n    VLAN VLANConfig `json:\"vlan,omitempty\"`\n}\n</code></pre>"},{"location":"okeps/okep-5085-localnet-api/#implementation-phases","title":"Implementation phases","text":"<ol> <li>Add support for localnet topology on top CUDN CRD</li> <li>Extend the CUDN CRD.</li> <li>Add support for localnet topology in the CUDN CRD controller.</li> <li>Adjust CI multi-homing jobs to enable testing localnet CUDN CRs.</li> <li>Update API reference docs.</li> <li>Introduce CEL validation rule to ensure <code>excludedSubnets</code> items are in range of specified items in <code>subnets</code>.</li> <li>Can be done once is resolved https://github.com/kubernetes/kubernetes/issues/130441.</li> <li>Update the Kubernetes version using in CI that includes the bugfix.</li> <li>Add the subject validation.</li> </ol>"},{"location":"okeps/okep-5085-localnet-api/#testing-details","title":"Testing Details","text":"<p>The controller business logic is agnostic to topology types, the proposed solution can be covered fully by unit test. In addition, the CRD controller business logic, and localnet topology functionality (using NADs) are tested e2e thoroughly.  E2e test for localnet topology CUDN CR is optional in this particular case.</p>"},{"location":"okeps/okep-5085-localnet-api/#documentation-details","title":"Documentation Details","text":""},{"location":"okeps/okep-5085-localnet-api/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-5085-localnet-api/#cel-rule-validations","title":"CEL rule validations","text":"<p>Validating that the specified subnets exclusions (<code>spec.network.localnet.excludedSubnets</code>) are in the range of the specified topology subnets  (<code>spec.network.localnet.excludedSubnets</code>) is currently impossible due to a bug in the CEL rule library for validation IPs and CIDRs. Such invalid CUDN CRs request will not be blocked by the cluster API. Once the bug is resolved the validation can be added. See the following issue for more details https://github.com/kubernetes/kubernetes/issues/130441.</p> <p>To mitigate this, the mentioned validation should be done by the CUDN CRD controller: In a scenario where a CUDN CR has at least one exclude-subnet that is not within the range of the topology subnet, the controller will not create the corresponding NAD and will report an error in the status.</p>"},{"location":"okeps/okep-5085-localnet-api/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":""},{"location":"okeps/okep-5085-localnet-api/#alternatives","title":"Alternatives","text":""},{"location":"okeps/okep-5085-localnet-api/#references","title":"References","text":""},{"location":"okeps/okep-5088-evpn/","title":"OKEP-5088: EVPN Support","text":"<ul> <li>Issue: #5088</li> </ul>"},{"location":"okeps/okep-5088-evpn/#problem-statement","title":"Problem Statement","text":"<p>The purpose of this enhancement is to add support for EVPN within the OVN-Kubernetes SDN, specifically with BGP. This effort will allow exposing User Defined Networks (UDNs) externally via a VPN to other entities either inside, or outside the cluster. BGP+EVPN is a common and native networking standard that will enable integration into a user's networks without SDN specific network protocol integration, and provide an industry standardized way to achieve network segmentation between sites.</p>"},{"location":"okeps/okep-5088-evpn/#goals","title":"Goals","text":"<ul> <li>To provide a user facing API to allow configuration of EVPN on Kubernetes worker nodes to integrate with a provider's   EVPN fabric.</li> <li>EVPN support will be provided for Layer 2 (MAC-VRF) or Layer 3 (IP-VRF) OVN-Kubernetes Primary User Defined Network   types.</li> <li>EVPN Multi-Homing + Mass Withdrawal support, including BFD support for link detection.</li> <li>FRR providing EVPN connectivity via BGP and acting as the Kubernetes worker node PE router.</li> <li>Support for EVPN in local gateway mode only.</li> <li>Support for EVPN in on-prem deployments only.</li> </ul>"},{"location":"okeps/okep-5088-evpn/#non-goals","title":"Non-Goals","text":"<ul> <li>Providing support for any other virtual router as a PE router.</li> <li>Asymmetric Integrated Routing and Bridging (IRB) with EVPN.</li> <li>Supporting EVPN via the Cluster Default Network (CDN).</li> </ul>"},{"location":"okeps/okep-5088-evpn/#future-goals","title":"Future-Goals","text":"<ul> <li>Support for EVPN in shared gateway mode once there is OVN support.</li> <li>Potentially advertising service Cluster IPs.</li> <li>Cloud platform BGP/EVPN enablement.</li> <li>Providing EVPN support for Secondary User Defined Network types.</li> <li>Specifying a VXLAN port other than 4789 for EVPN.</li> <li>Support for interconnecting two Kubernetes clusters with EVPN and then allowing VM migration across them.</li> </ul>"},{"location":"okeps/okep-5088-evpn/#introduction","title":"Introduction","text":"<p>The BGP enhancement has been implemented in OVN-Kubernetes, which allows a user expose pods and other internal Kubernetes network entities outside the cluster with dynamic routing.</p> <p>Additionally, the User Defined Network (UDN) feature has brought the capability for a user to be able to create per tenant networks. Combining these features today allows a user to either: - BGP advertise the Cluster Default Network (CDN) as well as leak non-IP-overlapping UDNs into default VRF. - Expose UDNs via BGP peering over different network interfaces on an OCP node, allowing a VPN to be terminated on the   next hop PE router, and preserved into the OCP node. Also known in the networking industry as VRF-Lite.</p> <p>While VRF-Lite allows for a UDN to be carried via a VPN to external networks, it is cumbersome to configure and requires an interface per UDN to be available on the host. By leveraging EVPN, these limitations no longer exist and all UDNs can traverse the same host interface, segregated by VXLAN. Furthermore, with exposing UDNs via BGP today there is a limitation that these networks are advertised as an L3 segment, even with VRF-Lite. With EVPN, we can now stretch the L2 UDN segment across the external network fabric.  Finally, EVPN is a common datacenter networking fabric that many users with Kubernetes clusters already rely on for their top of rack (TOR) network connectivity. It is a natural next step to enable the Kubernetes platform to be able to directly integrate with this fabric directly.</p>"},{"location":"okeps/okep-5088-evpn/#evpnudn-background","title":"EVPN/UDN Background","text":"<p>This section provides some background on UDN, EVPN. It is important to provide this context for a better understanding of the following sections and design choices.</p> <p>UDNs are separate and potentially overlapping networks that are naturally isolated from each other. Each Primary UDN maps to a corresponding VRF in Linux. This is true for both Layer 2 and Layer 3 networks. The ovn-k8s-mpx interface exists per UDN and is configured inside of this Linux VRF.</p> <p>EVPN provides a mechanism for carrying potentially overlapping Layer 2 and/or Layer 3 networks while keeping them isolated from each other in a Virtual Private Network (VPN) over an ethernet fabric via BGP extensions and an overlay mechanism, typically VXLAN.</p> <p>Therefore, it is a logical next step to leverage EVPN to carry PUDNs as it fits the paradigm well. However, there are key design choices to be aware of. With EVPN a Layer 2 network (MAC-VRF) can be thought of as simply a switch extended across the ethernet fabric and used for East/West traffic.  With EVPN a Layer 2 network can belong to MAC-VRF as well as an IP-VRF (Layer 3) network simultaneously. In practice, this means the Layer 2 switch has a Switch Virtual Interface (SVI) which is plugged into a router (VRF), and clients on the Layer 2 network use this SVI as their default gateway for routing. In other words, if you have a pod connected to a switch (MAC-VRF that spans all nodes) it can talk to other pods or entities on that Layer 2 network without leaving the MAC-VRF. On the other hand if this pod needs to talk to the internet, it will go via its default gateway (SVI) and then be routed via the IP-VRF. With EVPN, multiple Layer 2 networks can map to a single IP-VRF.</p> <p>Let's now look at how a Layer 2 UDN works in practice. For East/West it uses Geneve to ensure MAC-VRF like functionality. For North/South by default we leak the Layer 2 into the default VRF (CDN) and use masquerading to avoid collisions with other overlapping Layer 2 UDNs. If BGP is enabled, this Layer 2 UDN may be advertised directly (without masquerading) in the default VRF. Neither of these options provide IP-VRF like isolation, in both cases the network is leaked into the default VRF. When leaking into the default VRF, multiple Layer 2 UDNs may not overlap in their subnet range, each subnet must be unique. Another option is VRF-Lite, where a user can connect a dedicated NIC (think VLAN interface) to the VRF provisioned in the Linux host for the Layer 2 UDN. With this model, a Layer 3 VPN is created, but it relies on manual configuration on the host, as well as configuration on the upstream PE router. Now in order to route Northbound, the pod sends the packet to its default gateway, the transit router, which will either route towards ovn-k8s-mpx (local gw mode) or the gateway router (GR). Note, VRF-Lite only works for Layer 3 type VPNs, and does not provide a Layer 2 type of VPN technology to handle VM live migration use cases.</p> <p>For Layer 3 networks, in OVN-Kubernetes we break up a supernet into a per-node subnet for the PUDN. Whenever a pod on a node wants to talk to a pod on another node or anything externally, pure routing is used. This is the equivalent of an IP-VRF.</p> <p>Furthermore, so far we have talked about carrying VPN for Layer 2 and Layer 3 PUDNs. What about the Cluster Default Network (CDN)? This network lives in the default VRF. Typically, the default VRF is not carried over the EVPN fabric as a VPN. For the purpose of this enhancement it will be considered not be part of an EVPN. The CDN may still use Geneve, or it may rely on the upcoming no-overlay mode to use a pure BGP routing scheme.</p> <p>As far as underlying technologies go, we use FRR as our BGP provider. As EVPN relies on BGP, we will continue to use FRR for EVPN support.</p>"},{"location":"okeps/okep-5088-evpn/#transitioning-to-evpn-from-geneve","title":"Transitioning to EVPN from Geneve","text":"<p>For Layer 2 and Layer 3 PUDNs Geneve is no longer used when enabling EVPN. Instead, VXLAN is used as an overlay to carry per network VPN packets across the EVPN fabric. In practice this means we still need a switch domain that crosses all nodes for Layer 2, and we need per node routing for Layer 3. FRR integrates directly with Linux to provide EVPN support. It  uses netdevs like Linux Bridge, VXLAN VTEPs, VRFs in order to map Linux networks to advertised EVPN VNIs.</p> <p>We know that we create a VRF per Layer 2/Layer 3 UDN, which gives us our mapping to an EVPN VRF in Linux that FRR will leverage. Therefore, we can conclude that every Layer 2 and Layer 3 UDN maps to an IP-VRF already. This implicit design choice precludes us from taking several Layer 2 PUDNs and mapping them to a single IP-VRF. In other words, each MAC-VRF in OVN-Kubernetes will map 1 to 1 with an IP-VRF. In practice, the consequence of this is a user is unable to connect multiple Layer 2 PUDNs to the same \"router\" and allow routing between them. However, as we will see later, this can be provided by other forms of route leaking (route target importing/exporting) between IP-VRFs. Nonetheless, it is key to point out this implicit design choice and why it exists. It also provides implicit Network Isolation which was one of the key tenets to the UDN design in the first place. So one could view this as intended behavior.</p> <p>Additionally, we have the ovn-k8s-mpx interface, which gives us a way to route for Layer 2 networks into the VRF. This interface can be viewed as our SVI. It is a way that we can get the packet to the VRF (IP-VRF) to be routed. Note the SVI can be the same on every single node, since we are using symmetrical Integrated Routing and Bridging (IRB). The SVI IP will not be advertised to via BGP, and is only used for local pod routing.</p> <p>When we consider non-routed traffic for Layer 2 (just MAC-VRF traffic), we need a way for Layer 2 destined packets to get VXLAN encapsulated and sent out with the right VNI. As previously mentioned, FRR relies on a Linux Bridge to serve as the Layer 2 switch, which is connected to a VXLAN VTEP. With OVN, we obviously use a logical_switch that exists as flows in br-int. In order to integrate with FRR/EVPN for local gateway mode, we will need to connect a port from the logical_switch to the Linux Bridge. More technical details on that in the sections ahead.</p>"},{"location":"okeps/okep-5088-evpn/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>The user stories will be broken down into more detail in the subsections below. The main use cases include: * As a user, I want to connect my Kubernetes cluster to VMs or physical hosts on an external network. I want tenant   pods/VMs inside my Kubernetes cluster to be able to only communicate with certain network segments on this external   network. * As a user, I want to be able to live migrate VMs from my external network onto the Kubernetes platform. * As a user, my data center where I run Kubernetes is already using EVPN today. I want to eliminate the use of Geneve   which causes double encapsulation (VXLAN and Geneve), and integrate natively with my networking fabric. * As a user, I want to create overlapping IP address space UDNs, and then connect them to different external networks   while preserving network isolation.</p>"},{"location":"okeps/okep-5088-evpn/#extending-udns-into-the-provider-network-via-evpn","title":"Extending UDNs into the provider network via EVPN","text":"<p>This use case is about connecting a Kubernetes cluster to one or more external networks and preserving the network isolation of the UDN and external virtual routing and forwarding instances (VRFs) segments. Consider the following diagram:</p> <p></p> <p>In this example a user has traditional Finance and HR networks. These networks are in their own VRFs, meaning they are isolated from one another and are unable to communicate or even know about the other. These networks may overlap in IP addressing. Additionally, the user has a Kubernetes cluster where they are migrating some traditional servers/VMs workloads over to the Kubernetes platform. In this case, the user wants to preserve the same network isolation they had previously, while also giving the Kubernetes based Finance and HR tenants connectivity to the legacy external networks.</p> <p>By combining EVPN and UDN this becomes possible. The blue Finance network UDN is created with the Kubernetes cluster, and integrated into the user's EVPN fabric, extending it to the traditional Finance external network. The same is true for the yellow HR network. The Finance and HR network isolation is preserved from the Kubernetes cluster outward to the external networks.</p>"},{"location":"okeps/okep-5088-evpn/#extending-layer-2-udns-into-the-provider-network-to-allow-vm-migration","title":"Extending Layer 2 UDNs into the provider network to allow VM migration","text":"<p>Building upon the previous example, the network connectivity between a UDN and an external network can be done using either Layer 3 (IP-VRF) or Layer 2 (MAC-VRF). With the former, routing occurs between entities within the Kubernetes UDN and the corresponding external network, while with the latter, the UDN and the external network are both part of the same layer 2 broadcast domain. VM migration relies on being a part of the same L2 segment in order to preserve MAC address reachability as well as IP address consistency. With MAC-VRFs and EVPN it becomes possible to extend the layer 2 network between the kubernetes cluster and outside world:</p> <p></p> <p>The image above depicts a Layer 2 UDN which not only exists across the worker nodes node-1 and node-2 but is also stretched into Provider Network 1. In this scenario, vm-2 is able to migrate into node-1 on the UDN network, preserving the same IP address it had in the external provider network. Similarly, there is another Provider Network 2 which may or may not correspond to another UDN within the Kubernetes cluster. However, notice that the red and blue networks are both using the same IP addressing scheme and sharing the same hardware, however due to VRF isolation they are completely unaware and unable to communicate with each other.</p>"},{"location":"okeps/okep-5088-evpn/#using-evpn-as-the-overlay-for-user-defined-networks","title":"Using EVPN as the Overlay for User Defined Networks","text":"<p>With integrating into a customer's already existing TOR spine and leaf architecture, Geneve can be disabled, and network segmentation will still persist for east/west traffic due to VXLAN tunnels with EVPN. This is true for both IP-VRFs, and MAC-VRFs. This reduces packet overhead for customers, while also providing some other advantages that come with EVPN, such as link redundancy and broadcast, unknown unicast, and multicast (BUM) traffic suppression.</p>"},{"location":"okeps/okep-5088-evpn/#proposed-solution","title":"Proposed Solution","text":"<p>As mentioned in the previous sections, EVPN will continue to build upon the BGP support already implemented into OVN-Kubernetes using FRR. This support includes integration with an OVN-Kubernetes API as well as an FRR-K8S API for configuring BGP peering and routing for UDNs. FRR already supports EVPN and similar API resources will be leveraged to accomplish configuring FRR as the BGP/EVPN control plane. Recall that FRR relies on Linux netdevs to be configured in order for EVPN to work. There are two configuration modes in FRR to accomplish this.</p>"},{"location":"okeps/okep-5088-evpn/#multiple-vxlan-devices-mvd","title":"Multiple VXLAN Devices (MVD)","text":"<p>MVD is the classic way that FRR maps netdevs to EVPN configuration. In this model we create the following Linux constructs in order to use EVPN:</p> <ol> <li>A VRF device</li> <li>Linux Bridge enslaved to the VRF</li> <li>An SVI attached to the bridge (for IP-VRF)</li> <li>A VXLAN device enslaved to the bridge</li> <li>A VTEP IP configured locally (generally on a loopback interface)</li> </ol> <p>Devices 1-4 are needed to be configured for each network (UDN). Only a single VTEP IP is needed, although more than one could be configured if it was desired to use multiple tunnels (uncommon).</p>"},{"location":"okeps/okep-5088-evpn/#single-vxlan-device-svd","title":"Single VXLAN Device (SVD)","text":"<p>SVD is a newer way that was created in order to solve scale issues around creating too many netdevs with MVD. In this model only a single VXLAN device is created, along with a single Linux Bridge. VLANs are used within the Linux Bridge to segment networks. The following devices are required:</p> <ol> <li>A VRF device (one per UDN)</li> <li>Linux Bridge (single bridge with a VLAN per UDN)</li> <li>An SVI attached the bridge (for IP-VRF, one VLAN sub-interface per UDN)</li> <li>A VXLAN device enslaved to the bridge (added in the UDN VLAN)</li> <li>A VTEP IP configured locally (generally on a loopback interface)</li> </ol> <p>SVD is supposed to scale better than MVD, and is similar to how other physical routers allow a single VTEP to be used for multiple VNIs. The one drawback with the SVD implementation, is that by mapping VNIs to VLANs, we are limited to 4096 max VNIs per node, and thus limited to 4094 MAC+IP VRFs per node. It may be possible to extend SVD by creating another set of bridge/VTEP pair and mapping the VLANs to VNIs &gt; 4094. Needs to be investigated and confirmed.</p> <p>SVD is only supported in FRR9 and later.</p>"},{"location":"okeps/okep-5088-evpn/#frr-integration-with-ovn-kubernetes","title":"FRR integration with OVN-Kubernetes","text":"<p>OVN-Kubernetes will be in charge of managing and configuring these devices on the node. We do not want to support both MVD and SVD. Due to the simplicity and scalability, we will choose to support SVD and accept the potential drawback of limited number of VRFs we can advertise. Additionally, this enhancement limits the scope of EVPN support to local gateway mode, which means traffic will flow through the Linux Bridge (MAC-VRF) as well as through the Linux VRF (IP-VRF). In the future, OVN will add support for also watching the Linux netdevs, and then configuring OVN to act as a VXLAN VTEP for the UDNs.</p> <p>If we consider the SVD devices that are required for OVN-Kubernetes integration, recall that we already create:  * Linux VRF for every Layer2/Layer3 UDN  * An ovn-k8s-mpx interface connected to the VRF and plugged into the UDN worker switch</p> <p>These devices already give us the VRF and SVI devices we need for 2 of the 5 netdevs we need for EVPN. For an IP-VRF, we just need a way to get the packet from the pod into the IP-VRF, it does not matter if we use the ovn-k8s-mpx interface or if we use an SVI interface attached to the Linux Bridge. Therefore, we will never configure an IP address on the SVI interface of the Linux bridge, and rely on ovn-k8s-mpx to get packets to the IP-VRF.</p> <p>For the remaining devices, the API will drive their creation, covered in a later section.</p>"},{"location":"okeps/okep-5088-evpn/#workflow-description","title":"Workflow Description","text":"<p>Tenants as well as admins are able to create UDNs and CUDNs for their namespace(s), respectively. However, only CUDNs are allowed to be BGP advertised by admins. This trend will continue with EVPN, where it will require admin access in order to enable EVPN for one or more UDNs. A typical workflow will be:</p> <ol> <li>Configure BGP peering via interacting with the FRR-K8S API for a given set of worker nodes.</li> <li>Create a VTEP CR that defines the VTEP IP to be used with EVPN VXLAN.</li> <li>Create a primary Layer 2 or Layer 3 CUDN. Within the the CUDN CR specify an EVPN overlay configuration.</li> <li>Create a RouteAdvertisements CR to specify what routes should be advertised via EVPN for this UDN.</li> </ol>"},{"location":"okeps/okep-5088-evpn/#api-details","title":"API Details","text":""},{"location":"okeps/okep-5088-evpn/#frr-k8s","title":"FRR-K8S","text":"<p>FRR-K8S will need to be extended to allow for configuring specific EVPN FRR configuration. In order to advertise MAC-VRFs the following configuration needs to happen in FRR:</p> <pre><code>router bgp 64512\n !\n address-family l2vpn evpn\n  neighbor 192.168.1.0 activate\n  neighbor 192.168.1.0 allowas-in origin\n  advertise-all-vni\n</code></pre> <p>This configuration signals to FRR to advertise all VNIs detected for Layer 2 MAC-VRFs. It does this by looking at the netdevs and finding Linux Bridges/VLANs within the node.</p> <p>For IP-VRFs, we need to be able to add the following configuration:</p> <pre><code>vrf udnA\n vni 100\nexit-vrf\n!\nrouter bgp 64512 vrf udnA\n !\n address-family l2vpn evpn\n  route-target import 64512:100\n  route-target export 64512:100\n exit-address-family\nexit \n</code></pre> <p>In the above configuration, the \"vrf udnA\" refers to an IP-VRF, then we need a router bgp vrf stanza activating EVPN for it. EVPN also utilizes BGP Route-Targets (RT) in order to import/export routes between IP-VRFs. We must configure at least the targets for the VRF itself so that it will import/export routes from other nodes in its own IP-VRF. In the future this route-target config may be extended to include other IP-VRFs (UDNs) in order to connect networks together. Similar to the \"Network Connect\" feature to connect UDNs via OVN+Geneve.</p> <p>In order for OVN-Kubernetes to generate FRR-K8S configuration that includes this EVPN specific configuration, FRR-K8S API will need to be extended. Those API extensions will be designed and implemented within the FRR-K8S project.</p>"},{"location":"okeps/okep-5088-evpn/#vtep-crd","title":"VTEP CRD","text":"<p>A VTEP CRD will be created which allows an admin to define VTEP IPs to be associated with an EVPN enabled CUDN. In the the future this VTEP CRD may be extended to other use cases like providing VTEP IPs for Geneve tunnels, etc.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: VTEP\nmetadata:\n  name: evpn-vtep\nspec:\n  cidr: 100.64.0.0/24\n  mode: managed\n</code></pre> <p>The cidr field is mandatory. If the mode is \"managed\", then OVN-Kubernetes will handle allocating and assigning VTEP IPs per node. If the mode is not provided, or is \"unmanaged\", then it is left to some other provider to handle adding the IP address to each node from the subnet provided. In unmanaged mode, OVN-Kubernetes will find an interface on the node which has an IP address in the cidr, and use that IP address. Unmanaged mode may be preferred where a provider handles assigning VTEP IPs within its EVPN fabric. In this case OVN-Kubernetes cluster is integrating into an already configured EVPN fabric. Therefore VTEP IP provisioning may be done by the provider and configured for each node.</p> <p>The IP address assigned or found by OVN-Kubernetes will be annotated to the node as <code>k8s.ovn.org/&lt;vtep name&gt;: &lt;ip&gt;</code>. If all nodes do not have an IP address annotated for this VTEP, the VTEP CR will go into error state.</p>"},{"location":"okeps/okep-5088-evpn/#cudn-crd-changes-for-evpn","title":"CUDN CRD changes for EVPN","text":"<p>This API change depends on the No Overlay Feature which will introduce a new Transport field that we can extend to use EVPN. The API change will look like this:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: l2-primary\n  labels:\n    bgp: enabled\nspec:\n  namespaceSelector:\n    matchLabels:\n      kubernetes.io/metadata.name: udn-test\n  network:\n    topology: Layer2\n    layer2:\n      role: Primary\n      subnets:\n      - 10.20.100.0/16\n  transport: EVPN\n  evpnConfiguration:\n    vtep: evpn-vtep\n    macVRF:\n      vni: 100\n      routeTarget: \"65000:100\"\n    ipVRF:\n      vni: 101\n      routeTarget: \"65000:101\"\n</code></pre> <p>In the above example, a new transport type \"EVPN\" is introduced. This is paired with evpnConfiguration section, which specifies the name of the VTEP CR to use. We will only support specifying a single VTEP to use for EVPN.</p> <p>The macVRF field must be provided for Layer2 networks, and cannot be provided for Layer3. ipVRF must be provided for Layer3, and is optional for Layer2 if an IP-VRF is also desired for the Layer2 UDN.</p> <p>The \"vni\" specified under each will be used to determine the VNID for each EVPN segment. The VNI values may not overlap between any EVPN CR.</p> <p>Furthermore, routeTarget may be configured in order to specify the route target to import/export for the UDN.  The routeTarget field is optional, and if left unset, the routeTarget will be automatically determined in the format <code>&lt;AS Number&gt;:&lt;VNI&gt;</code>. If the routeTarget is the same between UDNs of different VRF types (MAC and IP VRFs), the overlapping IDs will have no effect. However, if routeTarget is the same between UDNs of the same VRF type, then UDN routes will be leaked between UDNs within the leaves (Kubernetes nodes). It is recommended to handle route leaking within the spine itself, when using eBGP or iBGP with route reflectors. However, in a full mesh iBGP environment, especially one without a TOR spine fabric, it is necessary to configure route leaking at the leaves (Kubernetes nodes). This may be accomplished by setting the same routeTarget value for multiple UDNs, or by adding extra FRR configuration to import other UDN's route targets.</p> <p>Additionally, with Layer 2 MAC-VRFs stretched outside the cluster, we need a way to be able to tell OVN-Kubernetes not to by default allocate certain IPs from the shared subnet. The Layer2 UDN already provides \"ReservedSubnets\" for this purpose. If a VM or other application is migrated from the external fabric over the L2 VPN, then it is able to use a static IP from this ReservedSubnets field. However, when a VM migrates it not only needs to preserve its IP address, but also its default gateway. OVN-Kubernetes uses the first IP in the subnet as the default gateway for pods. This may not align with the gateway IP used by a VM off-cluster, and in addition we use the second IP in the subnet as the ovn-k8s-mpx IP that may also be used on the Layer 2 network off cluster. To address there are other fields already provided in the Layer2 UDN spec that can be leveraged: - DefaultGatewayIPs - InfrastructureSubnets</p> <p>These configurable fields should be considered when creating a MAC-VRF that will be extended to outside the cluster.</p>"},{"location":"okeps/okep-5088-evpn/#route-advertisement-ra-crd-changes","title":"Route Advertisement (RA) CRD changes","text":"<p>Note, in order to use EVPN, a Route Advertisement CR must also be created which selects the CUDN. There are no foreseen changes required to the RA CRD.</p>"},{"location":"okeps/okep-5088-evpn/#implementation-details","title":"Implementation Details","text":""},{"location":"okeps/okep-5088-evpn/#vtep","title":"VTEP","text":"<p>When a VTEP CR is created in managed mode, ovnkube-cluster-manager will handle assigning an VTEP IP to each node. If the cidr range includes the Kubernetes node IP for a node, then the node IP will be used for VTEP IP. Note, using the node IP should be avoided in most cases, as that IP will already be tied to a specific interface on the node. This prevents proper Layer 3 failure handling. While the node IP on a dedicated link could use bonding to prevent Layer 2 failover, if something goes wrong on the Layer 3 interface, or the leaf connected to that link goes down, then there is no failover. With EVPN, it is advantageous to assign the VTEP IP to a loopback interface, so that multihoming and failover handling can occur. If a link goes down to one leaf, BFD will fire, and there will be a mass withdrawal of routes, moving all traffic to a second leaf.</p> <p>ovnkube-cluster-manager will handle annotating the node with the assigned IP address. If the VTEP is in unmanaged mode, then ovnkube-cluster-manager will only handle checking that all nodes have an IP address annotated for this VTEP. If a node is missing the IP, the VTEP will be updated with a failure status condition. Even if a single node fails, the other healthy nodes with assigned VTEPs will be configured for EVPN correctly.</p> <p>For unmanaged, the ovnkube-node component will handle detecting the IP address on the Linux node and setting the node annotation.</p>"},{"location":"okeps/okep-5088-evpn/#evpn","title":"EVPN","text":"<p>When a UDN is created with EVPN configuration, the UDN controller in ovnkube-cluster-manager will check to ensure that a VTEP exists for this EVPN. If one does not, then the NAD will not be rendered and the UDN will be put into error state. Additionally, a check will be done to ensure that VNIs do not overlap between any other UDNs in the cluster.</p> <p>Once the NAD has been rendered, ovnkube-controller and ovnkube-node network controllers will be started. The ovnkube-node network controller will detect that this network is EVPN enabled, and then create the correct network device configuration in Linux.</p> <p>The BGP RA controller will be responsible for detecting when a BGP RA selects a CUDN that is EVPN enabled. Once it does, it will generate the proper FRR-K8S configuration.</p> <p>For the rest of the examples in this section, assume there is a layer 2 UDN called \"blue\", with subnet 10.0.10.0/24.</p>"},{"location":"okeps/okep-5088-evpn/#node-configuration-mac-vrf-ip-vrf-combination-with-layer-2-udn","title":"Node Configuration: MAC-VRF + IP-VRF Combination with Layer 2 UDN","text":"<p>Once the VTEP IP is assigned, ovnkube-node will then handle configuring the following: <pre><code># VTEP IP assignment to loopback - only done in VTEP managed mode\nip addr add 100.64.0.1/32 dev lo\n\n# SVD bridge + VXLAN setup\nip link add br0 type bridge vlan_filtering 1 vlan_default_pvid 0\nip link set br0 addrgenmode none\nip link set br0 address aa:bb:cc:00:00:64\nip link add vxlan0 type vxlan dstport 4789 local 100.64.0.1 nolearning external vnifilter\nip link set vxlan0 addrgenmode none master br0\nip link set vxlan0 address aa:bb:cc:00:00:64\nip link set br0 up\nip link set vxlan0 up\nbridge link set dev vxlan0 vlan_tunnel on neigh_suppress on learning off\n\n# Create the IP-VRF\n# Map VLAN 11 &lt;-&gt; VNI 101\nbridge vlan add dev br0 vid 11 self\nbridge vlan add dev vxlan0 vid 11\nbridge vni add dev vxlan0 vni 101\nbridge vlan add dev vxlan0 vid 11 tunnel_info id 101\n\n# 802.1Q sub-interface for routing\nip link add br0.11 link br0 type vlan id 11\nip link set br0.11 address aa:bb:cc:00:00:64 addrgenmode none\n\n# Bind to the UDN VRF\nip link add blue type vrf table 10\nip link set br0.11 master blue\nip link set br0.11 up\nip link set blue up\n\n## Create the MAC-VRF\n# 1. Map VLAN 12 &lt;-&gt; VNI 100 on the SVD bridge\nbridge vlan add dev br0 vid 12 self\nbridge vlan add dev vxlan0 vid 12\nbridge vni add dev vxlan0 vni 100\nbridge vlan add dev vxlan0 vid 12 tunnel_info id 100\n\n# 2. Connect OVS to the Linux Bridge\novs-vsctl add-port br-int blue -- set interface blue type=internal external-ids:iface-id=blue\nip link set blue master br0\nbridge vlan add dev blue vid 12 pvid untagged\nip link set blue up\n</code></pre></p> <p>The Linux configuration ends up looking like this:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 OVN-KUBERNETES DOMAIN \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                 \n\u2502                                                                                    \u2502                                 \n\u2502  +-----------+        +------------------+        +--------------------+           \u2502                                 \n\u2502  |  Pod(s)   |--------|  Logical Switch  |        |  Logical Router    |           \u2502                                 \n\u2502  +-----------+        +------------------+        +--------------------+           \u2502                                 \n\u2502                               \u2502                                                    \u2502                                 \n\u2502                               \u2502                                                    \u2502                                 \n\u2502          +-------------------------------------+   +-----------------------------+ \u2502                                 \n\u2502          |           OVS br-int                |   |   ovn-k8s-mpx (host iface)  | \u2502                                 \n\u2502          |  (OVN datapath for this UDN)        |   |   attached to VRF blue      | \u2502                                 \n\u2502          +-------------------------------------+   +-----------------------\u2502-----+ \u2502                                 \n\u2502                     \u2502  (blue internal port)                                \u2502       \u2502                                 \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2518                                 \n                      \u2502                                                      \u2502                                         \n                      \u2502                                                      \u2502                                         \n                      \u2502                                                      \u2502                                         \n                      \u2502                                                      \u2502                                         \n                      \u2502                                                      \u2502                                         \n                      \u2502                                                      \u2502                                         \n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 LINUX HOST / EVPN STACK \u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \n\u2502                     \u2502                                                      \u2502                                      \u2502  \n\u2502             ________|                                                      \u2502                                      \u2502  \n\u2502            \u2502                                                               \u2502                                      \u2502  \n\u2502            \u2502                                                               \u2502                                      \u2502  \n\u2502            \u2502 vlan 12 (VNI 100)                                             \u2502                                      \u2502  \n\u2502            \u25bc                                                               \u2502                                      \u2502  \n\u2502  +----------------------+      +----------------------------------+       +\u25bc------------------------------+       \u2502  \n\u2502  |     br0 (SVD)        |\u2500\u2500\u2500\u2500\u2500\u2500|  br0.11 (VLAN 11 / L3VNI 101)   |\u2500\u2500\u2500\u2500\u2500\u2500&gt;|  VRF blue  (IP-VRF for UDN)   |        \u2502  \n\u2502  |  vlan_filtering=1    |      +----------------------------------+       +-------------------------------+       \u2502  \n\u2502  |  vxlan0 master port  |                                                                               \u25b2         \u2502  \n\u2502  +----------------------+                                                                               \u2502         \u2502  \n\u2502            \u2502 vlan\u2192vni mappings                                                                          \u2502         \u2502  \n\u2502            \u25bc                                                                                            \u2502         \u2502  \n\u2502     +-----------------+                                                                                 \u2502         \u2502  \n\u2502     |   vxlan0        |  local 100.64.0.1 dstport 4789 external                                         \u2502         \u2502  \n\u2502     +-----------------+                                                                                 \u2502         \u2502  \n\u2502            \u2502  (VTEP)                                                                                    \u2502         \u2502  \n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \n\u2502                                        Encapsulated EVPN traffic (VNI 100 &amp; 101)                                  \u2502  \n\u2502                                             via 100.64.0.1 &lt;-&gt; 100.64.0.x peers                                   \u2502  \n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n</code></pre> <p>The MAC address for the bridge is unique and will be automatically generated by OVN-Kubernetes. This MAC address is known as the \"router-mac\" and is used by Type 5 routes to know what the destination MAC of the next hop should be. The MAC can be the same on every node, but must be different per UDN.</p> <p>Furthermore, bridge and VXLAN link names may also change and will be decided by OVN-Kubernetes. </p> <p>While the IP-VRF uses pure routing to transmit traffic over the EVPN fabric, MAC-VRF relies on layer 2. For that reason, the layer 2 OVN network needs to be extended into the EVPN fabric. To do that, we connect br-int to the linux bridge for the MAC-VRF. This will allow layer 2 traffic to travel through br-blue and then eventually into the EVPN fabric via the VNID 100. This enables a user to disable the Geneve overlay and allow L2 communication between UDNs on different nodes via the MAC-VRF:</p> <p> </p> <p>Note, the Layer 2 domain for the MAC-VRF may be extended into the provider's physical network, and not just extended across Kubernetes nodes. This allows for VM migration and other layer 2 connectivity between entities external to the cluster and entities within.</p> <p>ovnkube-controller will be responsible for configuring OVN, including the extra OVS internal port attached to the worker logical switch.</p> <p>In addition to VTEP IP allocation, ovnkube-cluster-manager will be responsible for generating FRR-K8S config to enable FRR with EVPN. The config for the above example would look something like this:</p> <p><pre><code>vrf blue\n vni 101\n rd 65000:101\nexit-vrf\n!\nrouter bgp 65000\n !\n ! Peer with spine using eBGP\n neighbor 192.168.122.12 remote-as 65001\n !\n address-family ipv4 unicast\n  network 100.64.0.1/32\n exit-address-family\n ! MAC-VRF config start\n address-family l2vpn evpn\n  vni 100\n    rd 65000:100\n    route-target import 65000:100\n    route-target export 65000:100\n  ! \n  neighbor 192.168.122.12 activate\n  advertise-all-vni\n exit-address-family\nexit\n! MAC-VRF config end\n! IP-VRF config start\nrouter bgp 65000 vrf blue\n !\n address-family ipv4 unicast\n  network 10.0.10.0/24\n exit-address-family\n !\n !\n address-family l2vpn evpn\n  advertise ipv4 unicast\n  route-target import 65000:101\n  route-target export 65000:101\n exit-address-family\nexit\n! IP-VRF config end\n</code></pre> The MAC-VRF configuration for the Layer 2 UDN in the example above is contained within the <code>address-family l2vpn evpn</code> stanza under the global <code>router bgp 65000</code>, denoted with the comments showing the beginning and end of the MAC-VRF configuration. FRR automatically detects VNIs via netlink via the <code>advertise-all-vni</code> configuration, and it is not required to specify the MAC-VRFs that should use EVPN. However, the <code>vni 100</code> section explicitly lists the vni in order to explicitly configure the route-target. The <code>rd 65000:100</code> line specifies the route distiguisher. The route distinguisher is used uniquely identify routes for each VPN. This value is chosen automatically by OVN-Kubernetes in the format <code>&lt;AS Number&gt;:&lt;VNI&gt;</code>, which is the same format used by FRR itself. The entirety of this configuration section will enable Type 2 and Type 3 EVPN routes to be advertised.</p> <p>The IP-VRF configuration starts with the <code>router bgp 65000 vrf blue</code> stanza. The stanza indicates to FRR that VNI 101 is an IP-VRF and that it should advertise type 5 routes for the subnet <code>10.0.10.0/24</code>. Notice that the entire Layer 2 UDN subnet is advertised from each node. This can result in suboptimal routing as there may be an extra hop involved to deliver packets from the ECMP chosen path for 10.244.0.0/16 to the node where the pod actually lives. This can be mitigated in the future by advertising either static or kernel /32 routes for each pod IP on the node, however that is outside the scope of this enhancement.</p> <p>The RouteAdvertisements CRD will still work in conjunction with the EVPN CRD to determine what IPs should be advertised.</p>"},{"location":"okeps/okep-5088-evpn/#ip-vrf-layer-3-udn","title":"IP-VRF + Layer 3 UDN","text":"<p>A Layer 3 UDN with an IP-VRF is really just a subset of the previous example, as far as configuration of the node:</p> <pre><code># VTEP IP assignment to loopback - only done in VTEP managed mode\nip addr add 100.64.0.1/32 dev lo\n\n# SVD bridge + VXLAN setup\nip link add br0 type bridge vlan_filtering 1 vlan_default_pvid 0\nip link set br0 addrgenmode none\nip link set br0 address aa:bb:cc:00:00:64\nip link add vxlan0 type vxlan dstport 4789 local 100.64.0.1 nolearning external vnifilter\nip link set vxlan0 addrgenmode none master br0\nip link set vxlan0 address aa:bb:cc:00:00:64\nip link set br0 up\nip link set vxlan0 up\nbridge link set dev vxlan0 vlan_tunnel on neigh_suppress on learning off\n\n# Create the IP-VRF\n# Map VLAN 11 &lt;-&gt; VNI 101\nbridge vlan add dev br0 vid 11 self\nbridge vlan add dev vxlan0 vid 11\nbridge vni add dev vxlan0 vni 101\nbridge vlan add dev vxlan0 vid 11 tunnel_info id 101\n\n# 802.1Q sub-interface for routing\nip link add br0.11 link br0 type vlan id 11\nip link set br0.11 address aa:bb:cc:00:00:64 addrgenmode none\n\n# Bind to the UDN VRF\nip link add blue type vrf table 10\nip link set br0.11 master blue\nip link set br0.11 up\nip link set blue up\n</code></pre> <p>Note, it is not required to wire the OVN logical switch to the linux bridge in this case. It is also not required to modify routes in ovn_cluster_router. Pod egress traffic should be rerouted towards mpx as is done today with BGP.</p> <p>The FRR configuration remains almost the same as the previous example, but with IP-VRF we no longer need the <code>address-family l2vpn evpn</code> section under the global <code>router bgp 65000</code> section anymore:</p> <pre><code>vrf blue\n vni 101\n rd 65000:101\nexit-vrf\n!\nrouter bgp 65000\n !\n ! Peer with spine using eBGP\n neighbor 192.168.122.12 remote-as 65001\n !\n address-family ipv4 unicast\n  network 100.64.0.1/32\n exit-address-family\nexit\n! IP-VRF config start\nrouter bgp 65000 vrf blue\n !\n address-family ipv4 unicast\n  network 10.0.10.0/24\n exit-address-family\n !\n address-family l2vpn evpn\n  advertise ipv4 unicast\n  route-target import 65000:101\n  route-target export 65000:101\n exit-address-family\nexit\n! IP-VRF config end\n</code></pre> <p>An IP-VRF with a layer 3 UDN would look something like this:</p> <p></p> <p>In this case each node has its own layer 2 domain, and routing is used via the IP-VRF for inter-node UDN communication. </p>"},{"location":"okeps/okep-5088-evpn/#mac-vrf-layer-2-udn","title":"MAC-VRF + Layer 2 UDN","text":"<p>With only a MAC-VRF it is also a subset of the previous node configuration:</p> <pre><code># VTEP IP assignment to loopback - only done in VTEP managed mode\nip addr add 100.64.0.1/32 dev lo\n\n# SVD bridge + VXLAN setup\nip link add br0 type bridge vlan_filtering 1 vlan_default_pvid 0\nip link set br0 addrgenmode none\nip link set br0 address aa:bb:cc:00:00:64\nip link add vxlan0 type vxlan dstport 4789 local 100.64.0.1 nolearning external vnifilter\nip link set vxlan0 addrgenmode none master br0\nip link set vxlan0 address aa:bb:cc:00:00:64\nip link set br0 up\nip link set vxlan0 up\nbridge link set dev vxlan0 vlan_tunnel on neigh_suppress on learning off\n\n## Create the MAC-VRF\n# 1. Map VLAN 12 &lt;-&gt; VNI 100 on the SVD bridge\nbridge vlan add dev br0 vid 12 self\nbridge vlan add dev vxlan0 vid 12\nbridge vni add dev vxlan0 vni 100\nbridge vlan add dev vxlan0 vid 12 tunnel_info id 100\n\n# 2. Connect OVS to the Linux Bridge\novs-vsctl add-port br-int blue -- set interface blue type=internal external-ids:iface-id=blue\nip link set blue master br0\nbridge vlan add dev blue vid 12 pvid untagged\nip link set blue up\n</code></pre> <p>The FRR configuration is also a subset of the original config:</p> <pre><code>router bgp 65000\n ! Peer with spine using eBGP\n neighbor 192.168.122.12 remote-as 65001\n !\n address-family ipv4 unicast\n  network 100.64.0.1/32\n exit-address-family\n !\n address-family l2vpn evpn\n  vni 100\n    rd 65000:100\n    route-target import 65000:100\n    route-target export 65000:100\n  !\n  neighbor 192.168.122.12 activate\n  advertise-all-vni\n exit-address-family\nexit\n</code></pre> <p>Notice the vrf stanzas are no longer needed.</p> <p>Architecturally, the traffic pattern and topology will look the same as the diagram in the MAC-VRF + IP-VRF Combination with Layer 2 UDN section.</p>"},{"location":"okeps/okep-5088-evpn/#feature-compatibility","title":"Feature Compatibility","text":""},{"location":"okeps/okep-5088-evpn/#multiple-external-gateways-meg","title":"Multiple External Gateways (MEG)","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5088-evpn/#egress-ip","title":"Egress IP","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5088-evpn/#services","title":"Services","text":"<p>Full support for cluster IP access from pods. Limited support for external service access (node port, external IP, load balancer IP). MetalLB does not currently have a way to specify per VRF advertisements to the same BGP peer. Therefore, for users to advertise an external IP or LoadBalancer IP, they must configure FRR-K8S to manually advertise it. A user may also configure FRR-K8S to advertise cluster IP as well externally into the EVPN fabric.</p> <p>MetalLB may be extended in the future to support per-VRF advertisement.</p>"},{"location":"okeps/okep-5088-evpn/#egress-service","title":"Egress Service","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5088-evpn/#egress-firewall","title":"Egress Firewall","text":"<p>Full support.</p>"},{"location":"okeps/okep-5088-evpn/#egress-qos","title":"Egress QoS","text":"<p>Full support.</p>"},{"location":"okeps/okep-5088-evpn/#network-policyanp","title":"Network Policy/ANP","text":"<p>Full support.</p>"},{"location":"okeps/okep-5088-evpn/#ipsec","title":"IPSec","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5088-evpn/#multicast","title":"Multicast","text":"<p>IP-VRF will not be supported for Multicast. Although OVN can handle router forwarding of multicast traffic, the kernel routing will not be configured correctly. Multicast within IP-VRF is a less common use case than MAC-VRF. Multicast within a MAC-VRF will be supported. The OVN logical switch will handle IGMP snooping and forwarding unknown multicast with the logical_switch configuration set to <code>other_config:mcast_flood_unregistered=true</code> and <code>other_config:mcast_snoop=true</code>. The linux bridge will only have the VXLAN VTEP and OVN internal port connected to it, so it is not necessary to configure IGMP snooping there. Type 3 EVPN routes will be sent to announce VPN/VTEP membership and trigger ingress replication to flood multicast packets to the right VTEPs across the fabric. The linux bridge will simply \"flood\" the packet to the OVS internal port to the OVN logical switch. The OVN logical switch will then rely on IGMP snooping to limit which pods to send the packet to.</p>"},{"location":"okeps/okep-5088-evpn/#testing-details","title":"Testing Details","text":"<p>The EVPN feature will require E2E tests to be written which will simulate a spine and leaf topology that the KIND Kubernetes nodes are attached to. From there tests will be added that will create UDN+BGP+EVPN and test the following:</p> <ol> <li>UDN pods are able to talk to external applications on the same VPN.</li> <li>UDN pods are unable to talk to external applications on a different VPN.</li> <li>UDN pods are able to talk to other UDN pods on the same network without a Geneve overlay via EVPN.</li> <li>The above tests will apply for both IP-VRF and MAC-VRF EVPN types.</li> <li>For IP+MAC-VRF, a test will be added to ensure VM migration between two Kubernetes nodes with EVPN. This includes ensuring TCP connections are not broken as well as minimal packet loss during migration.</li> <li>Testing with Multicast on MAC-VRFs.</li> </ol>"},{"location":"okeps/okep-5088-evpn/#documentation-details","title":"Documentation Details","text":"<p>BGP documentation (including a user guide) needs to be completed first with details around how to configure with UDN. Following up on that EVPN documentation will be added to show users how configure EVPN and have it integrate with a spine and leaf topology.</p>"},{"location":"okeps/okep-5088-evpn/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<p>Interoperability with external provider networks EVPN infrastructure. Although BGP+EVPN is a standardized protocol, there may be nuances where certain features are not available or do not work as expected in FRR. There is no current FRR development expertise in our group, so we will have to rely on FRR community for help as we ramp up.</p> <p>Same drawbacks exist here that are highlighted in the BGP enhancement. Namely:</p> <ul> <li>Increased complexity of our SDN networking solution to support more complex networking.</li> <li>Increases support complexity due to integration with the user's provider network.</li> </ul> <p>Other considerations include FRR deployment. If the default cluster network is relying on EVPN or BGP to provide network connectivity, then FRR must be started and bootstrapped by the time the kubelet comes up. This includes considerations around node reboot, as well as fresh cluster install. The means to manage, deploy and maintain FRR is outside the scope of OVN-Kubernetes, but may be handled by another platform specific operator. For example, MetalLB may be used to install FRR for day 2 operations.</p> <p>Limitations include support for Local gateway mode only.</p> <p>There are other aspects to consider around Kubernetes services. Today OVN-Kubernetes works with MetalLB, and MetalLB is responsible for advertising externally exposed services across the BGP fabric. In OVN-Kubernetes we treat services as though they belong to a specific UDN. This is due to the fact that a service is namespace scoped, and namespace belongs to either the default cluster network or a UDN. However, when a user exposes a service externally via a nodeport, loadbalancer, or external IP; that service is now reachable externally over the default VRF (advertised via MetalLB). MetalLB has no concept of VRFs or UDNs, but it could be extended to allow advertising services in different VRFs. Until this support exists, external services may not be advertised by MetalLB over non-default VRF EVPNs. However, it may be desirable for OVN-Kubernetes to fill this void somewhat, by advertising the cluster IP of services on UDNs via BGP. This can be a future enhancement after we figure out what can be done in MetalLB.</p> <p>MEG will not be supported with EVPN as MEG is only supported in shared gateway mode, while EVPN is limited to local gateway mode.</p>"},{"location":"okeps/okep-5088-evpn/#ovn-kubernetes-version-skew","title":"OVN Kubernetes Version Skew","text":"<p>TBD</p>"},{"location":"okeps/okep-5088-evpn/#alternatives","title":"Alternatives","text":"<p>There is another alternative called OpenPERouter which manages EVPN FRR configuration in another Linux network namespace. A VRF-Lite style configuration is done by connecting veth devices between the default Linux network namespace and the OpenPERouter namespace. The FRR in the default network namespace peers with this OpenPERouter in order to advertise routes from the host. For OVN-Kubernetes to integrate, we would need to configure VRF-Lite on our side and connect to the OpenPERouter. While this does provide an easy way to plugin, the OpenPERouter approach has several major drawbacks:</p> <ol> <li> <p>Lack of OVN integration - We eventually want shared gateway mode to work, where OVN will be handling encapsulation of packets. This enables things like hardware offload. This is not possible with OpenPERouter.</p> </li> <li> <p>Management complexity - Adding additional FRR instance running in another network namespace, and plumbing virtual interfaces between the host/OVN and this other namespace increases resource footprint, and makes it harder to debug and manage. The Linux interfaces on the host must be moved to this other namespace as well.</p> </li> </ol> <p>For OVN-Kubernetes, the cost of implementing the code to manage the netdevs, and configure FRR-K8S outweighs the drawbacks listed above, and therefore OpenPERouter is not a viable alternative.</p>"},{"location":"okeps/okep-5088-evpn/#references","title":"References","text":"<ul> <li>FRR EVPN Configuration Guide</li> </ul>"},{"location":"okeps/okep-5094-layer2-transit-router/","title":"OKEP-5094: Primary UDN Layer2 topology improvements","text":"<ul> <li>Issue: #5094</li> </ul>"},{"location":"okeps/okep-5094-layer2-transit-router/#problem-statement","title":"Problem Statement","text":"<p>The primary UDN layer2 topology presents some problems related to VM's live migration that are being addressed by ovn-kubernetes sending GARPs or unsolicited router advertisement and blocking some OVN router advertisements, although this fixes the issue, is not the most robust way to address the problem and adds complexity to ovn-kubernetes live migration mechanism.</p> <p>EgressIP logical router policies (LRP) for layer2 networks are applied on the gateway router, compared to the cluster router for layer3 networks. To ensure proper traffic load balancing for EgressIPs with multiple IPs defined, especially for pods running on the EgressIP nodes themselves, a workaround was introduced. This workaround configures the LRP for a pod on one of the EgressIP nodes to use the external gateway IP as one of the next hops to achieve proper load balancing.</p> <p>For the following EgressIP:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: EgressIP\n  metadata:\n    annotations:\n      k8s.ovn.org/egressip-mark: \"50000\"\n  name: egressip-1\n...\n  spec:\n    egressIPs:\n    - 172.18.0.100\n    - 172.18.0.101\n...\n  status:\n    items:\n    - egressIP: 172.18.0.100\n      node: ovn-worker\n    - egressIP: 172.18.0.101\n      node: ovn-worker2\n</code></pre> <p>The following LRP is present on ovn-worker for a local pod with IP <code>10.10.0.8</code>:</p> <pre><code>ovn-nbctl lr-policy-list GR_cluster_udn_l2network_ovn-worker\nRouting Policies\n       100                               ip4.src == 10.10.0.8         reroute                100.65.0.3, 172.18.0.1               pkt_mark=50000\n</code></pre> <p>This policy redirects traffic the local external gateway <code>172.18.0.1</code>, or  to the join IP of the second egress node (ovn-worker2) <code>100.65.0.3</code>.</p> <p>While this approach works in most cases, it has the following limitations:</p> <ul> <li>Does not work when there's no default gateway.</li> <li>Does not work on platforms that use <code>/32</code> per node.</li> <li>Does not respect multiple gateways and always sends traffic to one of the gateways.</li> </ul> <p>We can make use of the new transit router OVN topology entity to fix these issues and changing the topology for primary UDN layer2.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#goals","title":"Goals","text":"<ol> <li>For layer2 topology advertise default gw with same IP and MAC address independently of the node where    the vm is running.</li> <li>Keep all the layer2 topology features at current topology.</li> <li>Eliminate the dependency on external gateway IPs from layer2 EgressIP implementation.</li> <li>Make the new topology upgradable with minor disruption.</li> </ol>"},{"location":"okeps/okep-5094-layer2-transit-router/#non-goals","title":"Non-Goals","text":"<ol> <li>Support non interconnect or interconnect with multiple nodes per zone.</li> <li>Extend the layer2 topology changes to other topologies.</li> </ol>"},{"location":"okeps/okep-5094-layer2-transit-router/#introduction","title":"Introduction","text":""},{"location":"okeps/okep-5094-layer2-transit-router/#layer2-default-gw-discovery-at-vms","title":"Layer2 default gw discovery at VMs","text":"<p>Currently at layer2 topology the virtual machine related to default gw routing looks like the following for ipv4, where the .1 address is configured using DHCP and pods send an ARP that is only answered by the local gateway router with the gateway router mac.</p> <pre><code>flowchart TD\n    classDef vmStyle fill:#0000FF,stroke:#ADD8E6,stroke-width:2px;\n\n    subgraph node1[\"node1\"]\n        GR-node1[\"GR-node1\"]\n        layer2-switch[\"Layer2 Switch\"]\n        subgraph VM[\"Virtual Machine\"]\n            class VM vmStyle;\n            route[\"default gw -&gt; 203.203.0.1 (0a:58:64:41:00:02)\"]\n        end\n        VM --&gt;|\"ARP 203.203.0.1\"|layer2-switch\n    end\n    GR-node1 &lt;--&gt;|\"router 203.203.0.1  (0a:58:64:41:00:02)\"| layer2-switch\n    GR-node2 &lt;--&gt;|\"remote 100.65.0.3   (0a:58:64:41:00:03)\"| layer2-switch\n    GR-node3 &lt;--&gt;|\"remote 100.65.0.4   (0a:58:64:41:00:04)\"| layer2-switch\n\n    class VM vmStyle;\n</code></pre> <pre><code>$ ip route\ndefault via 203.203.0.1 dev eth0 proto dhcp metric 100\n\n$ ip neigh\n203.203.0.1 dev eth0 lladdr 0a:58:64:41:00:02 REACHABLE\n</code></pre> <p>And this is how it looks for IPv6 where the RFC dictates the default gw route is advertised with the link local address. So every gateway router connected to the switch will send a router advertisement after receiving the router solicitation from the virtual machine. <pre><code>flowchart TD\n    classDef vmStyle fill:#0000FF,stroke:#ADD8E6,stroke-width:2px;\n\n    subgraph node1[\"node1\"]\n        GR-node1[\"GR-node1\"]\n        layer2-switch[\"Layer2 Switch\"]\n        subgraph VM[\"Virtual Machine\"]\n            class VM vmStyle;\n            route[\"default gw fe80::858:64ff:fe41:2 fe80::858:64ff:fe41:3 fe80::858:64ff:fe41:4\"]\n        end\n        layer2-switch--&gt; VM\n\n    end\n    GR-node1 --&gt;|\"RA fe80::858:64ff:fe41:2\"| layer2-switch\n    GR-node2 --&gt;|\"RA fe80::858:64ff:fe41:3\"| layer2-switch\n    GR-node3 --&gt;|\"RA fe80::858:64ff:fe41:4\"| layer2-switch\n\n    class VM vmStyle;\n</code></pre></p> <pre><code>$ ip -6 route\ndefault proto ra metric 100 pref low\n    nexthop via fe80::858:64ff:fe41:2 dev eth0 weight 1\n    nexthop via fe80::858:64ff:fe41:3 dev eth0 weight 1\n    nexthop via fe80::858:64ff:fe41:4 dev eth0 weight 1\n\n$ ip neigh\nfe80::858:64ff:fe41:3 dev eth0 lladdr 0a:58:64:41:00:03 router STALE\nfe80::858:64ff:fe41:4 dev eth0 lladdr 0a:58:64:41:00:04 router STALE\nfe80::858:64ff:fe41:2 dev eth0 lladdr 0a:58:64:41:00:02 router STALE\n</code></pre> <p>This is a view of the logical router ports connected to the switch, take into account that the 203.203.0.1 is only \"propagated\" on the node where the vm is running: <pre><code>$ ovnk ovn-control-plane ovn-nbctl show GR_test12_namespace.scoped_ovn-control-plane\nrouter 2b9a5f29-ef44-4bda-8d39-45198353013b (GR_test12_namespace.scoped_ovn-control-plane)\n    port rtos-test12_namespace.scoped_ovn_layer2_switch\n        mac: \"0a:58:64:41:00:03\"\n        ipv6-lla: \"fe80::858:64ff:fe41:3\"\n        networks: [\"100.65.0.3/16\", \"2010:100:200::1/60\", \"203.203.0.1/16\", \"fd99::3/64\"]\n\n$ ovnk ovn-worker ovn-nbctl show GR_test12_namespace.scoped_ovn-worker\nrouter dbbb9301-2311-4d2f-bfec-64e1caf78b8e (GR_test12_namespace.scoped_ovn-worker)\n    port rtos-test12_namespace.scoped_ovn_layer2_switch\n        mac: \"0a:58:64:41:00:02\"\n        ipv6-lla: \"fe80::858:64ff:fe41:2\"\n        networks: [\"100.65.0.2/16\", \"2010:100:200::1/60\", \"203.203.0.1/16\", \"fd99::2/64\"]\n\n$ ovnk ovn-worker2 ovn-nbctl show GR_test12_namespace.scoped_ovn-worker2\nrouter 148b41ca-3641-449e-897e-0d63bf395233 (GR_test12_namespace.scoped_ovn-worker2)\n    port rtos-test12_namespace.scoped_ovn_layer2_switch\n        mac: \"0a:58:64:41:00:04\"\n        ipv6-lla: \"fe80::858:64ff:fe41:4\"\n        networks: [\"100.65.0.4/16\", \"2010:100:200::1/60\", \"203.203.0.1/16\", \"fd99::4/64\"]\n</code></pre></p> <p>So the gist of it is that the default gw ip4 and ipv6 is dependent of where the VM is running, and that has important implications.</p> <p>Also having a multipath IPv6 default gateway means that the egress traffic is load balanced between nodes, across the geneve interconnect.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#virtual-machine-live-migration-and-default-gateway","title":"Virtual machine live migration and default gateway","text":"<p>When a virtual machine is live migrated, it is transferred from the node where it is running to a different one, in this case it can be from node1 to node2.</p> <p>After live migration has finished and the VM is running on a different node, the VM does not initiate any type of ARP or Router Solicitation to reconcile routes since from its point of view nothing has changed. This means it's running with the same network configuration, the consequence of that is that the VM will continue running with its default IPv4 gateway mac address pointing to node1 and for ipv6 it will continue to be the multipath default gw.</p> <p>One common scenario that triggers user live migrating VMs is related to doing some kind of node maintenance where the node need to go down. The VM is live migrated to a different node, then the node where it was original running is shutdown and some maintenance (e.g., hardware changes) is done before starting it up again.</p> <p>With that scenario in mind, after VM has live migrated: - the default IPv4 default gateway mac will point to a node that is currently down - one of default IPv6 gateway paths will be pointing to a node that is currently down.</p> <p>To fix that for IPv4, ovn-kubernetes sends a GARP after live migration to reconcile the default gw mac to the new node where the VM is running Pull Request 4964.</p> <p>For ipv6 there are changes to do something similar by blocking external gateway routers RAs Pull Request 4852 and reconciling gateways with unsolicited router advertisements Pull Request 4847.</p> <p>Although these fixes work, they are not very robust since messages can be lost or blocked so gateway do not get reconciled.</p> <p>This is how the topology will look after the virtual machine has being live migrated from node1 to node2 and shutting down node1 after it.</p> <p>ipv4: <pre><code>flowchart TD\n    classDef vmStyle fill:#0000FF,stroke:#ADD8E6,stroke-width:2px;\n    subgraph node1[\"node1\n    DOWN\"]\n        GR-node1[\"GR-node1 (0a:58:64:41:00:02)\"]\n    end\n    subgraph node2\n        GR-node2[\"GR-node2 (0a:58:64:41:00:03)\"]\n        subgraph VM[\"Virtual Machine\"]\n\n            class VM vmStyle;\n            route[\"default gw -&gt; 203.203.0.1 old node1 instead of node2\n            (0a:58:64:41:00:02)\"]\n        end\n        VM --&gt;layer2-switch\n    end\n     subgraph node3\n        GR-node3[\"GR-node3 (0a:58:64:41:00:04)\"]\n    end\n    GR-node1 &lt;--&gt;|\"remote 100.65.0.2  (0a:58:64:41:00:02)\"| layer2-switch\n    GR-node2 &lt;--&gt;|\"router 203.203.0.1   (0a:58:64:41:00:03)\"| layer2-switch\n    GR-node3 &lt;--&gt;|\"remote 100.65.0.4   (0a:58:64:41:00:04)\"| layer2-switch\n\n    class VM vmStyle;\n</code></pre></p> <p>ipv6: <pre><code>flowchart TD\n    classDef vmStyle fill:#0000FF,stroke:#ADD8E6,stroke-width:2px;\n\n    subgraph node1[\"node1 (DOWN)\"]\n\n        GR-node1[\"GR-node1 (fe80::858:64ff:fe41:2)\"]\n    end\n    subgraph node2\n        GR-node2[\"GR-node2 (fe80::858:64ff:fe41:3)\"]\n        layer2-switch[\"Layer2 Switch\"]\n        subgraph VM[\"Virtual Machine\"]\n\n            class VM vmStyle;\n            route[\"default gw\n            down(fe80::858:64ff:fe41:2)\n            fe80::858:64ff:fe41:3\n            fe80::858:64ff:fe41:4\"]\n        end\n        layer2-switch--&gt; VM\n    end\n    subgraph node3\n        GR-node3[\"GR-node3 (fe80::858:64ff:fe41:4)\"]\n    end\n\n    GR-node2 --&gt; layer2-switch\n    GR-node3 --&gt; layer2-switch\n\n    class VM vmStyle;\n</code></pre></p>"},{"location":"okeps/okep-5094-layer2-transit-router/#layer2-topology-limitations-for-eip","title":"Layer2 topology limitations for EIP","text":"<p>The current layer2 topology has some limitations with Egress IP. When multiple IPs are assigned to an Egress IP, and a pod is local to one of the egress nodes, only the egress path local to that node will be used.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":""},{"location":"okeps/okep-5094-layer2-transit-router/#story-1-seamless-live-migration","title":"Story 1: seamless live migration","text":"<p>As a kubevirt user, I want to live migrate a virtual machine using layer2 primary UDN, so that TCP connections to the external network are not broken and downtime is minimum with network configuration not being changed within the virtual machine.</p> <p>For example: User has a virtual machine serving a video conference using TCP connection and the node where is running needs to be shut down, so user does a live migration to move to other nodes, the video should continue with minimum downtime without changing virtual machine network configuration.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#story-2-eip-for-layer2-limitations","title":"Story 2: EIP for layer2 limitations","text":"<p>As an EIP L2 UDN user, new connections from a pod should be balanced correctly over multiple Egress IPs.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#proposed-solution","title":"Proposed Solution","text":"<p>The OVN community did introduce a new network topology element transit router that allows logical routers that are shared between OVN availability zones, this make possible to use a cluster router similar to layer3 topology ovn_cluster_router for layer2 so the logical router port that is connected to the layer2 switch will have just the .1 address and mac and ipv6 lla generated with it.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#ports-switches-and-routers-topology","title":"Ports, switches and routers topology","text":"<p>This is an overview of the topology with the transit subnet used to connect the transit_router and gateway routers directly (without a new logical switch - direct router port connections using the NB.Logical_Router_Port.peer field between the GR and the new transit_router).</p> <p>OVN routers cannot have multiple ports in the same subnet, so the trtor (transit router to gateway router) ports of the transit_router need to have the minimal possible subnet, to accommodate at least 2 IPs one per peer, from the transit switch subnet to connect the two peers, trtor (transit_router) &lt;-&gt; rtotr (GR).</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#ipv4","title":"IPv4","text":"<p>Address config <pre><code>subnet: 203.203.0.0/24\njoin-subnet: 100.65.0.0/16\ntransit-subnet: 100.88.0.0/16\ntransit-peers-node-subnet:\n  node1: 100.88.0.4/31\n  node2: 100.88.0.8/31\n  node3: 100.88.0.6/31\n</code></pre></p> <p>The transit per node subnet has to reserve 2 addresses for the transit router side and gateway router side. </p> <p>Let's inspect node1 transit-peers-node-subnet with the <code>100.88.0.4/31</code> subnet:  GR and transit_router peers ports should use <code>100.88.0.4</code> and <code>100.88.0.5</code>.</p> <pre><code>%%{init: {\"nodeSpacing\": 20, \"rankSpacing\": 100}}%%\nflowchart TD\n    classDef nodeStyle fill:orange,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef vmStyle fill:blue,stroke:none,color:white,rx:10px,ry:10px,font-size:25px;\n    classDef portStyle fill:#3CB371,color:black,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef routerStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef switchStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef termStyle font-family:monospace,fill:black,stroke:none,color:white;\n    subgraph node1[\"node1\"]\n        subgraph GR-node1\n            rtotr-GR-node1[\"trtor-GR-node1\n            100.65.0.2/16 100.88.0.5/31 (0a:58:64:41:00:02)\"]\n        end\n        subgraph VM[\"Virtual Machine\"]\n            class VM vmStyle;\n            term[\"default gw\n            203.203.0.1\n            (0a:58:CB:CB:00:01)\"]\n        end\n    end\n    subgraph node2\n        subgraph GR-node2\n            rtotr-GR-node2[\"rtotr-GR-node2 100.65.0.3/16 100.88.0.9/31 (0a:58:64:41:00:03)\"]\n        end\n    end\n    subgraph node3\n        subgraph GR-node3\n            rtotr-GR-node3[\"rtotr-GR-node3 100.65.0.4/16 100.88.0.7/31 (0a:58:64:41:00:04)\"]\n        end\n    end\n    subgraph layer2-switch\n        stor-transit_router[\"stor-transit_router\n        type: router\"]\n    end\n    subgraph transit_router[\"transit_router \"]\n        trtor-GR-node1[\"trtor-GR-node1 100.88.0.4/31\"]\n        trtor-GR-node2[\"trtor-GR-node2 100.88.0.8/31\"]\n        trtor-GR-node3[\"trtor-GR-node3 100.88.0.6/31\"]\n        rtos-layer2-switch[\"rtos-layer2-switch 203.203.0.1 (0a:58:CB:CB:00:01)\"]\n    end\n    rtotr-GR-node1 &lt;--&gt; trtor-GR-node1\n    rtotr-GR-node2 &lt;--&gt; trtor-GR-node2\n    rtotr-GR-node3 &lt;--&gt; trtor-GR-node3\n    VM &lt;--&gt;layer2-switch\n    rtos-layer2-switch &lt;--&gt; stor-transit_router\n\n    class VM vmStyle;\n    class rtotr-GR-node1 portStyle;\n    class rtotr-GR-node2 portStyle;\n    class rtotr-GR-node3 portStyle;\n    class trtor-GR-node1 portStyle;\n    class trtor-GR-node2 portStyle;\n    class trtor-GR-node3 portStyle;\n    class stor-transit_router portStyle;\n    class rtos-layer2-switch portStyle;\n    class GR-node1 routerStyle;\n    class GR-node2 routerStyle;\n    class GR-node3 routerStyle;\n    class transit_router routerStyle;\n    class layer2-switch switchStyle\n    class term termStyle;\n    class node1,node2,node3 nodeStyle;\n\n</code></pre> <p>As shown by the topology the VM default gw IP is the first address from the subnet <code>203.203.0.1</code> and the mac address is computed from it so this makes it independent of where the VM is running.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#ipv6","title":"IPv6","text":"<p>Address config <pre><code>subnet: 2010:100:200::0/60\njoin-subnet: fd99::/64\ntransit-subnet: fd97::/64\ntransit-peers-node-subnet:\n  node1: fd97::8/127\n  node2: fd97::6/127\n  node3: fd97::4/127\n</code></pre> In the case of IPv6 there is no broadcast concept so reserving two addresses (one bit) is enough that means the transit-peers-node-subnet size should be <code>128 - 1 = 127</code>.</p> <p>Again <code>ipcalc</code> we can inspect the subnet for node1: <pre><code>$ ipcalc fd97::8/127\nFull Network:   fd97:0000:0000:0000:0000:0000:0000:0008/127\nNetwork:    fd97::8/127\nNetmask:    ffff:ffff:ffff:ffff:ffff:ffff:ffff:fffe = 127\n\nAddress space:  Unique Local Unicast\nHostMin:    fd97::8\nHostMax:    fd97::9\nHosts/Net:  2\n</code></pre> In this case the node1 gateway and ovn_cluster routers peer ports should use <code>fd97::8</code> and <code>fd97::9</code>.</p> <pre><code>%%{init: {\"nodeSpacing\": 20, \"rankSpacing\": 100}}%%\nflowchart TD\n    classDef nodeStyle fill:orange,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef vmStyle fill:blue,stroke:none,color:white,rx:10px,ry:10px,font-size:25px;\n    classDef portStyle fill:#3CB371,color:black,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef routerStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef switchStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef termStyle font-family:monospace,fill:black,stroke:none,color:white;\n    subgraph node1[\"node1\"]\n        subgraph GR-node1\n            rtotr-GR-node1[\"trtor-GR-node1\n            fd97::9/127 fd99::4/64 (0a:58:64:41:00:02)\"]\n        end\n        subgraph VM[\"Virtual Machine\"]\n            class VM vmStyle;\n            term[\"default gw\n            fe80::858:cbff:fecb:1\n            (0a:58:CB:CB:00:01)\"]\n        end\n    end\n    subgraph node2\n        subgraph GR-node2\n            rtotr-GR-node2[\"rtotr-GR-node2 fd97::7/127 fd99::3/64 (0a:58:64:41:00:03)\"]\n        end\n    end\n    subgraph node3\n        subgraph GR-node3\n            rtotr-GR-node3[\"rtotr-GR-node3 fd97::5/127 fd99::2/64 (0a:58:64:41:00:04)\"]\n        end\n    end\n    subgraph layer2-switch\n        stor-transit_router[\"stor-transit_router\n        type: router\"]\n    end\n    subgraph transit_router[\"transit_router \"]\n        trtor-GR-node1[\"trtor-GR-node1 fd97::8/127\"]\n        trtor-GR-node2[\"trtor-GR-node2 fd97::6/127\"]\n        trtor-GR-node3[\"trtor-GR-node3 fd97::4/127\"]\n        rtos-layer2-switch[\"rtos-layer2-switch 2010:100:200::1 (ipv6-lla: fe80::858:cbff:fecb:1) (0a:58:CB:CB:00:01)\"]\n    end\n    rtotr-GR-node1 &lt;--&gt; trtor-GR-node1\n    rtotr-GR-node2 &lt;--&gt; trtor-GR-node2\n    rtotr-GR-node3 &lt;--&gt; trtor-GR-node3\n    VM &lt;--&gt;layer2-switch\n    rtos-layer2-switch &lt;--&gt; stor-transit_router\n\n    class VM vmStyle;\n    class rtotr-GR-node1 portStyle;\n    class rtotr-GR-node2 portStyle;\n    class rtotr-GR-node3 portStyle;\n    class trtor-GR-node1 portStyle;\n    class trtor-GR-node2 portStyle;\n    class trtor-GR-node3 portStyle;\n    class stor-transit_router portStyle;\n    class rtos-layer2-switch portStyle;\n    class GR-node1 routerStyle;\n    class GR-node2 routerStyle;\n    class GR-node3 routerStyle;\n    class transit_router routerStyle;\n    class layer2-switch switchStyle\n    class term termStyle;\n    class node1,node2,node3 nodeStyle;\n</code></pre> <p>According to RFC 4861/4862, routers advertise a link-local address (<code>fe80::/64</code>). Its Interface Identifier (IID) is derived from the router's MAC address (e.g., <code>fe80::858:cbff:fecb:1</code>). This LLA remains stable as a default gateway, independent of the virtual machine's hosting node.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#tunnel-keys-for-the-transit-switchesrouters","title":"Tunnel keys for the transit switches/routers","text":"<p>Every OVN switch and router has a unique tunnel key. It has to be manually allocated for all transit switches and routers, but \"regular\" (non-transit) switches and routers currently use automatic allocation by the northd. OVN has a reserved range of 2^16 for the transit switches and routers https://github.com/ovn-org/ovn/commit/969f7f54ea3868172f99119991d8d875bb8f240c that won't be assigned by the northd for the regular switches and routers to avoid conflict.</p> <p>We already use this key for the Layer3 transit switches and Layer2 switches.  So currently we can support not more than 2^16 (65K) UDNs or to be more precise, NADs, of any topology type (since the tunnel-key is allocated based on the network-id, which in turn is also allocated for Localnet networks).  Now we need more transit routers for the layer2 topology, but the allowed tunnel key range for all transit switches and routers stays the same. The same problem will arise when we get to UDN interconnect, because it will use even more transit routers.</p> <p>For this enhancement we have 3 options: 1. We could just split the range into 3 equal parts, use the first part for transit switches, the second part for layer2 transit routers, and the third part for UDN interconnect transit routers. That means we could support up to 21K UDNs (NADs) and up to 21K UDN interconnects. If we ever hit the scale limit, we can proceed with option 4. For backwards-compatability,  the first 21K keys for transit switches/routers will be allocated in the old range, and the new keys will use the new range. This option doesn't allow adding more features that require tunnel keys in the future since the whole range will be already consumed. 2. Second option is to use tunnel-keys outside the protected range. Current range is split into [0, 2^24-2^16-1] for northd allocation and [2^24-2^16, 2^24-1] for interconnect. Northd does sequential key allocation, that means (for single-node-zone) it will only use <code>(&lt;Number of UDNs&gt; + 1)*5</code> keys, so the following range <code>[(&lt;Number of UDNs&gt; + 1)*5; 2^24-2^16-1]</code> will be unused, and we could start allocating transit router keys from <code>2^24-2^16-1</code> going down. This is kind of dangerous as it only relies on the current northd behaviour. 3. Another option (centralized) is to add ID allocation from the transit tunnel key range. There is no way to migrate existing transit  switches to the new tunnel key, because all nodes need to do that at the same time, and rolling upgrade won't help here. Since current limit for networkIDs is <code>4096</code> https://github.com/ovn-kubernetes/ovn-kubernetes/blob/8f6e3ee9883bb6eb2230cea5c1c138d6098c95b0/go-controller/pkg/networkmanager/api.go#L17 we can consider [<code>first-interconnect-key</code>, <code>first-interconnect-key</code>+4095] to be reserved for the legacy transit switch keys, and use a new centralized transit keys allocation in the range [<code>first-interconnect-key</code>+4095, 2^24-1]. This option requires up to 2 new annotations on NADs, and makes the number of supported UDNs dependent on the number of UDN interconnects. 4. Fourth approach (distributed) is to start assigning all datapath keys ourselves (for all switches and routers, currently it is done by the northd), then we could use the full tunnel-key range of <code>2^24</code> (16M). Every regular (non-transit) switch or router only has to use unique key within its zone (that is within node for single-node-zone IC). That means that every node could re-use the same keys for regular datapaths (but it won't work for multi-node zones, the question is whether we need to support it). Currently, every Layer3 topology (the most switches/routers) needs 7 keys (node switch, cluster router, join switch,  gw router, transit router, transit switch, ext switch), so we could reserve 20 keys per network (just to be safe). These keys could be derived from the network-id, and then we could support e.g. 100K UDNs (2M keys) + 1M UDN interconnects (1M keys), and still have 12M keys left.</p> <p>We have agreed that we want to avoid implementing option 4 for as long as possible, which means we need to use tunnel keys in the most efficient way, which means option 3 is the way to go. We will add error handling in case we run out of tunnel keys and document the dependency on supported number of UDNs vs UDN interconnects (e.g. we can support 65K Layer3 UDNs or 32K Layer2 UDNs).</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#nat-configuration","title":"NAT configuration","text":"<p>As much as possible everything related to conntrack should not be modified since doing so can affect tcp connections.</p> <p>The only nat rule that needs to be moved from GR to transit_router is the one SNATing the traffic from the network subnet that goes to the management port. This is needed because now ovn_custer_router is the router directly connected to the layer2 switch (instead of the GR). The rest of the NAT configurations can stay unchanged on the GR.</p> <pre><code>allowed_ext_ips     : []\nexempted_ext_ips    : []\nexternal_ids        : {\"k8s.ovn.org/network\"=test12_namespace-scoped, \"k8s.ovn.org/topology\"=layer2}\nexternal_ip         : \"169.254.0.12\"\nexternal_mac        : []\nexternal_port_range : \"\"\ngateway_port        : []\nlogical_ip          : \"203.203.0.0/16\"\nlogical_port        : rtos-test12_namespace.scoped_ovn_layer2_switch\nmatch               : \"eth.dst == 0a:58:cb:cb:00:02\"\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n</code></pre> <p>Also the fact that join IP at gateway router is kept allow to maintain all the NATing done with it at gateway router to define OVN load balancers to implement k8s services.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#static-routes-and-logical-router-policies","title":"Static Routes and Logical Router Policies","text":"<p>Changing where routes and policies are configured does not affect TCP connections on upgrade so we can move some routes and policies from gateway router to transit_router without issues. In general it will be similar to the layer3 routes and policies.</p> <p>These will be the routes and policies configured on the new transit_router <pre><code>IPv4 Routes\nRoute Table &lt;main&gt;:\n                10.96.0.1               203.203.0.2 dst-ip &lt;-- api service\n               10.96.0.10               203.203.0.2 dst-ip &lt;-- dns service\n               100.65.0.2                100.88.0.6 dst-ip &lt;-- node1 traffic use node2 gr peer ip\n               100.65.0.3               100.88.0.10 dst-ip &lt;-- node2 traffic use node2 gr peer ip\n               100.65.0.4               100.88.0.14 dst-ip &lt;-- node3 traffic use node2 gr peer ip\n           203.203.0.0/16               100.88.0.14 src-ip &lt;-- network egress traffic goes to local gateway router peer ip\n</code></pre> <pre><code>      1004 inport == \"rtos-test12_namespace.scoped_ovn_layer2_switch\" &amp;&amp; ip4.dst == 172.18.0.3 /* test12_namespace.scoped_ovn_layer2_switch */         reroute               203.203.0.2\n      1004 inport == \"rtos-test12_namespace.scoped_ovn_layer2_switch\" &amp;&amp; ip6.dst == fc00:f853:ccd:e793::3 /* test12_namespace.scoped_ovn_layer2_switch */         reroute           2010:100:200::2\n       102 (ip4.src == $a10466913729612642039 || ip4.src == $a13607449821398607916) &amp;&amp; ip4.dst == $a3613486944346402462           allow\n       102 (ip4.src == $a10466913729612642039 || ip4.src == $a13607449821398607916) &amp;&amp; ip4.dst == $a712973235162149816           allow               pkt_mark=1008\n       102 (ip6.src == $a2718358047735721557 || ip6.src == $a13607452020421864338) &amp;&amp; ip6.dst == $a1091196985512978262           allow               pkt_mark=1008\n       102 (ip6.src == $a2718358047735721557 || ip6.src == $a13607452020421864338) &amp;&amp; ip6.dst == $a3613484745323146040           allow\n       102 ip4.src == 203.203.0.0/16 &amp;&amp; ip4.dst == 100.64.0.0/16           allow\n       102 ip4.src == 203.203.0.0/16 &amp;&amp; ip4.dst == 203.203.0.0/16           allow\n       102 ip6.src == 2010:100:200::/60 &amp;&amp; ip6.dst == 2010:100:200::/60           allow\n       102 ip6.src == 2010:100:200::/60 &amp;&amp; ip6.dst == fd98::/64           allow\n       102                                     pkt.mark == 42           allow\n</code></pre></p> <p>And these will be the gateway router configured routes (no policies needed): <pre><code>IPv4 Routes\nRoute Table &lt;main&gt;:\n           169.254.0.0/17               169.254.0.4 dst-ip rtoe-GR_test12_namespace.scoped_ovn-control-plane\n           203.203.0.0/16               100.88.0.13 dst-ip rtotr-GR_test12_namespace.scoped_ovn-control-plane &lt;-- cluster ingress or egress reply traffic going towards the pod network via transit_router peer IP.\n                0.0.0.0/0                172.18.0.1 dst-ip rtoe-GR_test12_namespace.scoped_ovn-control-plane\n</code></pre></p> <p>The logical router policies for EgressIP will be present on the transit router, using transit node peer ip as the next hop:</p> <pre><code>100                               ip4.src == 10.10.0.8         reroute               100.88.0.6,100.88.0.10               pkt_mark=50000\n</code></pre> <p>This approach eliminates the dependency on external IP addresses while maintaining proper load balancing for EgressIPs with multiple IP addresses.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#router-to-router-direct-connection-without-a-switch","title":"Router to router direct connection without a switch","text":"<p>To connect the transit_router to the local gateway router instead of using a join switch like the layer3 topology those ports can be connected each other directly using the <code>peer</code> field.</p> <p>This is how it looks the <code>trtor</code> port at transit_router: <pre><code>_uuid               : f697bce1-dac7-442d-9355-e298e1735c7b\ndhcp_relay          : []\nenabled             : []\nexternal_ids        : {\"k8s.ovn.org/network\"=test12_namespace-scoped, \"k8s.ovn.org/topology\"=layer2}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:64:58:00:0d\"\nname                : trtor-GR_test12_namespace.scoped_ovn-control-plane\nnetworks            : [\"100.88.0.8/31\", \"fd97::8/127\"]\noptions             : {requested-tnl-key=\"4\"}\npeer                : rtotr-GR_test12_namespace.scoped_ovn-control-plane &lt;------------- peer field\nstatus              : {}\n</code></pre></p> <p>And this is the <code>rtotr</code> port at GR: <pre><code>_uuid               : f9ab92f3-478c-41dd-b845-0d8ddf4a34e5\ndhcp_relay          : []\nenabled             : []\nexternal_ids        : {\"k8s.ovn.org/network\"=test12_namespace-scoped, \"k8s.ovn.org/topology\"=layer2}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:64:41:00:04\"\nname                : rtotr-GR_test12_namespace.scoped_ovn-control-plane\nnetworks            : [\"100.65.0.4/16\", \"100.88.0.9/31\", \"fd97::9/127\", \"fd99::4/64\"]\noptions             : {gateway_mtu=\"1400\"}\npeer                : trtor-GR_test12_namespace.scoped_ovn-control-plane &lt;-------------- peer field\nstatus              : {}\n</code></pre></p>"},{"location":"okeps/okep-5094-layer2-transit-router/#transit-router-specifics","title":"Transit router specifics","text":"<p>For transit router to work the LRPs referencing the same gateway router peer should have a unique tunnel key and if they are remote also a requested chassis pointing to the gateway router node.</p> <p>Also in the case we still need to support conditional SNAT, the transit router port connected to the switch needs to be configured as gateway router port.</p> <pre><code>%%{init: {\"nodeSpacing\": 20, \"rankSpacing\": 100}}%%\nflowchart TD\n    classDef nodeStyle fill:orange,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef vmStyle fill:blue,stroke:none,color:white,rx:10px,ry:10px,font-size:25px;\n    classDef portStyle fill:#3CB371,color:black,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef routerStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef switchStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef termStyle font-family:monospace,fill:black,stroke:none,color:white;\n    subgraph node2\n        subgraph GR-node2\n            rtos-GR-node2[\"rtos-GR-node2 100.65.0.3/16 (0a:58:64:41:00:03)\"]\n        end\n        subgraph layer2-switch_node2[\"layer2-switch\"]\n            stor-GR-node1[\"stor-GR-node1\n            type: remoterequested-tnl-key: 4\"]\n            stor-GR-node2[\"stor-GR-node2\n            type: routerrequested-tnl-key: 5\"]\n        end\n    end\n    subgraph node1[\"node1\"]\n        subgraph GR-node1\n            rtotr-GR-node1[\"trtor-GR-node1\n            100.65.0.2/16 100.88.0.5/31 (0a:58:64:41:00:02)\"]\n        end\n        subgraph transit_router_node1[\"transit_router \"]\n            trtor-GR-node1[\"trtor-GR-node1 100.88.0.4/31\"]\n            rtos-layer2-switch[\"rtos-layer2-switch 203.203.0.1 (0a:58:CB:CB:00:01)\"]\n        end\n        subgraph layer2-switch_node1[\"layer2-switch\"]\n            stor-transit_router[\"stor-transit_router\n            type: routerrequested-tnl-key: 4\"]\n            stor-GR-node1-node2[\"stor-GR-node2\n            type: remoterequested-tnl-key: 5\"]\n        end\n    end\n\n\n\n    rtotr-GR-node1 &lt;--&gt; trtor-GR-node1\n    rtos-layer2-switch &lt;--&gt; stor-transit_router\n    stor-GR-node1 &lt;--&gt; stor-transit_router\n    stor-GR-node2 &lt;--&gt; rtos-GR-node2\n    stor-GR-node1-node2 &lt;--&gt; stor-GR-node2\n\n    class VM vmStyle;\n    class rtotr-GR-node1 portStyle;\n    class rtos-GR-node2 portStyle;\n    class stor-GR-node2 portStyle;\n    class stor-GR-node1-node2 portStyle;\n    class stor-GR-node1 portStyle;\n    class trtor-GR-node1 portStyle;\n    class trtor-GR-node2 portStyle;\n    class stor-transit_router portStyle;\n    class rtos-layer2-switch portStyle;\n    class GR-node1 routerStyle;\n    class GR-node2 routerStyle;\n    class transit_router_node1 routerStyle;\n    class layer2-switch_node1 switchStyle;\n    class layer2-switch_node2 switchStyle;\n    class term termStyle;\n    class node1,node2 nodeStyle;\n</code></pre>"},{"location":"okeps/okep-5094-layer2-transit-router/#transit-subnet-conflict","title":"Transit subnet conflict","text":"<p>Before this change Layer2 networks using Subnet overlapping with the <code>transitSubnet</code> were allowed to be created (and would be working just fine). Now it is not possible anymore, and we need to decide what to do in this case: 1. Before upgrading to the new topology, check that no Layer2 networks with overlapping subnet are present. If they are, block the upgrade and inform the user to fix the issue. This option prevents the user from getting new topology for all networks until the next upgrade. 2. Upgrade all nodes, but leave old topology for networks with overlapping subnet. Report an event/warning for the user to fix the issue. When the network is upgraded/re-created with non-overlapping subnet, it will get the new topology. Other networks will be properly upgraded. 3. Add a new config field for Layer2 network similar to JoinSubnets to allow users to specify a custom transit subnet for the network, in case their Subnet overlaps with the default transit subnet. We don't allow UDN spec updates, so the only way to use this option (without introducing UDN spec updates) is to  re-create a UDN with the same fields + the new transit subnet and then migrate all the workloads to it. 4. Automatically select non-overlapping transit subnet and report it via status (it may be needed to avoid subnet overlap for Connecting UDNs). Transit subnet is not exposed to pods, so it may be fine to select it automatically as opposed to joinSubnet.</p> <p>Options 1 and 2 will require one more release of supporting the old topology (and in the next release we break the  networks if they were not upgraded). We think that overlapping subnets is a likely scenario, and we want this upgrade to be as smooth as possible, so option 4 is the least disruptive.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#api-details","title":"API Details","text":"<p>Every node will get a temporary annotation \"k8s.ovn.org/layer2-topology-version: 2.0\" that will be removed in the next ovn-k version (1.3 is the feature gets in 1.2). Every Layer2 and Layer3 NAD will get new annotations for tunnel keys distribution.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#implementation-details","title":"Implementation Details","text":""},{"location":"okeps/okep-5094-layer2-transit-router/#at-ovnkube-node","title":"At ovnkube-node","text":"<p>Create a helper that is able to derive the peer IPs from the node annotation for the transit peers node subnet.</p> <p>The layer2 controller should do the following: - Adapt and call the syncNodeClusterRouterPort from layer3 that creates the LRP that connects to the switch at transit_router - Pass transit_router as cluster_name to the gateway init functions - Remove from gateway.go the code that was attaching GR to layer2 switch - Add to gateway.go the code that connects GR to transit_router using the peer ports - Change egressip.go so routes are configured at transit_router instead of gateway router. - Change the ZoneInterConnect handler to be able to add the transit router remote ports to transit_router - In general re-use as much Layer3 code as possible since these makes Layer2 topology similar to it.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#at-ovnkube-control-plane","title":"At ovnkube-control-plane","text":"<p>At cluster manager at the zoneCluster controller calculate the peers node subnet and annotate the node.</p> <p>To calculate the transit router subnet for each node we have two options: - Derive it from the node-id (same we do for transit switch addrs. <code>offset := 2 * nodeID; subnet := 100.88.0.0 + offset /31</code>):   - good:     - No need to maintain the allocator lifecycle     - It's possible to entertain the idea of not annotating the node since the source of truth is the node-id   - bad:     - Add complexity in the form of node-id to subnet derivation code     - Make the transit peers node subnet dependent on node-id - Create a new subnet allocator that will re-sync on restarts and allocate   the subnet with it:   - good:     - Is simpler since it's matter of a new subnet allocator     - The transit peers node subnet is no longer dependent on node-id   - bad:     - Need to maintain the allocator lifecycle     - Use more memory</p> <p>We will go with the first option to avoid polluting already overloaded node annotations.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#rolling-upgrade-and-traffic-disruption","title":"Rolling upgrade and traffic disruption","text":"<p>OpenShift upgrades work as follows: first the ovn-k pods are upgraded while the  workload pods are still running. We can't upgrade topology at this point,  because it includes SNAT for the management port move from the GR to the transit router, which is disruptive  for existing connections. Some time after that the node is drained (no more workload pods are left)  and rebooted, at this point ovn-k is restarted with no workload pods, which is the time  when we can make the topology upgrade.</p> <p>From the ovn-k side we need to figure out when is that time with no workload pods, so we will  introduce a function similar to https://github.com/ovn-kubernetes/ovn-kubernetes/pull/5416/commits/8adda434a35831c49e9bd19b86888bfb074be89f#diff-214cf3919602fd060047b8d15fd6c0ca9d3ed3d42c47fff4b181a072c182b673R306  that will check whether a given network has already been updated and if not if it has running pods,  and only upgrade the topology when it doesn't. This means that we will have to leave the code for the previous topology version in place for one more release,  and only cleanup afterwards.</p> <p>This means, we don't need to worry about existing connections disruption (since the topology upgrade happens after the node reboot, so there are no running connections), but we need to make sure that new connections created when some nodes are upgraded and some are not will work and won't be disrupted.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#remote-pod-traffic","title":"Remote pod traffic","text":"<p>The pod to remote pod traffic is not affected since the logical switch will keep intact the pod's local and remote LSPs with its tunnel key.</p> <p>No downtime expected during upgrade</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#egress-traffic-over-local-node","title":"Egress traffic over local node","text":"<p>At normal egress without an egress IP, traffic exits through the node hosting the pod/VM. Therefore, a partial cluster upgrade does not impact the logical network topology.</p> <p>During the upgrade all the VMs are evicted from the node, hence, they'll be migrated to another node -  and the GR MACs will be updated via the VM migration process.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#egressingress-traffic-over-remote-node","title":"Egress/Ingress traffic over remote node","text":"<p>Both egress (using egressip feature) and ingress can come from a remote node, this introduces problematic scenarios that can happen during an upgrade: - A pod's node has new topology, but its ingress/egress node has old topology. - A pod's node has old topology, but its ingress/egress node has new topology.</p> <p>The following diagram show a possible topology to support those two scenarios:</p> <pre><code>%%{init: {\"nodeSpacing\": 20, \"rankSpacing\": 100}}%%\nflowchart TD\n    classDef nodeStyle fill:orange,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef vmStyle fill:blue,stroke:none,color:white,rx:10px,ry:10px,font-size:25px;\n    classDef portStyle fill:#3CB371,color:black,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef routerStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef switchStyle fill:brown,color:white,stroke:none,rx:10px,ry:10px,font-size:25px;\n    classDef termStyle font-family:monospace,fill:black,stroke:none,color:white;\n    subgraph node2\n        subgraph GR-node2\n            rtos-GR-node2[\"rtos-GR-node2 100.65.0.3/16203.203.0.1 (0a:58:64:41:00:03)\"]\n        end\n        subgraph layer2-switch_node2[\"layer2-switch\"]\n            stor-GR-node1[\"stor-GR-node1\n            type: remoterequested-tnl-key: 4100.65.0.2/16 (0a:58:64:41:00:02)\"]\n            stor-GR-node2[\"stor-GR-node2\n            type: routerrequested-tnl-key: 5\"]\n        end\n    end\n    subgraph node1[\"node1\"]\n        subgraph GR-node1\n            rtotr-GR-node1[\"rtotr-GR-node1\n            100.65.0.2/16 100.88.0.5/31 (0a:58:64:41:00:02)\"]\n        end\n        subgraph transit_router_node1[\"transit_router \"]\n            trtor-GR-node1[\"trtor-GR-node1 100.88.0.4/31\"]\n            rtos-layer2-switch[\"trtos-layer2-switch 203.203.0.1 169.254.0.22/17 (0a:58:CB:CB:00:01)\"]\n            trtos-layer2-switch-upgrade[\"trtos-layer2-switch-upgrade 100.65.255.254 (0a:58:64:41:00:02)\"]\n        end\n        subgraph layer2-switch_node1[\"layer2-switch\"]\n            stotr-layer2-switch[\"stotr-layer2-switch\n            type: router\"]\n            stotr-layer2-switch-upgrade[\"stotr-layer2-switch-upgrade\n            type: routerrequested-tnl-key: 4\"]\n            stor-GR-node1-node2[\"stor-GR-node2\n            type: remoterequested-tnl-key: 5100.65.0.3/16 (0a:58:64:41:00:03)\"]\n        end\n    end\n\n    rtotr-GR-node1 &lt;--&gt; trtor-GR-node1\n    rtos-layer2-switch &lt;--&gt; stotr-layer2-switch\n    stotr-layer2-switch-upgrade &lt;--&gt; stor-GR-node1\n    stor-GR-node2 &lt;--&gt; rtos-GR-node2\n    stor-GR-node1-node2 &lt;--&gt; stor-GR-node2\n    trtos-layer2-switch-upgrade &lt;--&gt; stotr-layer2-switch-upgrade\n\n    class VM vmStyle;\n    class rtotr-GR-node1 portStyle;\n    class rtos-GR-node2 portStyle;\n    class stor-GR-node2 portStyle;\n    class stor-GR-node1-node2 portStyle;\n    class stor-GR-node1 portStyle;\n    class trtor-GR-node1 portStyle;\n    class trtor-GR-node2 portStyle;\n    class stotr-layer2-switch portStyle;\n    class stotr-layer2-switch-upgrade portStyle;\n    class rtos-layer2-switch portStyle;\n    class trtos-layer2-switch-upgrade portStyle;\n    class GR-node1 routerStyle;\n    class GR-node2 routerStyle;\n    class transit_router_node1 routerStyle;\n    class layer2-switch_node1 switchStyle;\n    class layer2-switch_node2 switchStyle;\n    class term termStyle;\n    class node1,node2 nodeStyle;\n</code></pre> <p>The intermediate upgrade topology parts are bold, and will be removed once all nodes finish the upgrade. Nodes using the new topology must perform the following actions: - At the distributed switch:     - Retain <code>remote</code> type stor-GR LSPs from nodes still using the       old topology.     - Create a tmp <code>router</code> type stotr-upgrade LSP with <code>router-port</code> and same tunnel key as before       pointing to rtos-GR.   - Create a tmp transit router port <code>trtos-layer2-switch-upgrade</code> with the GR MAC address   - Add a dummy IP from the join subnet to the <code>trtos-layer2-switch-upgrade</code> port to enable pod on node1 -&gt; remote GR traffic.   - Add <code>trasit_router</code> routes to steer joinIP traffic for the ole nodes to the <code>trtos-layer2-switch-upgrade</code> port.     These routes look weird, but they work, like <code>100.65.0.3 100.65.0.3 dst-ip</code> for every joinIP of the old nodes. We need this because     in OVN routes with dst-ip and src-ip policies are evaluated at the same time and selected based on longest-prefix-match,     and we have the following route for pod network <code>10.10.0.0/24</code>:     <code>10.10.0.0/24                 10.10.0.2 src-ip</code>     Connected route for the join IP is based on the joinSubnet, which is always <code>/16</code>.     That means, that for traffic with src IP from the podSubnet dst IP from joinSubnet,     the winning route is joinIP if podSubnet mask is &lt;= 16 and podSubnet otherwise.     That is not the desired behavior, that is why we add always-winning routes (aka /32 or /128) for joinIPs.</p> <p>We need to make sure joinSubnet works between upgraded and non-upgraded nodes, as this is the only network that both topologies understand: - We need to make sure pod2 on node2 -&gt; <code>100.65.0.2</code> works (it is used e.g. for service reply traffic that came from node1 to pod2)   - pod2(<code>203.203.0.3</code>) -&gt; default GW (<code>203.203.0.1</code>)   - GR will ARP for <code>100.65.0.2</code> and hit layer2-switch -&gt; will use remote port <code>stor-GR-node1</code> config and ARP reply with <code>0a:58:64:41:00:02</code>   - <code>layer2-switch</code> sends packet to <code>0a:58:64:41:00:02</code> via <code>stor-GR-node1</code> with <code>tunnel-key=4</code>   - now we cross interconnect to node1 <code>stotr-layer2-switch-upgrade</code>    - it sends packet to <code>trtos-layer2-switch-upgrade</code> on the <code>transit_router</code> via direct peer connection   - MAC address <code>0a:58:64:41:00:02</code> is owned by the port and will be accepted by the transit_router   - dst IP=<code>100.65.0.2</code> =&gt; route lookup =&gt; choose between old joinIP routes via port <code>trtos-layer2-switch-upgrade</code>   and new transit route <code>100.65.0.2/32</code> via <code>100.88.0.6</code>   - <code>transit_router</code> will choose <code>100.65.0.2/32</code> with the longest-prefix match and send it to the <code>GR-node1</code> - We need to make sure pod 1 on node1 -&gt; <code>100.65.0.3</code> works (it is used e.g. for service reply traffic that came from node2 to pod1)   - pod1(<code>203.203.0.2</code>) -&gt; default GW (<code>203.203.0.1</code>)   - <code>transit_router</code> will do route lookup and use <code>100.65.0.3/32</code> via <code>trtos-layer2-switch-upgrade</code> since it has the dummy IP     from the same subnet   - <code>transit_router</code> will ARP for <code>100.65.0.3</code> and go to the <code>layer2-switch</code>, which will use the remote port <code>stor-GR-node2</code> config     and ARP reply with <code>0a:58:64:41:00:03</code>   - <code>transit_router</code> sends packet to dst MAC <code>0a:58:64:41:00:03</code> via <code>trtos-layer2-switch-upgrade</code>   - <code>layer2-switch</code> sends it out via <code>stor-GR-node2</code> with <code>tunnel-key=5</code>   - <code>node2</code> handles the packet exactly as with the old topology</p> <p>For comparison, fully upgraded scenario for ingress service via node1 to local pod1 <code>203.203.0.2</code> looks like this: - incoming packet gets DNAT'ed to the <code>serviceIP</code> and SNAT'ed to the node masq IP (src=<code>169.254.0.2</code>), then comes to the GR - GR does its usual DNAT to the backend pod IP (dst=<code>203.203.0.2</code>) and SNAT to the joinIP (src=<code>100.65.0.2</code>) - GR sends the packet via <code>rtotr-GR-node1</code> to the <code>transit_router</code> - <code>transit_router</code> sends the packet (dst=<code>203.203.0.2</code>) directly to the <code>layer2-switch</code>, done Now reply: - pod1 (<code>203.203.0.2</code>) replies to <code>100.65.0.2</code> -&gt; default GW (<code>203.203.0.1</code>) - <code>transit_router</code> will ARP for <code>100.65.0.2</code> and get a reply from <code>GR-node1</code> via port <code>rtotr-GR-node1</code> with <code>0a:58:64:41:00:02</code> - <code>transit_router</code> will do route lookup and choose <code>100.65.0.2/32</code> via <code>100.88.0.6</code> with the longest-prefix match    and send it to the <code>GR-node1</code> - the usual un-DNAT (dst=<code>169.254.0.2</code>), un-SNAT (src=<code>serviceIP</code>) happens here before being sent out</p> <p>Some of the scenarios that we care about: - Ingress traffic coming to GR via service   - upgraded GR -&gt; non-upgraded node pod: we SNAT to the same joinIP as before, reply traffic comes back to the      <code>transit_router</code> because <code>trtos-layer2-switch-upgrade</code> port is configured with the GW MAC.   - non-upgraded GR -&gt; upgraded node pod: uses the same joinIP as before. Reply uses the newly assigned dummyIP     from the join subnet, to enable routing from the upgraded node pod to the joinIP subnet. Without the dummy IP     transit_router doesn't know what to do with the <code>dst: 100.65.0.3</code> traffic. - Egress IP traffic: uses the same joinIP to route as before, works similar to service traffic.</p> <p>The same GR MAC address is used on the GR port (<code>rtotr-GR-node1</code>) and the transit router port (<code>trtos-layer2-switch-upgrade</code>). That's fine because they're different L2 domains.</p> <p>Dummy IP can be resued on all nodes, since it is never used as a source or destination IP, it is only used for the routing decision. You may wonder why we need per-joinIP routes if dummyIP should be enough for routing decision based on the connected route. It is about longest prefix match algorithm that OVN uses. Without <code>/32</code> routes for every joinIP, we have 2 fighting routes for pod1-&gt;joinIP traffic, one with dummy IP <code>ip4.dst == 100.65.0.0/16</code> and another one for pod subnet <code>ip4.src == &lt;podSubnet&gt;</code>  (this one sends everything to the GR). So which route wins currently depends on the <code>podSubnet</code> netmask:  if it is <code>/16</code> or larger, dummy IP wins and everything works, otherwise pod subnet route wins and the traffic is blackholed.</p> <p>joinIP is not really needed in the new topology, but we will have to keep it during the upgrade, and we can't remove it after the upgrade is done, because there will be conntrack entries using it. Not too big of a deal, but a side effect of supporting the upgrade. There is a way to remove joinIPs eventually (not a part of this enhancement, but leaving here for future reference),  which requires 2 more upgrades: 1. On the next upgrade replace all NATs using joinIPs with the transitIPs (at this point the new topology is used and all nodes understand transit IPs). This can only be done in the same manner as this upgrade (i.e. after the node reboots) to avoid disruption. Routes on the pod will also need to be updated. 2. On the next-after-next upgrade we can safely remove joinIPs from the topology, since they shouldn't be used anymore.</p> <p>There is another aspect to this: joinSubnet is currently configurable in the Layer2 network spec, and it is exposed to the pods. So if we get rid of it and replace it with the transitSubnet, we need to remove that field and see if transitSubnet also needs to be configurable.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#upgrade-details","title":"Upgrade Details","text":"<ol> <li>every node has to identify when it starts using new topology (so that the other nodes can switch from the switch  remote ports to the router remote ports). This can be done with annotations, but it either will have to be per-network  (which is a lot of updates) or it needs to check that absolutely no pods are present on the node for all layer2 networks,  and use node-scope annotation. The only downside of per-node annotation is that if any layer2 network for whatever (buggy) reason still has a local pod, none of the networks will be upgraded. Since the whole upgrade procedure relies on pod eviction before node reboot, it is ok to use per-node annotation.  The node will check that it has no running pods, and set the annotation \"k8s.ovn.org/layer2-topology-version: 2.0\".</li> <li>every node will need to upgrade to the intermediate topology, when it has no running pods (when the node has been drained and rebooted)</li> <li>ovnkube is updated on each node, no upgrade action is taken for the layer 2 topology. Networks continue to function as before.</li> <li>Once network has rolled out, OpenShift Machine Config Operator (MCO) will start draining and rebooting nodes to update the host OS.</li> <li>As each upgraded node comes up, it will modify its layer 2 networks as shown in the upgrade topology,      the upgraded node will refuse to bring up any ovn-networked pods while the topology is updating.</li> <li>Other ovnkube pods will see this ovnkube pod's node event, and then reconfigure their remote port to directly connect to the router.</li> <li>After all nodes have added their annotation, the nodes will then remove the backwards compatible upgrade topology      (\"stotr-layer2-switch-upgrade\" port, and the local join subnet address 100.65.255.254)</li> <li>During topology upgrade we also need to cleanup/remove old topology parts, like GR-switch ports, old GR routes and NATs,      and these cleanups need to be idempotent and spread across multiple controllers.</li> <li>remove the upgrade topology artifacts (like extra ports or IPs) when all nodes finished upgrading.</li> </ol>"},{"location":"okeps/okep-5094-layer2-transit-router/#testing-details","title":"Testing Details","text":"<ul> <li>Unit test checking topology will need to be adapted.</li> <li>E2e tests should be the same and pass since this is a refactoring not adding or removing features.</li> <li>Upgrade tests are the most difficult part in this case.</li> <li>Perform an upgrade while using all the ovn-kubernetes layer2 topology features and check that these continue working.</li> <li>Make sure old layer2 topology is preserved and keeps working until all pods are removed from the node</li> <li>We don't have such upgrade tests yet, and they don't make sense after this upgrade is done, so the suggestion is to     do this testing manually.</li> <li>Make sure VM live migration works during the upgrade.</li> </ul>"},{"location":"okeps/okep-5094-layer2-transit-router/#documentation-details","title":"Documentation Details","text":"<p>N/A</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<p>This topology repositions the NAT, which masquerades management port traffic, from the gateway router to the <code>transit_router</code>. This change might disrupt name resolution (DNS service access) and local gateway access to external traffic.</p> <p>However, disrupting dns resolution is not a big issue since client would retry the resolution</p> <p>Consider that there are plans to migrate the SNAT from the logical router to iptables on the nodes. If this occurs, the disruption would be resolved. Therefore, it might be advisable to await this enhancement's implementation before proceeding.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#tunnel-keys-on-local-ports-of-distributed-switches-and-routers","title":"Tunnel keys on local ports of distributed switches and routers","text":"<p>In general, for OVN (transit) routers and switches the logical port keys (SB.Port_Binding.tunnel-key populated by ovn-northd  from the NB requested-tnl-key if any is provided or allocated otherwise) need to match on all chassis that process  traffic for the routers/switches.</p> <p>That's because the \"logical ingress port\" key is embedded in the geneve metadata of the packet that's tunneled between  chassis for routers/switches. Tunneling happens in between the logical ingress and egress pipelines.  OVN uses the logical ingress port key in the egress pipeline for various features (e.g., to-lport ACLs, SNAT).  In general, there's no guarantee ovn-northd won't be using the logical inport in the egress pipeline in the future for more features.</p> <p>In the OVN interconnect case the constraint also stands. If the native ovn-ic daemon were to be used to synchronize  transit switches (and routers in the future) it would ensure that in all AZ's NBs the logical switch ports would have  the same tunnel key. As ovn-kubernetes doesn't use the native ovn-ic daemon, it has to ensure itself that the constraint is satisfied.</p> <p>Note: In this specific case no current OVN feature would break if we didn't synchronize keys for the L2 network ports. However, it's best if we do.</p> <p>We will update the transit router to use the same tunnel key for transit router to layer2 switch ports across all nodes. But for the layer2 switch, we have a centralized tunnel key distribution logic via annotations, and there is a management port that is local and should use tunnel-key too, but it doesn't. We think it is ok to leave it as is (since this feature aka transit router doesn't change the layer2 switch and its tunnel key allocation logic), and we don't seem to use any of the \"broken\" features mentioned above.</p> <p>It still would be nice to address if we ever get to change the tunnel key allocation logic on the layer2 switch.</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#ovn-kubernetes-version-skew","title":"OVN Kubernetes Version Skew","text":"<p>which version is this feature planned to be introduced in? check repo milestones/releases to get this information for when the next release is planned for</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#alternatives","title":"Alternatives","text":"<p>N/A</p>"},{"location":"okeps/okep-5094-layer2-transit-router/#references","title":"References","text":"<ul> <li>OVN Transit Router: https://www.ovn.org/support/dist-docs/ovn-nb.5.html</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/","title":"OKEP-5193: User Defined Network Segmentation","text":"<ul> <li>Issue: #5193</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#problem-statement","title":"Problem Statement","text":"<p>OVN-Kubernetes today allows multiple different types of networks per secondary network: layer 2, layer 3, or localnet. Pods can be connected to different networks without discretion. For the primary network, OVN-Kubernetes only supports all pods connecting to the same layer 3 virtual topology. The scope of this effort is to bring the same flexibility of the secondary network to the primary network. Therefore, pods are able to connect to different types of networks as their primary network.</p> <p>Additionally, multiple and different instances of primary networks may co-exist for different users, and they will provide native network isolation.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#terminology","title":"Terminology","text":"<ul> <li>Primary Network - The network which is used as the default gateway for the pod. Typically recognized as the eth0   interface in the pod.</li> <li>Secondary Network - An additional network and interface presented to the pod. Typically created as an additional   Network Attachment Definition (NAD), leveraging Multus. Secondary Network in the context of this document refers to a   secondary network provided by the OVN-Kubernetes CNI.</li> <li>Cluster Default Network - This is the routed OVN network that pods attach to by default today as their primary network.   The pods default route, service access, as well as kubelet probe are all served by the interface (typically eth0) on this network.</li> <li>User-Defined Network - A network that may be primary or secondary, but is declared by the user.</li> <li>Layer 2 Type Network - An OVN-Kubernetes topology rendered into OVN where pods all connect to the same distributed   logical switch (layer 2 segment) which spans all nodes. Uses Geneve overlay.</li> <li>Layer 3 Type Network - An OVN-Kubernetes topology rendered into OVN where pods have a per-node logical switch and subnet.   Routing is used for pod to pod communication across nodes. This is the network type used by the cluster default network today.   Uses Geneve overlay.</li> <li>Localnet Type Network - An OVN-Kubernetes topology rendered into OVN where pods connect to a per-node logical switch   that is directly wired to the underlay.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#goals","title":"Goals","text":"<ul> <li>Provide a configurable way to indicate that a pod should be connected to a user-defined network of a specific type as a   primary interface.</li> <li>The primary network may be configured as a layer 3 or layer 2 type network.</li> <li>Allow networks to have overlapping pod IP address space. This range may not overlap with the default cluster subnet   used for allocating pod IPs on the cluster default network today.</li> <li>The cluster default primary network defined today will remain in place as the default network pods attach to. The cluster   default network will continue to serve as the primary network for pods in a namespace that has no primary user-defined network. Pods   with primary user-defined networks will still attach to the cluster default network with limited access to Kubernetes system resources.   Pods with primary user-defined networks will have at least two network interfaces, one connected to the cluster default network and one   connected to the user-defined network. Pods with primary user-defined networks will use the user-defined network as their default   gateway.</li> <li>Allow multiple namespaces per network.</li> <li>Support cluster ingress/egress traffic for user-defined networks, including secondary networks.</li> <li>Support for ingress/egress features on user-defined primary networks where possible:<ul> <li>QoS</li> <li>EgressIP</li> <li>Load Balancer and NodePort Services, as well as services with External IPs.</li> </ul> </li> <li>In addition to ingress service support, there will be support for Kubernetes Cluster IP services in user-defined networks. The   scope of reachability to that service as well as endpoints selected for that service will be confined to the network   and corresponding namespace(s) where that service was created.</li> <li>Support for pods to continue to have access to the cluster default primary network for DNS and KAPI service access.</li> <li>Kubelet healthchecks/probes will still work on all pods.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#non-goals","title":"Non-Goals","text":"<ul> <li>Allowing different service CIDRs to be used in different networks.</li> <li>Localnet will not be supported initially for primary networks.</li> <li>Allowing multiple primary networks per namespace.</li> <li>Hybrid overlay support on user-defined networks.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#future-goals","title":"Future-Goals","text":"<ul> <li>DNS lookup for pods returning records for IPs on the user-defined network. In the first phase DNS will return the pod   IP on the cluster default network instead.</li> <li>Admin ability to configure networks to have access to all services and/or expose services to be accessible from all   networks.</li> <li>Ability to advertise user-defined networks to external networks using BGP/EVPN. This will enable things like:<ul> <li>External -&gt; Pod ingress per VRF (Ingress directly to pod IP)</li> <li>Multiple External Gateway (MEG) in a BGP context, with ECMP routes</li> </ul> </li> <li>Allow connection of multiple networks via explicit router API configuration.</li> <li>An API to allow user-defined ports for pods to be exposed on the cluster default network. This may be used for things   like promethus metric scraping.</li> <li>Potentially, coming up with an alternative solution for requiring the cluster default network connectivity to the pod,   and presenting the IP of the pod to Kubernetes as the user-defined primary network IP, rather than the cluster default   network IP.</li> <li>Support for Egress Service</li> <li>Support for Host Networked Pods -&gt; UDN pods or UDN services</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#introduction","title":"Introduction","text":"<p>As users migrate from OpenStack to Kubernetes, there is a need to provide network parity for those users. In OpenStack, each tenant (akin to Kubernetes namespace) by default has a layer 2 network, which is isolated from any other tenant. Connectivity to other networks must be specified explicitly as network configuration via a Neutron router. In Kubernetes the paradigm is opposite, by default all pods can reach other pods, and security is provided by implementing Network Policy. Network Policy can be cumbersome to configure and manage for a large cluster. It also can be limiting as it only matches TCP, UDP, and SCTP traffic. Furthermore, large amounts of network policy can cause performance issues in CNIs. With all these factors considered, there is a clear need to address network security in a native fashion, by using networks per tenant to isolate traffic.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<ul> <li>As a user I want to be able to migrate applications traditionally on OpenStack to Kubernetes, keeping my tenant network   space isolated and having the ability to use a layer 2 network.</li> <li>As a user I want to be able to ensure network security between my namespaces without having to manage and configure   complex network policy rules.</li> <li>As an administrator, I want to be able to provision networks to my tenants to ensure their networks and applications   are natively isolated from other tenants.</li> <li>As a user, I want to be able to request a unique, primary network for my namespace without having to get administrator   permission.</li> <li>As a user, I want to be able to request new secondary networks for my namespace, without having to get administrator   permission.</li> <li>As a user, I want user-defined primary networks to be able to have similar functionality as the cluster default network,   regardless of being on a layer 2 or layer 3 type network. Features like Egress IP, Egress QoS, Kubernetes services,   Ingress, and pod Egress should all function as they do today in the cluster default network.</li> <li>As a user, I want to be able to use my own consistent IP addressing scheme in my network. I want to be able to specify   and re-use the same IP subnet for my pods across different namespaces and clusters. This provides a consistent   and repeatable network environment for administrators and users.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#proposed-solution","title":"Proposed Solution","text":"<p>By default, in OVN-Kubernetes pods are attached to what is known as the \u201ccluster default\" network, which is a routed network divided up into a subnet per node. All pods will continue to have an attachment to this network, even when assigned a different primary network. Therefore, when a pod is assigned to a user-defined network, it will have two interfaces, one to the cluster default network, and one to the user-defined network. The cluster default network attachment is required in order to provide support for Kubelet healthcheck probes to the pod.</p> <p>All other traffic from the pod will be dropped by firewall rules on this network, when the pod is assigned a user-defined primary network. Routes will be added to the user-defined OVN network to route KAPI/DNS traffic out towards the cluster default network. Note, it may be desired to allow access to any Kubernetes service on the cluster default network (instead of just KAPI/DNS), but at a minimum KAPI/DNS will be accessible. Furthermore, the IP of the pod from the Kubernetes API will continue to show the IP assigned in the cluster default network.</p> <p>In OVN-Kubernetes secondary networks are defined using Network Attachment Definitions (NADs). For more information on how these are configured, refer to:</p> <p>https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/features/multi-homing.md</p> <p>The proposal here is to leverage this existing mechanism to create the network. A new field, \u201crole\u201d is introduced to the NAD spec which indicates that this network should be used for the pod's primary network. Additionally, a new \"joinSubnets\" field is added in order to specify the join subnet used inside the OVN network topology. An example OVN-Kubernetes NAD may look like:</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l3-network\n  namespace: default\nspec:\n  config: |2\n    {\n            \"cniVersion\": \"0.3.1\",\n            \"name\": \"l3-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer3\",\n            \"subnets\": \"10.128.0.0/16/24,2600:db8::/29\",\n            \"joinSubnets\": \"100.65.0.0/24,fd99::/64\",\n            \"mtu\": 1400,\n            \"netAttachDefName\": \"default/l3-network\",\n            \"role\": primary\n    }\n</code></pre> <p>The NAD must be created before any pods are created for this namespace. In order to enforce this requirement, a required label will need to be added to a namespace during namespace creation time to indicate this namespace will have a primary UDN. The required label will be \"k8s.ovn.org/primary-user-defined-network\". It is recommended to use an admission policy so that a namespace cannot be updated to add/remove this label, and that it must be added only at namespace creation time:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: user-defined-networks-namespace-label\nspec:\n  matchConstraints:\n    resourceRules:\n      - apiGroups:   [\"\"]\n        apiVersions: [\"v1\"]\n        operations:  [\"UPDATE\"]\n        resources:   [\"namespaces\"]\n  failurePolicy: Fail\n  validations:\n    - expression: \"('k8s.ovn.org/primary-user-defined-network' in oldObject.metadata.labels) == ('k8s.ovn.org/primary-user-defined-network' in object.metadata.labels)\"\n      message: \"The 'k8s.ovn.org/primary-user-defined-network' label cannot be added/removed after the namespace was created\"\n\n---\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicyBinding\nmetadata:\n  name: user-defined-networks-namespace-label-binding\nspec:\n  policyName: user-defined-networks-namespace-label\n  validationActions: [Deny]\n  matchResources:\n    resourceRules:\n      - apiGroups:   [\"\"]\n        apiVersions: [\"v1\"]\n        operations:  [\"UPDATE\"]\n        resources:   [\"namespaces\"]\n</code></pre> <p>The following conditions regarding the namespace label are:</p> <ol> <li>If namespace is missing the label, and a pod is created, it attaches to default network.</li> <li>If the namespace is missing the label, and a primary UDN or CUDN is created that matches that namespace, the UDN/CUDN will report error status and the NAD will not be generated.</li> <li>If the namespace is missing the label, and a primary UDN/CUDN exists, a pod in the namespace will be created and attached to default network.</li> <li>If the namespace has the label, and a primary UDN/CUDN does not exist a pod in the namespace will fail creation until the UDN/CUDN is created.</li> </ol> <p>Only one primary network may exist per namespace. If more than one user-defined network is created with the \"role\" key set to primary, then future pod creations will return an error on CNI ADD until the network configuration is corrected.</p> <p>A pod may not connect to multiple primary networks other than the cluster default. When the NAD is created, OVN-Kubernetes will validate the configuration, as well as that no pods have been created in the namespace already. If pods existed before the NAD was created, errors will be logged, and no further pods will be created in this namespace until the network configuration is fixed.</p> <p>After creating the NAD, pods created in this namespace will connect to the newly defined network as their primary network. The primaryNetwork key is used so that OVN-Kubernetes knows which network should be used, in case there are multiple NADs created for a namespace (secondary networks).</p> <p>After a pod is created that shall connect to a user-defined network, it will then be annotated by OVN-Kubernetes with the appropriate networking config:</p> <pre><code>trozet@fedora:~/Downloads$ oc get pods -o yaml -n ns1\napiVersion: v1\nitems:\n- apiVersion: v1\n  kind: Pod\n  metadata:\n    annotations:\n      k8s.ovn.org/pod-networks: |\n        {\n          \"default\": {\n            \"ip_addresses\": [\"10.244.0.3/24\"],\n            \"mac_address\": \"0a:58:0a:f4:00:03\",\n            \"routes\": [\n              {\"dest\": \"10.244.0.0/16\", \"nextHop\": \"10.244.0.1\"},\n              {\"dest\": \"100.64.0.0/16\", \"nextHop\": \"10.244.0.1\"}\n            ],\n            \"ip_address\": \"10.244.0.3/24\",\n            \"role\": \"infrastructure-locked\"\n          },\n          \"udn-test/l3-primary\": {\n            \"ip_addresses\": [\"10.20.2.5/24\"],\n            \"mac_address\": \"0a:58:0a:14:02:05\",\n            \"gateway_ips\": [\"10.20.2.1\"],\n            \"routes\": [\n              {\"dest\": \"10.20.0.0/16\", \"nextHop\": \"10.20.2.1\"},\n              {\"dest\": \"10.96.0.0/16\", \"nextHop\": \"10.20.2.1\"},\n              {\"dest\": \"100.65.0.0/16\", \"nextHop\": \"10.20.2.1\"}\n            ],\n            \"ip_address\": \"10.20.2.5/24\",\n            \"gateway_ip\": \"10.20.2.1\",\n            \"role\": \"primary\"\n          }\n        }\n      k8s.v1.cni.cncf.io/network-status: |-\n        [{\n            \"name\": \"ovn-kubernetes\",\n            \"interface\": \"eth0\",\n            \"ips\": [\n                \"10.244.0.3\"\n            ],\n            \"mac\": \"0a:58:0a:f4:00:03\",\n            \"dns\": {}\n        },{\n            \"name\": \"ovn-kubernetes\",\n            \"interface\": \"ovn-udn1\",\n            \"ips\": [\n                \"10.20.2.5\"\n            ],\n            \"mac\": \"0a:58:0a:14:02:05\",\n            \"default\": true,\n            \"dns\": {}\n        }]\n    creationTimestamp: \"2025-04-22T18:50:50Z\"\n    labels:\n      pod-name: client\n    name: client\n    namespace: udn-test\n    resourceVersion: \"2093\"\n    uid: dca1ff46-1990-4e84-a0a5-7c9fdde01993\nstatus:\n    podIP: 10.244.0.3\n    podIPs:\n    - ip: 10.244.0.3\n</code></pre> <p>In the above output the primary network is listed within the k8s.ovn.org/pod-networks annotation. It is also listed in the network-status cncf annotation as \"ovn-udn1\". A user does not have to manually request that the pod is attached to the primary network. The attachment to the cluster default network (CDN) and the primary UDN are done within the same CNI ADD call.</p> <p>Multiple namespaces may also be configured to use the same network. In this case the underlying OVN network will be the same, following a similar pattern to what is already supported today for secondary networks.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#ip-addressing","title":"IP Addressing","text":"<p>As previously mentioned, one of the goals is to allow user-defined networks to have overlapping pod IP addresses. This is enabled by allowing a user to configure what CIDR to use for pod addressing when they create the network. However, this range cannot overlap with the default cluster CIDR used by the cluster default network today.</p> <p>Furthermore, the internal masquerade subnet and the Kubernetes service subnet will remain unique and will exist globally to serve all networks. The masquerade subnet must be large enough to accommodate enough networks. Therefore, the subnet size of the masquerade subnet is equal to the number of desired networks * 2, as we need 2 masquerade IPs per network. The masquerade subnet remains localized to each node, so each node can use the same IP addresses and the size of the subnet does not scale with number of nodes.</p> <p>The transit switch subnets may overlap between all networks. This network is just used for transport between nodes, and is never seen by the pods or external clients.</p> <p>The join subnet of the default cluster network may not overlap with the join subnet of user-defined networks. This is due to the fact that the pod is connected to the default network, as well as the user-defined primary network. The join subnet is SNAT'ed by the GR of that network in order to facilitate ingress reply service traffic going back to the proper GR, in case it traverses the overlay. For this reason, the pods may see this IP address and routes are added to the pod to steer the traffic to the right interface (100.64.0.0/16 is the default cluster network join subnet):</p> <pre><code>[root@pod3 /]# ip route show\ndefault via 10.244.1.1 dev eth0 \n10.96.0.0/16 via 10.244.1.1 dev eth0 \n10.244.0.0/16 via 10.244.1.1 dev eth0 \n10.244.1.0/24 dev eth0 proto kernel scope link src 10.244.1.8 \n100.64.0.0/16 via 10.244.1.1 dev eth0\n</code></pre> <p>Since the pod needs routes for each join subnet, any layer 3 or layer 2 network that is attached to the pod needs a unique join subnet. Consider a pod connected to the default cluster network, a user-defined, layer 3, primary network, and a layer 2, secondary network:</p> Network Pod Subnet Node Pod Subnet Join Subnet Cluster Default 10.244.0.0/16 10.244.0.0/24 100.64.0.0/16 Layer 3 10.245.0.0/16 10.245.0.0/24 100.65.0.0/16 Layer 2 10.246.0.0/16 N/A 100.66.0.0/16 <p>The routing table would look like:</p> <pre><code>[root@pod3 /]# ip route show\ndefault via 10.245.0.1 dev eth1 \n10.96.0.0/16 via 10.245.0.1 dev eth1\n10.244.0.0/16 via 10.244.0.1 dev eth0\n10.245.0.0/16 via 10.245.0.1 dev eth1\n10.244.0.0/24 dev eth0 proto kernel scope link src 10.244.0.8\n10.245.0.0/24 dev eth1 proto kernel scope link src 10.245.0.8\n10.246.0.0/16 dev eth2 proto kernel scope link src 10.246.0.8\n100.64.0.0/16 via 10.244.0.1 dev eth0\n100.65.0.0/16 via 10.245.0.1 dev eth1\n100.66.0.0/16 via 10.246.0.1 dev eth2\n</code></pre> <p>Therefore, when specifying a user-defined network it will be imperative to ensure that the networks a pod will connect to do not have overlapping pod network or join network subnets. OVN-Kubernetes should be able to detect this scenario and refuse to CNI ADD a pod with conflicts.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#dns","title":"DNS","text":"<p>DNS lookups will happen via every pod\u2019s access to the DNS service on the cluster default network. CoreDNS lookups for pods will resolve to the pod\u2019s IP on the cluster default network. This is a limitation of the first phase of this feature and will be addressed in a future enhancement. DNS lookups for services and external entities will function correctly.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#services","title":"Services","text":"<p>Services in Kubernetes are namespace scoped. Any creation of a service in a namespace without a user-defined network (using cluster default network as primary) will only be accessible by other namespaces also using the default network as their primary network. Services created in namespaces served by user-defined networks, will only be accessible to namespaces connected to the user-defined network.</p> <p>Since most applications require DNS and KAPI access, there is an exception to the above conditions where pods that are connected to user-defined networks are still able to access KAPI and DNS services that reside on the cluster default network. In the future, access to more services on the default network may be granted. However, that would require more groundwork around enforcing network policy (which is evaluated typically after service DNAT) as potentially nftables rules. Such work is considered a future enhancement and beyond the scope of this initial implementation.</p> <p>With this proposal, OVN-Kubernetes will check which network is being used for this namespace, and then only enable the service there. The cluster IP of the service will only be available in the network of that service, except for KAPI and DNS as previously explained. Host networked pods in a namespace with a user-defined primary network will also be limited to only accessing the cluster IP of the services for that network. Load balancer IP and nodeport services are also supported on user-defined networks. Service selectors are only able to select endpoints from the same namespace where the service exists. Services that exist before the user-defined network is assigned to a namespace will result in OVN-Kubernetes executing a re-sync on all services in that namespace, and updating all load balancers. Keep in mind that pods must not exist in the namespace when the namespace is assigned to a new network or the new network assignment will not be accepted by OVN-Kubernetes.</p> <p>Services in a user-defined network will be reachable by other namespaces that share the same network.</p> <p>As previously mentioned, Kubernetes API and DNS services will be accessible by all pods.</p> <p>Endpoint slices will provide the IPs of the cluster default network in Kubernetes API. For this implementation the required endpoints are those IP addresses which reside on the user-defined primary network. In order to solve this problem, OVN-Kubernetes may create its own endpoint slices or may choose to do dynamic lookups at runtime to map endpoints to their primary IP address. Leveraging a second set of endpoint slices will be the preferred method, as it creates less indirection and gives explicit Kube API access to what IP addresses are being used by OVN-Kubernetes. Read more about the endpoint slice mirroring implementation.</p> <p>Kubelet health checks to pods are queried via the cluster default network. When endpoints are considered unhealthy they will be removed from the endpoint slice, and thus their primary IP will be removed from the OVN load balancer. However, it is important to note that the healthcheck is being performed via the cluster default network interface on the pod, which ensures the application is alive, but does not confirm network connectivity of the primary interface. Therefore, there could be a situation where OVN networking on the primary interface is broken, but the default interface continues to work and reports 200 OK to Kubelet, thus rendering the pod serving in the endpoint slice, but unable to function. Although this is an unlikely scenario, it is good to document.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#network-policy","title":"Network Policy","text":"<p>Network Policy will be fully supported for user-defined primary networks as it is today with the cluster default network. However, configuring network policies that allow traffic between namespaces that connect to different user-defined primary networks will have no effect. This traffic will not be allowed, as the networks have no connectivity to each other. These types of policies will not be invalidated by OVN-Kubernetes, but the configuration will have no effect. Namespaces that share the same user-defined primary network will still benefit from network policy that applies access control over a shared network. Additionally, policies that block/allow cluster egress or ingress traffic will still be enforced for any user-defined primary network.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#api-details","title":"API Details","text":"<p>Network Attachment Definitions (NADs) are the current way to configure the network in OVN-Kubernetes today, and the method proposed in this enhancement. There are two major shortcomings of NAD:</p> <ol> <li>It has free-form configuration that depends on the CNI. There is no API validation of what a user enters, leading to    mistakes which are not caught at configuration time and may cause unexpected functional behavior at runtime.</li> <li>It requires cluster admin RBAC in order to create the NAD.</li> </ol> <p>In order to address these issues, a proper CRD may be implemented which indirectly creates the NAD for OVN-Kubernetes. This solution may consist of more than one CRD, namely an Admin based CRD and one that is namespace scoped for tenants. The reasoning behind this is we want tenants to be able to create their own user-defined network for their namespace, but we do not want them to be able to connect to another namespace\u2019s network without permission. The Admin based version would give higher level access and allow an administrator to create a network that multiple namespaces could connect to. It may also expose more settings in the future for networks that would not be safe in the hands of a tenant, like deciding if a network is able to reach other services in other networks. With tenants having access to be able to create multiple networks, we need to consider potential attack vectors like a tenant trying to exhaust OVN-Kubernetes resources by creating too many secondary networks.</p> <p>Furthermore, by utilizing a CRD, the status of the network CR itself can be used to indicate whether it is configured by OVN-Kubernetes. For example, if a user creates a network CR and there is some problem (like pods already existed) then an error status can be reported to the CR, rather than relying on the user to check OVN-Kubernetes logs.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#api-overview","title":"API Overview","text":"<p>Two CRDs shall be introduced. Note for the final implementation, see the official api. - Namespace scoped CRD - represent user request for creating namespace scoped OVN network.   - Shall be defined with <code>namespace</code> scope.   - Targeted for cluster admin and non-admin users, enable creating OVN network in a specific namespace. - Cluster scoped CRD - represent user request for creating cluster scoped OVN network, enables cross-namespace networking.   - Shall be defined with <code>cluster</code> scope   - Targeted for cluster admin users only, enable creating shared OVN network across multiple namespaces.</p> <p>Having a namespace-scope CRD targeted for admin and non-admin users, a cluster-scope CRD for admins only and utilizing RBAC mechanism, enable allowing non-admin users create OVN networks in namespaces they permitted to with no admin intervention, without the risk of destabilizing the cluster nodes or break the cluster network.</p> <p>There should be a finalizer on the CRDs, so that upon deletion OVN-Kubernetes can validate that there are no pods still using this network. If there are pods still attached to this network, the network will not be removed.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#crd-spec","title":"CRD Spec","text":"<p>The CRDs spec defines as follows:</p> Field name Description optional Topology The topological configuration for the network. Must be one of <code>Layer2</code>, <code>layer3</code>, <code>Localnet</code>. No Role Select the network role in the pod, either <code>Primary</code> or <code>Secondary</code>. Primary network support topologies <code>Layer2</code> and <code>Layer3</code> only. No MTU The maximum transmission unit (MTU).The default is 1400. Yes Subnets The subnet to use for the network across the cluster.E.g. <code>10.100.200.0/24</code>.IPv6 <code>2001:DBB::/64</code> and dual-stack <code>192.168.100.0/24</code>,<code>2001:DBB::/64</code> subnets are supported.When omitted, the logical switch implementing the network only provides layer 2 communication, and users must configure IP addresses.Port security only prevents MAC spoofing if the subnets are omitted. Yes ExcludeSubnets List of CIDRs.IP addresses are removed from the assignable IP address pool and are never passed to the pods. Yes JoinSubnets Subnet used inside the OVN network topology.  When omitted, this means no opinion and the platform is left to choose a reasonable default which is subject to change over time. Yes IPAM.Lifecycle Control IP addresses management lifecycle. When <code>Persistent</code> is specified it enable workloads have persistent IP addresses. For example: Virtual Machines will have the same IP addresses along their lifecycle (stop, start migration, reboots). Supported by Topology <code>Layer2</code> &amp; <code>Localnet</code>. Yes IPAM.Mode Control how much of the IP configuration will be managed by OVN-Kubernetes. Must be one of <code>Enabled</code>, <code>Disabled</code>. Yes <p>The cluster scoped CRD should have the following additional field:</p> Field name Description optional NamespaceSelector List of the standard <code>metav1.LabelSelector</code> selector for which namespace the network should be available for. No Template The user defined network spec. No <p>The template type should be the namespace scope CRD spec.</p> <p>Note: The spec should be extended with care and strive to have minimal set of fields to provide nice abstraction for the NAD spec.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#crd-status","title":"CRD Status","text":"<p>The CRD status should reflect the NAD state through conditions. For example, when the NAD its created the condition should be placed with status <code>True</code>.</p> <p>For cluster scoped networks, the condition should be true once all desired namespaces are provisioned with the corresponding NAD.</p> <p>The cluster scoped CRD status should reflect on which namespaces the network is available.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#namespace-scoped-crd","title":"Namespace scoped CRD","text":"<p><pre><code>// UserDefinedNetwork describe network request for a Namespace\ntype UserDefinedNetwork struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:XValidation:rule=\"self == oldSelf\", message=\"Spec is immutable\"\n    // +kubebuilder:validation:XValidation:rule=\" self.topology != 'Layer3' || (self.topology == 'Layer3' &amp;&amp; size(self.subnets) &gt; 0)\", message=\"Subnets is required for Layer3 topology\"\n    // +kubebuilder:validation:XValidation:rule=\"self.topology != 'Localnet' || (self.topology == 'Localnet' &amp;&amp; self.role == 'Secondary')\", message=\"Topology localnet is not supported for primary network\"\n    // +kubebuilder:validation:XValidation:rule=\"!has(self.ipamLifecycle) || (self.ipamLifecycle == 'Persistent' &amp;&amp; (self.topology == 'Layer2' || self.topology == 'Localnet'))\", message=\"ipamLifecycle is supported for Layer2 and Localnet topologies\"\n    // +required\n    Spec UserDefinedNetworkSpec `json:\"spec\"`\n    // +optional\n    Status UserDefinedNetworkStatus `json:\"status,omitempty\"`\n}\n</code></pre> Suggested API validation rules: - Spec defined as immutable      Avoid incomplete state in a scenario where a NAD is created according to UDN spec, and pods connected to the network,   UDN spec changes, now pods connected to a network that was created from previous revision spec. - <code>Subnets</code> are mandatory for <code>Layer3</code> topology. - <code>Localnet</code> topology is not supported for primary network. - <code>IPAM.Lifecycle</code> is supported for <code>Layer2</code> and <code>Localnet</code> topology. - <code>IPAM.Mode</code> can be set to <code>Disabled</code> only on <code>Layer2</code> or <code>Localnet</code> topologies for <code>Secondary</code> networks, where the <code>Subnets</code> parameter must be omitted. When set to <code>Enabled</code>, the <code>Subnets</code> attribute must be defined.</p> <p>Suggested CRD short-name: <code>udn</code></p> <p><pre><code>// UserDefinedNetworkSpec defines the desired state of UserDefinedNetwork.\ntype UserDefinedNetworkSpec struct {\n    // The topological configuration for the network.\n    // +kubebuilder:validation:Enum=Layer2;Layer3;Localnet\n    Topology NetworkTopology `json:\"topology\"`\n\n    // The network role in the pod (e.g.: Primary, Secondary).\n    // +kubebuilder:validation:Enum=Primary;Secondary\n    Role NetworkRole `json:\"role\"`\n\n    // The maximum transmission unit (MTU).\n    // MTU is optional, if not provided the globally configured value in OVN-Kubernetes (defaults to 1400) is used for the network.    \n    // +optional\n    MTU uint `json:\"mtu,omitempty\"`\n\n\n    // The subnet to use for the pod network across the cluster.\n    //\n    // Dualstack clusters may set 2 subnets (one for each ip family), \n    // otherwise only 1 subnet is allowed.\n    //\n    // When topology is `Layer3`, the given subnet is split into smaller subnets for every node.\n    // To specify how the subnet should be split, the following format is supported for `Layer3` network:\n    // `10.128.0.0/16/24`, which means that every host will get a `/24` subnet.\n    // If host subnet mask is not set (for example, `10.128.0.0/16`), it will be assigned automatically.\n    // \n    // For `Layer2` and `Localnet` topology types, the format should match standard CIDR notation, without\n    // providing any host subnet mask.\n    // This field is required when `ipam.mode` is set to `Enabled` and is ignored otherwise.\n    // +optional\n    Subnets []string `json:\"subnets,omitempty\"`\n\n    // A list of CIDRs.\n    // IP addresses are removed from the assignable IP address pool and are never passed to the pods.\n    // +optional\n    ExcludeSubnets []string `json:\"excludeSubnets,omitempty\"`\n\n    // Subnet used inside the OVN network topology.\n    // This field is ignored for non-primary networks (e.g.: Role Secondary).\n    // When omitted, this means no opinion and the platform is left to choose a reasonable default which is subject to change over time.    \n    // +kubebuilder:validation:XValidation:rule=\"1 &lt;= size(self) &amp;&amp; size(self) &lt;= 2\", message=\"Unexpected number of join subnets\"\n    // +optional\n    JoinSubnets []string `json:\"joinSubnets,omitempty\"`\n\n    // IPAM section contains IPAM-related configuration for the network.\n    IPAM *IPAMSpec `json:\"ipam,omitempty\"`\n}\n\ntype IPAMSpec struct {\n    // Mode controls how much of the IP configuration will be managed by OVN.\n    // `Enabled` means OVN-Kubernetes will apply IP configuration to the SDN infrastructure and it will also assign IPs\n    // from the selected subnet to the individual pods.\n    // `Disabled` means OVN-Kubernetes will only assign MAC addresses and provide layer 2 communication, letting users\n    // configure IP addresses for the pods.\n    // `Disabled` is only available for `Layer2` and `Localnet` topologies for Secondary networks.\n    // By disabling IPAM, any Kubernetes features that rely on selecting pods by IP will no longer function\n    // (such as network policy, services, etc). Additionally, IP port security will also be disabled for interfaces attached to this network.\n    // Defaults to `Enabled`.\n    // +optional\n    Mode IPAMMode `json:\"mode\"`\n\n    // Lifecycle controls IP addresses management lifecycle.\n    //\n    // The only allowed value is Persistent. When set, OVN-Kubernetes assigned IP addresses will be persisted in an\n    // `ipamclaims.k8s.cni.cncf.io` object. These IP addresses will be reused by other pods if requested.\n    // Only supported when \"mode\" is `Enabled`.\n    //\n    // +optional\n    Lifecycle NetworkIPAMLifecycle `json:\"lifecycle,omitempty\"`\n}\n</code></pre> Suggested API validation rules: - <code>Topology</code> and <code>Role</code> fields are mandatory. - <code>Topology</code> can be one of <code>Layer2</code>, <code>Layer3</code>, <code>Localnet</code>. - <code>Role</code> can be one of <code>Primary</code>, <code>Secondary</code>. - <code>IPAM.Lifecycle</code> can be <code>Persistent</code>. - <code>IPAM.Mode</code> can be set to <code>Disabled</code> only on <code>Layer2</code> or <code>Localnet</code> topologies for <code>Secondary</code> networks, where the <code>Subnets</code> parameter must be omitted. When set to <code>Enabled</code>, the <code>Subnets</code> attribute must be defined. - <code>JoinSubnets</code> length can be 1 or 2.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#cluster-scoped-crd","title":"Cluster scoped CRD","text":"<pre><code>// ClusterUserDefinedNetwork describes shared OVN network across namespaces request.\ntype ClusterUserDefinedNetwork struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n    // +kubebuilder:validation:Required\n    // +required\n    Spec ClusterUserDefinedNetworkSpec `json:\"spec\"`\n    // +optional\n    Status ClusterUserDefinedNetworkStatus `json:\"status,omitempty\"`\n}\n\n// ClusterUserDefinedNetwork defines the desired state of ClusterUserDefinedNetwork.\ntype ClusterUserDefinedNetwork struct {\n    // NamespaceSelector Label selector for which namespace network should be available for.\n    // +kubebuilder:validation:Required\n    // +required\n    NamespaceSelector metav1.LabelSelector `json:\"namespaceSelector\"`\n\n    // Template is direct specification of UserDefinedNetwork.\n    // +kubebuilder:validation:Required\n    // +required\n    Template *UserDefinedNetworkTemplateSpec `json:\"template\"`\n}\n\n// UserDefinedNetworkTemplateSpec UserDefinedNetwork spec template.\ntype UserDefinedNetworkTemplateSpec struct {\n    // UserDefinedNetworkSpec contains the UserDefinedNetwork specification.\n    Spec UserDefinedNetworkSpec `json:\"spec,omitempty\"`\n}\n\n// ClusterUserDefinedNetworkStatus contains the observed status of the ClusterUserDefinedNetwork.\ntype ClusterUserDefinedNetworkStatus struct {\n    // ActiveNamespaces indicates in which namespaces network is available.\n    ActiveNamespaces []string `json:\"activeNamespaces,omitempty\"`\n\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n}\n</code></pre> <p>Suggested CRD short-name: <code>cudn</code></p>"},{"location":"okeps/okep-5193-user-defined-networks/#example","title":"Example","text":"<p>Existing conditions that reflect if the Network Attachment Definition has been created, and if network allocation has been done for every node:</p> <pre><code>status:\n  conditions:\n    - lastTransitionTime: \"2025-05-13T18:01:05Z\"\n      message: NetworkAttachmentDefinition has been created\n      reason: NetworkAttachmentDefinitionCreated\n      status: \"True\"\n      type: NetworkCreated\n    - lastTransitionTime: \"2025-05-13T18:01:05Z\"\n      message: Network allocation succeeded for all synced nodes.\n      reason: NetworkAllocationSucceeded\n      status: \"True\"\n      type: NetworkAllocationSucceeded\n</code></pre> <p>To be implemented condition reflecting the network readiness underlying OVN network state: <pre><code>status:\n  conditions:\n  - type: NetworkCreated\n    status: \"True\"\n    reason: OVNNetworkCreated\n    message: OVN network has been created\n</code></pre></p>"},{"location":"okeps/okep-5193-user-defined-networks/#example-namespace-scoped-network","title":"Example - Namespace scoped network","text":"<pre><code>kind: UserDefinedNetwork\nmetadata:\n  name: db-network\n  namespace: demo\nspec:\n  topology: Layer2\n  role: Primary\n  mtu: 9000\n  subnets: [\"10.0.0.0/24\"]\n  excludeSubnets: [\"10.0.0.0/26\"]\n  ipamLifecycle: Persistent\n</code></pre> <p>After creation: <pre><code>kind: UserDefinedNetwork\nmetadata:\n  name: db-network\n  namespace: demo\n  finalizers:\n    - k8s.ovn.org/user-defined-network-protection\nspec:\n  topology: Layer2\n  role: Primary\n  mtu: 9000\n  subnets: [\"10.0.0.0/24\"]\n  excludeSubnets: [\"10.0.0.100/26\"]\n  ipam:\n    lifecycle: Persistent\nstatus:\n  conditions:\n    - lastTransitionTime: \"2025-05-13T18:01:05Z\"\n      message: NetworkAttachmentDefinition has been created\n      reason: NetworkAttachmentDefinitionCreated\n      status: \"True\"\n      type: NetworkCreated\n    - lastTransitionTime: \"2025-05-13T18:01:05Z\"\n      message: Network allocation succeeded for all synced nodes.\n      reason: NetworkAllocationSucceeded\n      status: \"True\"\n      type: NetworkAllocationSucceeded\n</code></pre></p>"},{"location":"okeps/okep-5193-user-defined-networks/#example-cluster-scoped-network","title":"Example - Cluster scoped network","text":"<pre><code>kind: ClusterUserDefinedNetwork\nmetadata:\n  name: db-network\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values:\n        - \"mynamespace\"\n        - \"theirnamespace\"\n  template:\n    topology: Layer2\n    role: Primary\n    mtu: 9000\n    subnets: [\"10.0.0.0/24\"]\n    excludeSubnets: [\"10.0.0.100/26\"]\nstatus:\n  conditions:\n    - lastTransitionTime: \"2025-05-13T19:26:13Z\"\n      message: 'NetworkAttachmentDefinition has been created in following namespaces:\n      [mynamespace, theirnamespace]'\n      reason: NetworkAttachmentDefinitionCreated\n      status: \"True\"\n      type: NetworkCreated\n</code></pre>"},{"location":"okeps/okep-5193-user-defined-networks/#tenant-use-case","title":"Tenant Use Case","text":"<p>As a tenant I want to ensure when I create pods in my namespace their network traffic is isolated from other tenants on the cluster. In order to ensure this, I first create a network CRD that is namespace scoped and indicate:</p> <ul> <li>Type of network (Layer 3 or Layer 2)</li> <li>IP addressing scheme I wish to use (optional)</li> <li>Indicate this network will be the primary network</li> </ul> <p>After creating this CRD, I can check the status of the CRD to ensure it is actively being used as the primary network for my namespace by OVN-Kubernetes. Once verified, I can now create pods and they will be in their own isolated SDN.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#admin-use-case","title":"Admin Use Case","text":"<p>As an admin, I have a customer who has multiple namespaces and wants to connect them all to the same private network. In order to accomplish this, I first create an admin network CRD that is cluster scoped and indicate:</p> <ul> <li>Type of network (Layer 3 or Layer 2)</li> <li>IP addressing scheme I wish to use (optional)</li> <li>Indicate this network will be the primary network</li> <li>Selector to decide which namespaces may connect to this network. May use the <code>kubernetes.io/metadata.name</code> label to   guarantee uniqueness and eliminates the ability to falsify access.</li> </ul> <p>After creating the CRD, check the status to ensure OVN-Kubernetes has accepted this network to serve the namespaces selected. Now tenants may go ahead and be provisioned their namespace.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#implementation-details","title":"Implementation Details","text":""},{"location":"okeps/okep-5193-user-defined-networks/#crd-implementation-details","title":"CRD Implementation Details","text":""},{"location":"okeps/okep-5193-user-defined-networks/#cluster-manager-udn-controller","title":"Cluster Manager UDN Controller","text":"<p>New controller should be introduced, it should manage the new CRDs lifecycle.</p> <p>The controller should utilize the CNI plugin API and create OVN networks using <code>NetworkAttachmentDefinition</code> using the <code>ovn-k8s-overlay-cni</code> CNI plugin according to desired spec.</p> <p>The controller should watch NAD object in order to reconcile them and reflect the network state in the CRDs status.</p> <p>The request corresponding NADs should be created with a finalizer, so that upon deletion OVN-Kubernetes can validate that there are no pods still using this network. If there are pods still attached to this network, the network will not be removed.</p> <p>The CRD spec should be validated before creating the NAD, see the validation section for more details.</p> <p>The controller should create the requested NAD with: 1. finalizer, enable the controller release resources before NAD is deleted, ensure no pod is connected to the network. 2. owner-reference referring to the request CR object.    Using the owner-reference mechanism should prevent deletion of the NAD before the corresponding CRD instance is deleted.    In addition, the owner-reference make teardown seamless, when the request CR object instance is deleted,    the cluster garbage collected will dispose the corresponding NAD when all finalizers are gone.</p> <p>In a scenario a NAD already exist at the target namespace, the controller should check if the existing NAD is managed by the controller as follows: 1. Check the owner reference match the request CR UID. 2. OVN-K user-defined-network finalizer exist. 3. NAD spec correspond to desired spec.</p> <p>In case one of the previous checks fails, the controller should reconcile the request and this state in the CR status network is not ready.</p> <p>In scenario of cluster-scope CRD request, the controller should do best effort to create the desired NAD in each specified namespace. In case on or more NAD creation fails (e.g.: due to namespace not exist), the controller should continue to the next namespace. When finished, reflect in the status all namespaces where NAD creation failed to create.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#general-crd-flow","title":"General CRD flow","text":"<ol> <li>On namespaced-scope CRD creation:</li> <li>Validates the CRD spec.</li> <li>Generate NAD manifest from the CRD spec.</li> <li>Check that the desired NAD is not already exist. If not, create the NAD and return.</li> <li>Otherwise, verify the existing NAD correspond to desired spec, if so return.</li> <li>In case foreign NAD* already exist at the target namespace, raise an error and return.</li> <li>In case the NAD is malformed, reconcile it to desired state.</li> <li> <p>Update the status as follows:</p> <ul> <li>Reflect the reconciliation errors, which namespaces failed and the reason.</li> </ul> </li> <li> <p>on cluster-scope CRD creation:</p> </li> <li>Validates the CRD spec.</li> <li>Generate NAD manifest from the CRD spec.</li> <li>For each namespace specified in the spec:<ul> <li>Check that the desired NAD does not already exist. If not, create the NAD and continue.</li> <li>Otherwise, verify the existing NAD correspond to desired spec, if so continue.</li> <li>In case foreign NAD* already exist at the target namespace, record an error and continue.</li> <li>In case the NAD is malformed, reconcile it to desired state.</li> </ul> </li> <li> <p>Update the status as follows:</p> <ul> <li>Reflect namespaces where network is available.</li> <li>Reflect the reconciliation errors, which namespaces failed and the reason.</li> </ul> </li> <li> <p>On namespace-scope CRD deletion:</p> </li> <li>If no NAD exist, return.</li> <li> <p>In case a NAD exist, ensure no pod specifying the network.</p> <ul> <li>In case no pod specifying the network, remove the finalizer, allowing the cluster garbage collector dispose the NAD object.</li> <li>Otherwise, reflect in the status the network is being deleted because its in use.</li> </ul> </li> <li> <p>On cluster-scope CRD deletion:</p> </li> <li>For each namespace specified in the spec:</li> <li>In case a NAD exist, ensure no pod specifying the network.</li> <li>In case no pod specifying the network, remove the finalizer, allowing the cluster garbage collector dispose the NAD object.</li> <li>Otherwise, reflect in the status the network is cannot be deleted because its in use.</li> </ol> <p>Note: NAD considered foreign when - Has the same <code>meta.namesapce</code> and <code>meta.name</code> as the requested NAD. - Has no owner-reference to the request CR object.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#networkattachmentdefinition-rendering","title":"NetworkAttachmentDefinition Rendering","text":"<p>The underlying OVN network ID (network-name) is represented by the NAD CNI's network config <code>name</code> field, and must be unique.</p> <p>The network-name is generated by the controller and should not be exposed for modification.</p> <p>Having the network-name unique and non-configurable, avoid the risk where a malicious entity could guess other networks name, specify then in the spec and tap into other networks.</p> <p>The network-name should be composed for the subject CRD <code>metadata.namespace</code> and <code>metadata.name</code>, in the following format: <code>mynamespace.myetwork</code></p> <p>The <code>NetworkAttachmentDefinition</code> object <code>metadata.namespace</code> and <code>metadata.name</code> should correspond to the request CRD.</p> <p>Creating namespace scoped CRD instance should trigger creation of a corresponding NAD at the namespace the CRD instance reside. Following the example, the following NAD should be created: <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: db-network\n  namespace: demo\n  finalizers:\n  - k8s.ovn.org/user-defined-network-protection\n  ownerReferences:\n  - apiVersion: k8s.ovn.org/v1alpha1\n    blockOwnerDeletion: true\n    kind: UserDefinedNetwork\n    name: db-network\n    uid: f45efb13-9511-48c1-95d7-44ee17c949f4\nspec:\n  config: &gt;    \n      '{\n          \"cniVersion\":\"0.3.1\",\n          \"mtu\":1500,   \n          \"name\":\"demo.network\",  &lt;--- generated unique network-name\n          \"netAttachDefName\":\"demo/poc-db-network\",\n          \"subnets\":\"10.0.0.0/24\",\n          \"topology\":\"layer2\",\n          \"type\":\"ovn-k8s-cni-overlay\",\n          \"role\": \"primary\",\n          \"persistentIPs\": \"true\"\n      }'\n</code></pre></p> <p>For cluster-scoped CRDs, the NAD <code>metadata.name</code> should correspond to the request CRD <code>metadata.name</code> with an additional prefix. Having the prefix avoids conflicting with existing NADs who has the same <code>metadata.name</code>. For example: Given the CR meta.name is <code>db-network</code>,the NAD metadata.name will be <code>cluster.udn.db-network</code>.</p> <p>Creating cluster scoped CRD instance should trigger creation of the corresponding NAD at each namespace specified in the spec. Following the above cluster-scope CRD example, the following NADs should be created: <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: cluster-db-network          &lt;--- name starts with \"cluster\"\n  namespace: mynamespace\n  finalizers:\n  - k8s.ovn.org/user-defined-network-protection\n  ownerReferences:\n  - apiVersion: k8s.ovn.org/v1alpha1\n    blockOwnerDeletion: true\n    kind: ClusterUserDefinedNetwork\n    name: db-network\n    uid: f45efb13-9511-48c1-95d7-44ee17c949f4 \nspec:\n  config: &gt;    \n      '{\n          \"cniVersion\":\"0.3.1\",\n          \"excludeSubnets\":\"10.0.0.100/24\",\n          \"mtu\":1500,   \n          \"name\":\"cluster.udn.db-network\",   &lt;--- generated unique network-name\n          \"netAttachDefName\":\"mynamespace/db-network\",\n          \"subnets\":\"10.0.0.0/24\",\n          \"topology\":\"layer2\",\n          \"type\":\"ovn-k8s-cni-overlay\",\n          \"role\": \"primary\"\n      }'\n---\napiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: cluster-db-network                 &lt;--- same name as in other nameapces\n  namespace: theirnamespace\n  finalizers:\n  - k8s.ovn.org/user-defined-network-protection\n  ownerReferences:\n  - apiVersion: k8s.ovn.org/v1alpha1\n    blockOwnerDeletion: true\n    kind: ClusterUserDefinedNetwork\n    name: db-network\n    uid: f45efb13-9511-48c1-95d7-44ee17c949f4\nspec:\n  config: &gt;\n    '{\n        \"cniVersion\":\"0.3.1\",\n        \"excludeSubnets\":\"10.0.0.100/24\",\n        \"mtu\":1500,   \n        \"name\":\"cluster.udn.db-network\",   &lt;--- same name as in other namespaces\n        \"netAttachDefName\":\"theirnamespace/db-network\",\n        \"subnets\":\"10.0.0.0/24\",\n        \"topology\":\"layer2\",\n        \"type\":\"ovn-k8s-cni-overlay\",\n        \"role\": \"primary\"\n    }'\n</code></pre></p>"},{"location":"okeps/okep-5193-user-defined-networks/#validations","title":"Validations","text":"<p>The controller should validate the request CRD spec and verify: - CIDRs are valid. - <code>Subnets</code> length is at least 1 when topology is <code>Layer3</code>. - <code>Topology</code> is one of <code>Layer2</code>, <code>Layer3</code> or <code>Localnet</code>. - <code>Role</code> is one of <code>Primary</code> or <code>Secondary</code>. - <code>IPAM.Lifecycle</code> can be <code>Persistent</code>, and set only when topology is <code>Layer2</code> or <code>Localnet</code>. - In case <code>Topology: Localnet</code>, <code>Role</code> cannot be <code>Primary</code></p> <p>In addition, the following scenarios should be validated: - The join subnet shall not overlap with the configured cluster default network join subnet.   If there is overlap, OVN-Kubernetes should report an error in the request CR status.</p> <ul> <li>When primary network is requested (i.e.: <code>spec.role: Primary</code>)</li> <li> <p>Verify no primary network exist at the target namespace (i.e.: no NAD with <code>\"primaryNetwork\": \"true\"</code> exist).     In case primary-network already exist, the request CR status should reflect network is not ready because primary network     already exist at the target namespace.</p> </li> <li> <p>In a scenario primary network created following CR request, and a primary NAD is created at the same target namespace,   the CR status should reflect there's a conflicting primary NAD.</p> </li> <li> <p>When CRD instance is deleted, ensure the network is not in use before continuing with deletion process (e.g.: remove finalizer).</p> </li> <li>Check no pod using the CRD instance corresponding NAD.</li> <li> <p>In case at least on pod using the network, update the status to reflect network cannot be deleted because its being used.</p> </li> <li> <p>When a managed NAD already exist at the target namespace:</p> </li> <li> <p>In case no owner-reference exist or owner-reference doesn't match the request CR object's UID,     the controller should re-enqueue the request and reflect the error in status saying a foreign NAD exist.</p> </li> <li> <p>When there is existing primary network cluster-scope CR \"net1\" (<code>spec.role: Primary</code>) specifies namespace \"A\",\"B\",\"C\".   The admin create new primary network cluster-scope CR \"net2\" (<code>spec.role: Primary</code>) specifies namespaces \"C\",\"D\",\"E\".</p> </li> <li>The \"net2\" network should not be ready at namespace \"C\", the status should reflect network is not ready because     primary network already exist in namespace \"C\".</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#best-practices-for-managing-ovn-network-using-crds","title":"Best practices for managing OVN network using CRDs","text":"<ul> <li>NAD should be managed by cluster-admin only, it's not recommended to grant non-admin users permissions to create/modify delete NADs.</li> <li>Managing user-defined-networks should be done using the suggested CRDs.      Creating user-defined-networks using NADs may introduce unexpected behaviour and collisions with NAD object the controller manage.</li> <li>Managed NAD object should not be deleted manually, in order to delete a network, the corresponding CR instance should be deleted.</li> <li>Only one primary network per namespace is supported.</li> <li>Make sure no workloads exist on a namespace before creating primary network in that namespace.</li> <li>For the cluster-scoped CRD, its recommended to use <code>kubernetes.io/metadata.name</code> label to specify the target namespaces.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#udn-functional-implementation-details","title":"UDN Functional Implementation Details","text":"<p>OVN offers the ability to create multiple virtual topologies. As with secondary networks in OVN-Kubernetes today, separate topologies are created whenever a new network is needed. The same methodology will be leveraged for this design. Whenever a new network of type layer 3 or layer 2 is requested, a new topology will be created for that network where pods may connect to.</p> <p>The limitation today with secondary networks is that there is only support for east/west traffic. This RFE will address adding support for user-defined primary and secondary network north/south support. In order to support north/south traffic, pods on different networks need to be able to egress, typically using the host\u2019s IP. Today in shared gateway mode we use a Gateway Router (GR) in order to provide this external connectivity, while in local gateway mode, the host kernel handles SNAT\u2019ing and routing out egress traffic. Ingress traffic also follows similar, and reverse paths. There are some exceptions to these rules:</p> <ol> <li>MEG traffic always uses the GR to send and receive traffic.</li> <li>Egress IP on the primary NIC always uses the GR, even in local gateway mode.</li> <li>Egress Services always use the host kernel for egress routing.</li> </ol> <p>To provide an ingress/egress point for pods on different networks the most simple solution may appear to be to connect them all to a single gateway router. This introduces an issue where now networks are all connected to a single router, and there may be routing happening between networks that were supposed to be isolated from one another. Furthermore in the future, we will want to extend these networks beyond the cluster, and to do that in OVN would require making a single router VRF aware, which adds more complexity into OVN.</p> <p>The proposal here is to create a GR per network. With this topology, OVN will create a patch port per network to the br-ex bridge. OVN-Kubernetes will be responsible for being VRF/network aware and forwarding packets via flows in br-ex to the right GR. Each per-network GR will only have load balancers configured on it for its network, and only be able to route to pods in its network. The logical topology would look something like this, if we use an example of having a cluster default primary network, a layer 3 primary network, and a layer 2 primary network:</p> <p></p> <p>In the above diagram, each network is assigned a unique conntrack zone and conntrack mark. These are required in order to be able to handle overlapping networks egressing into the same VRF and SNAT\u2019ing to the host IP. Note, the default cluster network does not need to use a unique CT mark or zone, and will continue to work as it does today. This is due to the fact that no user-defined network may overlap with the default cluster subnet. More details in the next section.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#shared-gateway-mode","title":"Shared Gateway Mode","text":""},{"location":"okeps/okep-5193-user-defined-networks/#pod-egress","title":"Pod Egress","text":"<p>On pod egress, the respective GR of that network will handle doing the SNAT to a unique masquerade subnet IP assigned to this network. For example, in the above diagram packets leaving GR-layer3 would be SNAT\u2019ed to 169.254.169.5 in zone 64005. The packet will then enter br-ex, where flows in br-ex will match this packet, and then SNAT the packet to the node IP in zone 0, and apply its CT mark of 5. Finally, the packet will be recirculated back to table 0, where the packet will be CT marked with 1 in zone 64000, and sent out of the physical interface. In OVN-Kubernetes we use zone 64000 to track things from OVN or the host and additionally, we mark packets from OVN with a CT Mark of 1 and packets from the host with 2. Pseudo openflow rules would look like this (assuming node IP of 172.18.0.3):</p> <pre><code>pod--&gt;GR(snat, 169.254.169.5, zone 64005)-&gt;br-ex(snat, 172.18.0.3, zone 0, mark 5, table=0) --&gt;recirc table0 (commit\nzone 64000, mark 1) --&gt;eth0\n</code></pre> <p>The above design will accommodate for overlapping networks with overlapping ports. The worst case scenario is if two networks share the same address space, and two pods with identical IPs are trying to connect externally using the same source and destination port. Although unlikely, we have to plan for this type of scenario. When each pod tries to send a packet through their respective GR, SNAT\u2019ing to the unique GR masquerade IP differentiates the conntrack entries. Now, when the final SNAT occurs in br-ex with zone 0, they can be determined as different connections via source IP, and when SNAT\u2019ing to host IP, conntrack will detect a collision using the same layer 4 port, and choose a different port to use.</p> <p>When reply traffic comes back into the cluster, we must now submit the packet to conntrack to find which network this traffic belongs to. The packet is always first sent into zone 64000, where it is determined whether this packet belonged to OVN (CT mark of 1) or the host. Once identified by CT mark as OVN traffic, the packet will then be unSNAT\u2019ed in zone 0 via br-ex rules and the CT mark restored of which network it belonged to. Finally, we can send the packet to the correct GR via the right patch port, by matching on the restored CT Mark. From there, OVN will handle unSNAT\u2019ing the masquerade IP and forward the packet to the original pod.</p> <p>To support KubeVirt live migration the GR LRP will have an extra address with the configured gateway for the layer2 subnet (to allow the gateway IP to be independent of the node where the VM is running on). After live migration succeeds, OVN should send a GARP for VMs to clean up its ARP tables since the gateway IP has different mac now.</p> <p>The live migration feature at layer 2 described here will work only with OVN interconnect (OVN IC, which is used by OCP). Since there is no MAC learning between zones, so we can have the same extra address on every gateway router port, basically implementing anycast for this SVI address.</p> <p>Following is a picture that illustrate all these bits with a topology</p> <p></p>"},{"location":"okeps/okep-5193-user-defined-networks/#services_1","title":"Services","text":"<p>When ingress service traffic enters br-ex, there are flows installed that steer service traffic towards the OVN GR. With additional networks, these flows will be modified to steer traffic to the correct GR-&lt;network&gt;\u2019s patch port.</p> <p>When a host process or host networked pod on a Kubernetes node initiates a connection to a service, iptables rules will DNAT the nodeport or loadbalancer IP into the cluster IP, and then send the traffic via br-ex where it is masqueraded and sent into the OVN GR. These flows can all be modified to detect the service IP and then send to the correct GR-&lt;network&gt; patch port. For example, in the br-ex (breth0) bridge today we have flows that match on packets sent to the service CIDR (10.96.0.0/24):</p> <pre><code>[root@ovn-worker ~]# ovs-ofctl dump-flows breth0 table=0 | grep 10.96\n cookie=0xdeff105, duration=22226.373s, table=0, n_packets=41, n_bytes=4598, idle_age=19399,priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.0/16 actions=ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <p>Packets that are destined to the service CIDR are SNAT'ed to the masquerade IP of the host (169.254.169.2) and then sent to the dispatch table 2:</p> <pre><code>[root@ovn-worker ~]# ovs-ofctl dump-flows breth0 table=2\n cookie=0xdeff105, duration=22266.310s, table=2, n_packets=41, n_bytes=4598, actions=mod_dl_dst:02:42:ac:12:00:03,output:\"patch-breth0_ov\"\n</code></pre> <p>In the above flow, all packets have the dest MAC address changed to be that of the OVN GR, and then sent on the patch port towards the OVN GR. With multiple networks, host access to cluster IP service flows will now be modified to be on a per cluster IP basis. For example, if we assume two services exist on two user defined namespaces with cluster IPs 10.96.0.5 and 10.96.0.6. The flows would look like:</p> <pre><code>[root@ovn-worker ~]# ovs-ofctl dump-flows breth0 table=0 | grep 10.96\n cookie=0xdeff105, duration=22226.373s, table=0, n_packets=41, n_bytes=4598, idle_age=19399,priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.5 actions=set_field:2-&gt;reg1,ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n cookie=0xdeff105, duration=22226.373s, table=0, n_packets=41, n_bytes=4598, idle_age=19399,priority=500,ip,in_port=LOCAL,nw_dst=10.96.0.6 actions=set_field:3-&gt;reg1,ct(commit,table=2,zone=64001,nat(src=169.254.169.2))\n</code></pre> <p>The above flows are now per cluster IP and will send the packet to the dispatch table while also setting unique register values to differentiate which OVN network these packets should be delivered to:</p> <pre><code>[root@ovn-worker ~]# ovs-ofctl dump-flows breth0 table=2\n cookie=0xdeff105, duration=22266.310s, table=2, n_packets=41, n_bytes=4598, reg1=0x2 actions=mod_dl_dst:02:42:ac:12:00:05,output:\"patch-breth0-net1\"\n cookie=0xdeff105, duration=22266.310s, table=2, n_packets=41, n_bytes=4598, reg1=0x3 actions=mod_dl_dst:02:42:ac:12:00:06,output:\"patch-breth0-net2\"\n</code></pre> <p>Furthermore, host networked pod access to services will be restricted to the network it belongs to. For more information see the Host Networked Pods section.</p> <p>Additionally, in the case where there is hairpin service traffic to the host (Host-&gt;Service-&gt;Endpoint is also the host), the endpoint reply traffic will need to be distinguishable on a per network basis. In order to achieve this, each OVN GR\u2019s unique masquerade IP will be leveraged.</p> <p>For service access towards KAPI/DNS or potentially other services on the cluster default network, there are two potential technical solutions. Assume eth0 is the pod interface connected to the cluster default network, and eth1 is connected to the user-defined primary network:</p> <ol> <li> <p>Add routes for KAPI/DNS specifically into the pod to go out eth0, while all other service access will go to eth1.    This will then just work normally with the load balancers on the switches for the respective networks.</p> </li> <li> <p>Do not send any service traffic out of eth0, instead all service traffic goes to eth1. In this case all service    traffic is flowing through the user-defined primary network, where only load balancers for that network are configured    on that network's OVN worker switch. Therefore, packets to KAPI/DNS (services not on this network) are not DNAT'ed at    the worker switch and are instead forwarded onwards to the ovn_cluster_router_&lt;user-defined network&gt; or    GR-&lt;node-user-defined-network&gt; for layer 3 or layer 2 networks, respectively . This router is    configured to send service CIDR traffic to ovn-k8s-mp0-&lt;user-defined network&gt;. IPTables rules in the host only permit    access to KAPI/DNS and drop all other service traffic coming from ovn-k8s-mp0-&lt;user-defined network&gt;. The traffic then    gets routed to br-ex and default GR where it hits the OVN load balancer there and forwarded to the right endpoint.</p> </li> </ol> <p>While the second option is more complex, it allows for not configuring routes to service addresses in the pod that could hypothetically change.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-ip","title":"Egress IP","text":"<p>This feature works today by labeling and choosing a node+network to be used for egress, and then OVN logical routes and logical route policies are created which steer traffic from a pod towards a specific gateway router (for primary network egress). From there the packets are SNAT\u2019ed by the OVN GR to the egress IP, and sent to br-ex. Egress IP is cluster scoped, but applies to selected namespaces, which will allow us to only apply the SNAT and routes to the GR and OVN topology elements of that network. In the layer 3 case, the current design used today for the cluster default primary network will need some changes. Since Egress IP may be served on multiple namespaces and thus networks, it is possible that there could be a collision as previously mentioned in the Pod Egress section. Therefore, the same solution provided in that section where the GR SNATs to the masquerade subnet must be utilized. However, once the packet arrives in br-ex we will need a way to tell if it was sent from a pod affected by a specific egress IP. To address this, pkt_mark will be used to mark egress IP packets and signify to br-ex which egress IP to SNAT to. An example where the egress IP is 1.1.1.1 that maps to pkt_mark 10 would look something like this:</p> <p></p> <p>For layer 2, egress IP has never been supported before. With the IC design, there is no need to have an ovn_cluster_router and join switch separating the layer 2 switch network (transit switch) from the GR. Non-IC will not be supported. In the layer 2 IC model, GRs per node on a network will all be connected to the layer 2 transit switch:</p> <p></p> <p>In the above diagram, Node 2 is chosen to be the egress IP node for any pods in namespace A. Pod 1 and Pod 2 have default gateway routes to their respective GR on their node. When egress traffic leaves Pod 2, it is sent towards its GR-A on node 2, where it is SNAT\u2019ed to the egress IP and the traffic sent to br-ex. For Pod 1, its traffic is sent to its GR-A on Node 1, where it is then rerouted towards GR-A on Node 2 for egress.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-firewall","title":"Egress Firewall","text":"<p>Egress firewall is enforced at the OVN logical switch, and this proposal has no effect on its functionality.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-qos","title":"Egress QoS","text":"<p>Egress QoS is namespace scoped and functions by marking packets at the OVN logical switch, and this proposal has no effect on its functionality.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-service","title":"Egress Service","text":"<p>Egress service is namespace scoped and its primary function is to SNAT egress packets to a load balancer IP. As previously mentioned, the feature works the same in shared and local gateway mode, by leveraging the local gateway mode path. Therefore, its design will be covered in the Local Gateway Mode section of the Design Details.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#multiple-external-gateways-meg-admin-based-policy-routing-abpr","title":"Multiple External Gateways (MEG) / Admin Based Policy Routing (ABPR)","text":"<p>There will be no support for MEG or pod direct ingress on any network other than the primary, cluster default network. This support may be enhanced later by extending VRFs/networks outside the cluster.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#local-gateway-mode","title":"Local Gateway Mode","text":"<p>With local gateway mode, egress/ingress traffic uses the kernel\u2019s networking stack as a next hop. OVN-Kubernetes leverages an interface named \u201covn-k8s-mp0\u201d in order to facilitate sending traffic to and receiving traffic from the host. For egress traffic, the host routing table decides where to send the egress packet, and then the source IP is masqueraded to the node IP of the egress interface. For ingress traffic, the host routing table steers packets destined for pods via ovn-k8s-mp0 and SNAT\u2019s the packet to the interface address.</p> <p>For multiple networks to use local gateway mode, some changes are necessary. The ovn-k8s-mp0 port is a logical port in the OVN topology tied to the cluster default network. There will need to be multiple ovn-k8s-mp0 ports created, one per network. Additionally, all of these ports cannot reside in the default VRF of the host network. Doing so would result in an inability to have overlapping subnets, as well as the host VRF would be capable of routing packets between namespace networks, which is undesirable. Therefore, each ovn-k8s-mp0-&lt;network&gt; interface must be placed in its own VRF:</p> <p></p> <p>The VRFs will clone the default routing table, excluding routes that are created by OVN-Kubernetes for its networks. This is similar to the methodology in place today for supporting Egress IP with multiple NICs.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#pod-egress_1","title":"Pod Egress","text":"<p>Similar to the predicament outlined in Shared Gateway mode, we need to solve the improbable case where two networks have the same address space, and pods with the same IP/ports are trying to talk externally to the same server. In this case, OVN-Kubernetes will reserve an extra IP from the masquerade subnet per network. This masquerade IP will be used to SNAT egress packets from pods leaving via mp0. The SNAT will be performed by ovn_cluster_router for layer 3 networks and the gateway router (GR) for layer 2 networks using configuration like:</p> <pre><code>[root@ovn-worker ~]# ovn-nbctl lr-nat-list daac7843-ad73-4b73-b415-e432a28f0d61\nTYPE             GATEWAY_PORT          MATCH                 EXTERNAL_IP        EXTERNAL_PORT    LOGICAL_IP          EXTERNAL_MAC         LOGICAL_PORT\nsnat                                   eth.dst == 0a:58:0    169.254.0.100                       10.20.1.0/24\n</code></pre> <p>Now when egress traffic arrives in the host via mp0, it will enter the VRF, where clone routes will route the packet as if it was in the default VRF out a physical interface, typically towards br-ex, and the packet is SNAT\u2019ed to the host IP.</p> <p>When the egress reply comes back into the host, iptables will unSNAT the packet and the destination will be 169.254.169.100. At this point, an ip rule will match the destination on the packet and do a lookup in the VRF where a route specifying 169.254.169.100/32 via 10.244.0.1 will cause the packet to be sent back out the right mp0 port for the respective network.</p> <p>Note, the extra masquerade SNAT will not be required on the cluster default network's ovn-k8s-mp0 port. This will preserve the previous behavior, and it is not necessary to introduce this SNAT since the default cluster network subnet may not overlap with user-defined networks.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#services_2","title":"Services","text":"<p>Local gateway mode services function similar to the behavior described in host -&gt; service description in the Shared Gateway Mode Services section. When the packet enters br-ex, it is forwarded to the host, where it is then DNAT\u2019ed to the cluster IP and typically sent back into br-ex towards the OVN GR. This traffic will behave the same as previously described. There are some exceptions to this case, namely when external traffic policy (ETP) is set to local. In this case traffic is DNAT\u2019ed to a special masquerade IP (169.254.169.3) and sent via ovn-k8s-mp0. There will need to be IP rules to match on the destination node port and steer traffic to the right VRF for this case. Additionally, with internal traffic policy (ITP) is set to local, packets are marked in the mangle table and forwarded via ovn-k8s-mp0 with an IP rule and routing table 7. This logic will need to ensure the right ovn-k8s-mp0 is chosen for this case as well.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-ip_1","title":"Egress IP","text":"<p>As previously mentioned, egress IP on the primary NIC follows the pathway of shared gateway mode. The traffic is not routed by the kernel networking stack as a next hop. However, for multi-nic support, packets are sent into the kernel via the ovn-k8s-mp0 port. Here the packets are matched on, sent to an egress IP VRF, SNAT\u2019ed and sent out the chosen interface. The detailed steps for a pod with IP address 10.244.2.3 affected by egress IP look like:</p> <ol> <li>Pod sends egress packet, arrives in the kernel via ovn-k8s-mp0 port, the packet is marked with 1008 (0x3f0 in hex)    if it should skip egress IP. It has no mark if the packet should be affected by egress IP.</li> <li> <p>IP rules match the source IP of the packet, and send it into an egress IP VRF (rule 6000):</p> <p>```    sh-5.2# ip rule</p> </li> </ol> <p>0:   from all lookup local    30:  from all fwmark 0x1745ec lookup 7    5999:    from all fwmark 0x3f0 lookup main    6000:    from 10.244.2.3 lookup 1111    32766:   from all lookup main    32767:   from all lookup default    ```</p> <ol> <li> <p>Iptables rules save the packet mark in conntrack. This is only applicable to packets that were marked with 1008 and    are bypassing egress IP:</p> <pre><code>sh-5.2# iptables -t mangle -L  PREROUTING\n\nChain PREROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nCONNMARK   all  --  anywhere             anywhere             mark match 0x3f0 CONNMARK save\nCONNMARK   all  --  anywhere             anywhere             mark match 0x0 CONNMARK restore\n</code></pre> </li> <li> <p>VRF 1111 has a route in it to steer the packet to the right egress interface:</p> <pre><code>sh-5.2# ip route show table 1111\ndefault dev eth1\n</code></pre> </li> <li> <p>IPTables rules in NAT table SNAT the packet:</p> <pre><code>-A OVN-KUBE-EGRESS-IP-MULTI-NIC -s 10.244.2.3/32 -o eth1 -j SNAT --to-source 10.10.10.100\n</code></pre> </li> <li> <p>For reply bypass traffic, the 0x3f0 mark is restored, and ip rules 5999 send it back into default VRF for routing    back into mp0 for non-egress IP packets. This is rule and connmark restoring is required for the packet to pass the    reverse path filter (RPF) check. For egress IP reply packets, there is no connmark restored and the packets hit the    default routing table to go back into mp0.</p> </li> </ol> <p>This functionality will continue to work, with ip rules steering the packets from the per network VRF to the appropriate egress IP VRF. CONNMARK will continue to be used so that return traffic is sent back to the correct VRF. Step 5 in the above may need to be tweaked to match on mark in case 2 pods have overlapping IPs, and are both egressing the same interface with different Egress IPs. The flow would look something like this:</p> <p></p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-firewall_1","title":"Egress Firewall","text":"<p>Egress firewall is enforced at the OVN logical switch, and this proposal has no effect on its functionality.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-qos_1","title":"Egress QoS","text":"<p>Egress QoS is namespace scoped and functions by marking packets at the OVN logical switch, and this proposal has no effect on its functionality.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#egress-service_1","title":"Egress Service","text":"<p>Egress service functions similar to Egress IP in local gateway mode, with the exception that all traffic paths go through the kernel networking stack. Egress Service also uses IP rules and VRFs in order to match on traffic and forward it out the right network (if specified in the CRD). It uses iptables in order to SNAT packets to the load balancer IP. Like Egress IP, with user-defined networks there will need to be IP rules with higher precedence to match on packets from specific networks and direct them to the right VRF.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#multiple-external-gateways-meg","title":"Multiple External Gateways (MEG)","text":"<p>There will be no support for MEG or pod direct ingress on any network other than the primary, cluster default network. Remember, MEG works the same way in local or shared gateway mode, by utilizing the shared gateway path. This support may be enhanced later by extending VRFs/networks outside the cluster.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#kubernetes-readinessliveness-probes","title":"Kubernetes Readiness/Liveness Probes","text":"<p>As previously mentioned, Kubelet probes will continue to work. This includes all types of probes such as TCP, HTTP or GRPC. Additionally, we want to restrict host networked pods in namespaces that belong to user-defined networks from being able to access pods in other networks. For that reason, we need to block host networked pods from being able to access pods via the cluster default network. In order to do this, but still allow Kubelet to send probes; the cgroup module in iptables will be leveraged. For example:</p> <pre><code>root@ovn-worker:/# iptables -L -t raw -v\nChain PREROUTING (policy ACCEPT 6587 packets, 1438K bytes)\n pkts bytes target     prot opt in     out     source               destination         \n\nChain OUTPUT (policy ACCEPT 3003 packets, 940K bytes)\n pkts bytes target     prot opt in     out     source               destination         \n 3677 1029K ACCEPT     all  --  any    any     anywhere             anywhere             cgroup kubelet.slice/kubelet.service\n    0     0 ACCEPT     all  --  any    any     anywhere             anywhere             ctstate ESTABLISHED\n  564 33840 DROP       all  --  any    any     anywhere             10.244.0.0/16    \n</code></pre> <p>From the output we can see that traffic to the pod network <code>10.244.0.0/16</code> will be dropped by default. However, traffic coming from kubelet will be allowed.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#host-networked-pods","title":"Host Networked Pods","text":""},{"location":"okeps/okep-5193-user-defined-networks/#vrf-considerations","title":"VRF Considerations","text":"<p>By encompassing VRFs into the host, this introduces some constraints and requirements for the behavior of host networked type pods. If a host networked pod is created in a Kubernetes namespace that has a user-defined network, it should be confined to only talking to ovn-networked pods on that same user-defined network.</p> <p>With Linux VRFs, different socket types behave differently by default. Raw, unbound sockets by default are allowed to listen and span multiple VRFs, while TCP, UDP, SCTP and other protocols are restricted to the default VRF. There are settings to control this behavior via sysctl, with the defaults looking like this:</p> <pre><code>trozet@fedora:~/Downloads/ip-10-0-169-248.us-east-2.compute.internal$ sudo sysctl -A | grep net | grep l3mdev\nnet.ipv4.raw_l3mdev_accept = 1\nnet.ipv4.tcp_l3mdev_accept = 0\nnet.ipv4.udp_l3mdev_accept = 0\n</code></pre> <p>Note, there is no current support in the kernel for SCTP, and it does not look like there is support for IPv6. Given the desired behavior to restrict host networked pods to talking to only pods in their namespace/network, it may make sense to set raw_l3dev_accept to 0. This is set to 1 by default to allow legacy ping applications to work over VRFs. Furthermore, a user modifying sysctl settings to allow applications to listen across all VRFs will be unsupported. Reasons include there can be odd behavior and interactions that occur with applications communicating across multiple VRFs, as well as the fact that this would break the native network isolation paradigm offered by this feature.</p> <p>For host network pods to be able to communicate with pod IPs on their user-defined network, the only supported method will be for the applications to bind their socket to the VRF device. Many applications will not be able to support this, so in the future it makes sense to come up with a better solution. One possibility is to use ebpf in order to intercept the socket bind call of an application (that typically will bind to INADDR_ANY) and force it to bind to the VRF device. Note, host network pods will still be able to communicate with pods via services that belong to its user-defined network without any limitations. See the next section Service Access for more information.</p> <p>Keep in mind that if a host network pod runs and does not bind to the VRF device, it will be able to communicate on the default VRF. This means the host networked pod will be able to talk to other host network pods. However, due to nftables rules in the host however, it will not be able to talk to OVN networked pods via the default cluster network/VRF.</p> <p>For more information on how VRFs function in Linux and the settings discussed in this section, refer to https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/Documentation/networking/vrf.rst?h=v6.1 for more details.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#service-access","title":"Service Access","text":"<p>Host networked pods in a user-defined network will be restricted to only accessing services in either: 1. The cluster default network. 2. The user-defined network in which the host networked pod's namespace belongs to.</p> <p>This will be enforced by iptables/nftables rules added that match on the cgroup of the host networked pod. For example:</p> <p><pre><code>root@ovn-worker:/# iptables -L -t raw -v\nChain PREROUTING (policy ACCEPT 60862 packets, 385M bytes)\n pkts bytes target     prot opt in     out     source               destination         \n\nChain OUTPUT (policy ACCEPT 36855 packets, 2504K bytes)\n pkts bytes target     prot opt in     out     source               destination         \n   17  1800 ACCEPT     all  --  any    any     anywhere             10.96.0.1            cgroup /kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-besteffort.slice/kubelet-kubepods-besteffort-pod992d3b9e_3f85_42e2_9558_9d4273d4236f.slice\n23840 6376K ACCEPT     all  --  any    any     anywhere             anywhere             cgroup kubelet.slice/kubelet.service\n    0     0 ACCEPT     all  --  any    any     anywhere             anywhere             ctstate ESTABLISHED\n  638 37720 DROP       all  --  any    any     anywhere             10.244.0.0/16       \n   28  1440 DROP       all  --  any    any     anywhere             10.96.0.0/16\n</code></pre> In the example above, access to the service network of <code>10.96.0.0/16</code> is denied by default. However, one host networked pod is given access to the 10.96.0.1 cluster IP service, while other host networked pods are blocked from access.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#testing-details","title":"Testing Details","text":"<ul> <li>E2E upstream CI jobs covering supported features across multiple networks.</li> <li>E2E tests which ensure network isolation between OVN networked and host networked pods, services, etc.</li> <li>E2E tests covering network subnet overlap and reachability to external networks.</li> <li>Scale testing to determine limits and impact of multiple user-defined networks. This is not only limited to OVN, but   also includes OVN-Kubernetes\u2019 design where we spawn a new network controller for every new network created.</li> <li>Integration testing with other features like IPSec to ensure compatibility.</li> <li>E2E tests verify the expected NAD is generated according to CRDs spec.</li> <li>E2E tests verify workloads on different namespaces connected to namespace scoped OVN network with the same name cannot communicate.</li> <li>E2E tests verify workloads on different namespaces connected to cluster-scope OVN network can communicate.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#documentation-details","title":"Documentation Details","text":"<ul> <li>ovn-kubernetes.io will be updated with a UDN user guide, and a support matrix showing what features are supported   with UDN.</li> <li>Additional dev guides will be added to the repo to show how the internal design of UDN is implemented.</li> </ul>"},{"location":"okeps/okep-5193-user-defined-networks/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-5193-user-defined-networks/#risks-and-mitigations","title":"Risks and Mitigations","text":"<p>The biggest risk with this feature is hitting scale limitations. With many namespaces and networks, the number of internal OVN objects will multiply, as well as internal kernel devices, rules, VRFs. There will need to be a large-scale effort to determine how many networks we can comfortably support.</p> <p>Following the introduction of a CRD for non-admin users create OVN network, there is a risk a non-admin users could cause node / OVN resources starvation due to creating too many OVN networks. To mitigates it, CRDs controller could monitor how many OVN network exists and reject new ones in case a given limit is exceeded.</p> <p>Alternatively, OVN-K resources should be exposed as node resource (using the device-plugin API). Once a node resource is exposed, it will enable using the resource-quota API and put boundaries on how many networks could exist.</p> <p>There is also a risk of breaking secondary projects that integrate with OVN-Kubernetes, such as Metal LB or Submariner.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#drawbacks","title":"Drawbacks","text":"<p>As described in the Design Details section, this proposal will require reserving two IPs per network in the masquerade subnet. This is a private subnet only used internally by OVN-Kubernetes, but it will require increasing the subnet size in order to accommodate multiple networks. Today this subnet by default is configured as a /29 for IPv4, and only 6 IP addresses are used. With this new design, users will need to reconfigure their subnet to be large enough to hold the desired number of networks. Note, API changes will need to be made in order to support changing the masquerade subnet post-installation.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":"<p>UDN will be delivered in version 1.1.0.</p>"},{"location":"okeps/okep-5193-user-defined-networks/#alternatives","title":"Alternatives","text":"<p>None</p>"},{"location":"okeps/okep-5193-user-defined-networks/#references","title":"References","text":"<p>None</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/","title":"OKEP-5233: Predefined addresses for primary user defined networks workloads","text":"<ul> <li>Issue: #5233</li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#problem-statement","title":"Problem Statement","text":"<p>Migrating legacy workloads with predefined network configurations (IP, MAC, default gateway) to OVN-Kubernetes is currently not possible. There is a need to import these workloads, preserving their network configuration, while also enabling non-NATed traffic to better integrate with existing infrastructures.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#goals","title":"Goals","text":"<ul> <li>Enable pods on primary Layer2 User Defined Network (UDN) and Cluster UDN to use a predefined static network   configuration including IP address, MAC address, and default gateway.</li> <li>Ensure it is possible to enable non-NATed traffic for pods with predefined static network configuration   by exposing the Layer2 Cluster UDN through BGP (see Risks, Known Limitations and Mitigations for current BGP support limitations).</li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#non-goals","title":"Non-Goals","text":"<ul> <li>Modifying the default gateway and management IPs of a primary UDN after it was created.</li> <li>Modifying a pod's network configuration after the pod was created.</li> <li>Non-NATed traffic support in secondary networks.</li> <li>Predefined IP/MAC addresses support for pods in Layer3 UDNs.</li> <li>Configurable default gateway and infrastructure addresses in Layer3 UDNs.</li> <li>Predefined IP/MAC addresses support for pods in Localnet UDNs.</li> <li>Configuring default gateway and infrastructure addresses in Layer2 (Cluster) UDNs that do not belong to the networks subnets.</li> <li>No-downtime workload migration.</li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#introduction","title":"Introduction","text":"<p>Legacy workloads, particularly virtual machines, are often set up with static network configurations. When migrating to OVN-Kubernetes UDNs, it should be possible to integrate these gradually to prevent disruptions.</p> <p>Currently, OVN-Kubernetes allocates IP addresses dynamically and it generates the MAC addresses from it. It sets the pod's default gateway to the first usable IP address of its subnet. For primary UDNs, it additionally reserves the second usable IP address for the internal management port which excludes it from being available for workloads.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<ul> <li> <p>As a user, I want to define a custom default gateway IP for a new primary Layer2 UDN so that my migrated workloads can maintain their existing network configuration without disruption.</p> </li> <li> <p>As a user, I want the ability to configure a new primary Layer2 UDN with a custom management IP address to prevent IP conflicts with the workloads I am importing.</p> </li> <li> <p>As a user, I want to assign a predefined IP address and MAC address to a pod to ensure the network identity of my imported workload is maintained.</p> </li> <li> <p>As a user, I want to prevent OVN-Kubernetes from automatically assigning  IP addresses that are already in use by my existing infrastructure, so that I can migrate my services gradually without network conflicts.</p> </li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#proposed-solution","title":"Proposed Solution","text":""},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#primary-udn-configuration","title":"Primary UDN configuration","text":"<p>To support the migration of pre-configured workloads, the UDN and cluster UDN API has to be enhanced. The aim is to provide control over the IP addresses that OVN-Kubernetes consumes in the overlay network, this includes the default gateway and management IPs. The proposed changes are specified in the Layer2 User Defined Network API changes section.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#pod-network-identity","title":"Pod network identity","text":"<p>OVN-Kubernetes currently supports configuring pods' secondary network interfaces through the <code>k8s.v1.cni.cncf.io/networks</code> annotation, which contains a JSON array of NetworkSelectionElement objects. Additionally, it is possible to modify the cluster's default network attachment by setting the <code>v1.multus-cni.io/default-network</code> annotation to a singular NetworkSelectionElement object.</p> <p>To enable using predefined MAC and IP addresses on pods attached to a primary UDN, the <code>v1.multus-cni.io/default-network</code> will be reused, as it is a well-known annotation for configuring the pod's default network. The <code>k8s.v1.cni.cncf.io/networks</code> annotation is specific to secondary networks and expects a list of networks, which does not fit well with primary UDNs. With the proposed approach, the <code>k8s.ovn.org/primary-udn-ipamclaim</code> annotation, used to link a pod with a matching claim, will be deprecated in favor of the <code>IPAMClaimReference</code> field in the NetworkSelectionElement. When <code>IPAMClaimReference</code> is specified we will update its status to reflect the result of the IP allocation, see IPAMClaim API changes. OVN-Kubernetes will keep track of all allocated MAC and IP addresses to detect conflicts. When a conflict is detected, OVN-Kubernetes will emit a Kubernetes event to the pod indicating the specific conflict (IP or MAC address already in use) and prevent the pod from starting.</p> <pre><code>%%{init: { 'sequence': {'messageAlign': 'left'} }}%%\nsequenceDiagram\nactor User\nparticipant K8s_API_Server as \"K8s API Server\"\nparticipant OVN_K_Controller as \"OVN-Kubernetes\"\n\nnote over User, K8s_API_Server: Pre-Step: User defines UDN  (Optional) with custom Default Gateway / Management IP\n\nUser-&gt;&gt;K8s_API_Server: Create Pod with annotation:  'v1.multus-cni.io/default-network':  [{  name: 'default',  namespace: 'ovn-kubernetes', ips: ['10.0.0.10'],  mac: '00:1A:2B:3C:4D:5E',  ipam-claim-reference: 'my-claim'  }]\n\n\nK8s_API_Server-&gt;&gt;OVN_K_Controller: Notify: New Pod Spec\n\nOVN_K_Controller-&gt;&gt;OVN_K_Controller: Parse 'v1.multus-cni.io/default-network' annotation Perform IP/MAC conflict check within UDN  (Verify requested IP/MAC are not in use)\n\n\nalt \"No IP/MAC Conflict\"\nopt \"IPAMClaimReference is specified in annotation\"\nOVN_K_Controller-&gt;&gt;K8s_API_Server: Update Status conditions and addresses of referenced IPAMClaim\nend\n\nOVN_K_Controller-&gt;&gt;OVN_K_Controller: Configure Pod's Primary Network Interface\nnote right of OVN_K_Controller: Pod provisioning succeeds\nelse \"IP/MAC Conflict Detected in UDN\"\nopt \"IPAMClaimReference is specified in annotation\"\nOVN_K_Controller-&gt;&gt;K8s_API_Server: Update Status conditions of referenced IPAMClaim\nend\nOVN_K_Controller-&gt;&gt;K8s_API_Server: Emit IP/MAC Conflict error event to the Pod\nnote right of OVN_K_Controller: Pod provisioning fails\nend\n</code></pre>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#api-details","title":"API Details","text":""},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#layer2-user-defined-network-api-changes","title":"Layer2 User Defined Network API changes","text":"<p>Proposed API change adds <code>infrastructureSubnets</code> <code>reservedSubnets</code> and <code>defaultGatewayIPs</code> fields to the <code>Layer2Config</code> which is a part of both the UDN and cluster UDN specs:</p> <pre><code>// +kubebuilder:validation:XValidation:rule=\"has(self.ipam) &amp;&amp; has(self.ipam.mode) &amp;&amp; self.ipam.mode != 'Enabled' || has(self.subnets)\", message=\"Subnets is required with ipam.mode is Enabled or unset\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.ipam) || !has(self.ipam.mode) || self.ipam.mode != 'Disabled' || !has(self.subnets)\", message=\"Subnets must be unset when ipam.mode is Disabled\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.ipam) || !has(self.ipam.mode) || self.ipam.mode != 'Disabled' || self.role == 'Secondary'\", message=\"Disabled ipam.mode is only supported for Secondary network\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.joinSubnets) || has(self.role) &amp;&amp; self.role == 'Primary'\", message=\"JoinSubnets is only supported for Primary network\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.subnets) || !has(self.mtu) || !self.subnets.exists_one(i, isCIDR(i) &amp;&amp; cidr(i).ip().family() == 6) || self.mtu &gt;= 1280\", message=\"MTU should be greater than or equal to 1280 when IPv6 subnet is used\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.defaultGatewayIPs) || has(self.role) &amp;&amp; self.role == 'Primary'\", message=\"defaultGatewayIPs is only supported for Primary network\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.defaultGatewayIPs) || self.defaultGatewayIPs.all(ip, self.subnets.exists(subnet, cidr(subnet).containsIP(ip)))\", message=\"defaultGatewayIPs must belong to one of the subnets specified in the subnets field\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.reservedSubnets) || has(self.reservedSubnets) &amp;&amp; has(self.subnets)\", message=\"reservedSubnets must be unset when subnets is unset\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.reservedSubnets) || self.reservedSubnets.all(e, self.subnets.exists(s, cidr(s).containsCIDR(cidr(e))))\",message=\"reservedSubnets must be subnetworks of the networks specified in the subnets field\",fieldPath=\".reservedSubnets\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.infrastructureSubnets) || has(self.infrastructureSubnets) &amp;&amp; has(self.subnets)\", message=\"infrastructureSubnets must be unset when subnets is unset\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.infrastructureSubnets) || self.infrastructureSubnets.all(e, self.subnets.exists(s, cidr(s).containsCIDR(cidr(e))))\",message=\"infrastructureSubnets must be subnetworks of the networks specified in the subnets field\",fieldPath=\".infrastructureSubnets\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.infrastructureSubnets) || !has(self.defaultGatewayIPs) || self.defaultGatewayIPs.all(ip, self.infrastructureSubnets.exists(subnet, cidr(subnet).containsIP(ip)))\", message=\"defaultGatewayIPs have to belong to infrastructureSubnets\"\n+ // +kubebuilder:validation:XValidation:rule=\"!has(self.infrastructureSubnets) || !has(self.reservedSubnets) || self.infrastructureSubnets.all(infra, !self.reservedSubnets.exists(reserved, cidr(infra).containsCIDR(reserved) || cidr(reserved).containsCIDR(infra)))\", message=\"infrastructureSubnets and reservedSubnets must not overlap\"\ntype Layer2Config struct {\n\n// Role describes the network role in the pod.\n//\n// Allowed value is \"Secondary\".\n// Secondary network is only assigned to pods that use `k8s.v1.cni.cncf.io/networks` annotation to select given network.\n//\n// +kubebuilder:validation:Enum=Primary;Secondary\n// +kubebuilder:validation:Required\n// +required\nRole NetworkRole `json:\"role\"`\n\n// MTU is the maximum transmission unit for a network.\n// MTU is optional, if not provided, the globally configured value in OVN-Kubernetes (defaults to 1400) is used for the network.\n//\n// +kubebuilder:validation:Minimum=576\n// +kubebuilder:validation:Maximum=65536\n// +optional\nMTU int32 `json:\"mtu,omitempty\"`\n\n// Subnets are used for the pod network across the cluster.\n// Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.\n//\n// The format should match standard CIDR notation (for example, \"10.128.0.0/16\").\n// This field must be omitted if `ipam.mode` is `Disabled`.\n//\n// +optional\nSubnets DualStackCIDRs `json:\"subnets,omitempty\"`\n\n+ // reservedSubnets specifies a list of CIDRs reserved for static IP assignment, excluded from automatic allocation.\n+ // reservedSubnets is optional. When omitted, all IP addresses in `subnets` are available for automatic assignment.\n+ // IPs from these ranges can still be requested through static IP assignment in pod annotations.\n+ // Each item should be in range of the specified CIDR(s) in `subnets`.\n+ // The maximum number of entries allowed is 25.\n+ // The format should match standard CIDR notation (for example, \"10.128.0.0/16\").\n+ // This field must be omitted if `subnets` is unset or `ipam.mode` is `Disabled`.\n+ // +optional\n+ // +kubebuilder:validation:MinItems=1\n+ // +kubebuilder:validation:MaxItems=25\n+ ReservedSubnets []CIDR `json:\"reservedSubnets,omitempty\"`\n\n// JoinSubnets are used inside the OVN network topology.\n//\n// Dual-stack clusters may set 2 subnets (one for each IP family), otherwise only 1 subnet is allowed.\n// This field is only allowed for \"Primary\" network.\n// It is not recommended to set this field without explicit need and understanding of the OVN network topology.\n// When omitted, the platform will choose a reasonable default which is subject to change over time.\n//\n// +optional\nJoinSubnets DualStackCIDRs `json:\"joinSubnets,omitempty\"`\n\n+ // infrastructureSubnets specifies a list of internal CIDR ranges that OVN-Kubernetes will reserve for internal network infrastructure.\n+ // Any IP addresses within these ranges cannot be assigned to workloads.\n+ // When omitted, OVN-Kubernetes will automatically allocate IP addresses from `subnets` for its infrastructure needs.\n+ // When `reservedSubnets` is also specified the CIDRs cannot overlap.\n+ // When `defaultGatewayIPs` is also specified  the default gateway IPs must belong to one of the CIDRs.\n+ // Each item should be in range of the specified CIDR(s) in `subnets`.\n+ // The maximum number of entries allowed is 10.\n+ // The format should match standard CIDR notation (for example, \"10.128.0.0/16\").\n+ // This field must be omitted if `subnets` is unset or `ipam.mode` is `Disabled`.\n+ // +optional\n+ // +kubebuilder:validation:MinItems=1\n+ // +kubebuilder:validation:MaxItems=10\n+ InfrastructureSubnets []CIDR `json:\"infrastructureSubnets,omitempty\"`\n\n+ // defaultGatewayIPs specifies the default gateway IP used in the internal OVN topology.\n+ //\n+ // Dual-stack clusters may set 2 IPs (one for each IP family), otherwise only 1 IP is allowed.\n+ // This field is only allowed for \"Primary\" network.\n+ // It is not recommended to set this field without explicit need and understanding of the OVN network topology.\n+ // When omitted, an IP from network subnet is used.\n+ //\n+ // +optional\n+ DefaultGatewayIPs DualStackIPs `json:\"defaultGatewayIPs,omitempty\"`\n\n// IPAM section contains IPAM-related configuration for the network.\n// +optional\nIPAM *IPAMConfig `json:\"ipam,omitempty\"`\n}\n\n// +kubebuilder:validation:XValidation:rule=\"isIP(self)\", message=\"IP is invalid\"\ntype IP string\n\n// +kubebuilder:validation:MinItems=1\n// +kubebuilder:validation:MaxItems=2\n// +kubebuilder:validation:XValidation:rule=\"size(self) != 2 || !isIP(self[0]) || !isIP(self[1]) || ip(self[0]).family() != ip(self[1]).family()\", message=\"When 2 IPs are set, they must be from different IP families\"\ntype DualStackIPs []IP\n</code></pre> <p>The API changes mentioned above will be carried to the <code>NetworkAttachmentDefinition</code> JSON spec.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#ipamclaim-api-changes","title":"IPAMClaim API changes","text":"<p>The following pull request is tracking the IPAMClaim API change that introduces the status conditions: https://github.com/k8snetworkplumbingwg/ipamclaims/pull/9</p> <p>IPAMClaim CRD doc - <code>IPAM allocation on behalf of other entities</code> section</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#usage-example","title":"Usage Example","text":"<p>A user migrating services wants to import a workload pod preserving it's original IP address. Workload data:</p> <pre><code>IP: 192.168.100.205\nMAC: 00:1A:2B:3C:4D:5E\nDefault Gateway: 192.168.100.2\n</code></pre> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: network-l2\nspec:\n  topology: \"Layer2\"\n  layer2:\n    role: Primary\n    subnets: [\"192.168.100.0/24\"]\n    infrastructureSubnets: [\"192.168.100.0/30\"]  # used for OVN-Kubernetes infrastructure\n    reservedSubnets: [\"192.168.100.200/29\"]      # reserved for workloads that will require predefined addresses\n    defaultGatewayIPs: [\"192.168.100.2\"]\n</code></pre> <p>With this configuration, OVN-Kubernetes automatically assigns IPs from <code>.4-.199</code> and <code>.208-.254</code> for new workloads, while pods can request specific IPs from the reserved range:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: migrated-app\n  annotations:\n    v1.multus-cni.io/default-network: |\n      {\"name\": \"default\", \"namespace\": \"ovn-kubernetes\", \"ips\": [\"192.168.100.205\"], \"mac\": \"00:1A:2B:3C:4D:5E\", \"ipam-claim-reference\": \"my-claim\"}\nspec:\n</code></pre>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#implementation-details","title":"Implementation Details","text":""},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#configurability","title":"Configurability","text":"<p>The changes outlined in this enhancement should be configurable. This means a configuration knob is required to instruct OVN-Kubernetes on whether to process the annotation described in the Pod network identity section. The feature knob will be called <code>preconfigured-udn-addresses-enable</code>.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#networkselectionelement-annotation","title":"NetworkSelectionElement annotation","text":"<p>Currently, the <code>v1.multus-cni.io/default-network</code> annotation is only processed for the cluster default network. This enhancement will extend this behavior, allowing it to be applied to pods created in the primary Layer2 UDN as well. The annotation should only be processed for new pods, modifying it after the addresses were allocated won't be reflected in the pods network configuration and this should be blocked through a Validating Admission Policy:</p> <pre><code>apiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingAdmissionPolicy\nmetadata:\n  name: predefined-network-addresses\nspec:\n  matchConstraints:\n    resourceRules:\n      - apiGroups:   [\"\"]\n        apiVersions: [\"v1\"]\n        operations:  [\"UPDATE\"]\n        resources:   [\"pods\"]\n  failurePolicy: Fail\n  validations:\n    - expression: \"('v1.multus-cni.io/default-network' in oldObject.metadata.annotations) == ('v1.multus-cni.io/default-network' in object.metadata.annotations)\"\n      message: \"The 'v1.multus-cni.io/default-network' annotation cannot be changed after the pod was created\"\n</code></pre> <p>The <code>NetworkSelectionElement</code> structure has an extensive list of fields, this enhancement focuses only on the following:</p> <pre><code>type NetworkSelectionElement struct {\n    // Name contains the name of the Network object this element selects\n    Name string `json:\"name\"`\n    // Namespace contains the optional namespace that the network referenced\n    // by Name exists in\n    Namespace string `json:\"namespace,omitempty\"`\n    // IPRequest contains an optional requested IP addresses for this network\n    // attachment\n    IPRequest []string `json:\"ips,omitempty\"`\n    // MacRequest contains an optional requested MAC address for this\n    // network attachment\n    MacRequest string `json:\"mac,omitempty\"`\n    // IPAMClaimReference container the IPAMClaim name where the IPs for this\n    // attachment will be located.\n    IPAMClaimReference string `json:\"ipam-claim-reference,omitempty\"`\n}\n</code></pre> <p>Any other field set in the struct will be ignored by OVN-Kubernetes.</p> <p>When using the <code>v1.multus-cni.io/default-network</code> annotation, Multus strictly requires its value to reference an existing NAD. Multus then builds the CNI requests based on it. This proposal introduces a static default NAD object applied to the cluster. This object will serve as a stub to generate the CNI calls, preserving the current behavior:</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: default\n  namespace: ovn-kubernetes\nspec:\n  config: '{\"cniVersion\": \"0.4.0\", \"name\": \"ovn-kubernetes\", \"type\": \"ovn-k8s-cni-overlay\"}'\n</code></pre> <p>With this approach, users must configure the <code>Name</code> to <code>default</code> and the <code>Namespace</code> to <code>ovn-kubernetes</code>. This configuration ensures Multus still references the default network while OVN-Kubernetes will internally use the primary UDN to handle MAC/IP requests from the NSE.</p> <p>The default NAD object specified above is already used when the default network is exposed through BGP as part of the route advertisement feature. The proposal is to have it available all the time.</p> <p>With <code>k8s.ovn.org/primary-udn-ipamclaim</code> being deprecated in favor of the <code>IPAMClaimReference</code> field in the <code>NetworkSelectionElement</code> we have to define the expected behavior. To avoid conflicting settings when <code>v1.multus-cni.io/default-network</code> is set the <code>k8s.ovn.org/primary-udn-ipamclaim</code> is going to be ignored, it will be reflected in the opposite scenario for backwards compatibility with a plan to remove it in a future release. Deprecation plan for the <code>k8s.ovn.org/primary-udn-ipamclaim</code> annotation:</p> <ul> <li>release-N - emit a warning event stating that the annotation is deprecated and will be removed in a future release.</li> <li>release-N+1 - fail to configure pods with the annotation set.</li> <li>release-N+2 - remove any code handling the annotation, effectively ignoring it.</li> </ul> <p>Note that <code>GatewayRequest</code> is not listed, the default gateway is an attribute of the network is not going to be configurable per pod.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#address-allocation","title":"Address allocation","text":"<p>OVN-Kubernetes currently generates the overlay MAC addresses from the IPs:</p> <ul> <li>IPv4: It takes the four octets of the address (e.g <code>AA.BB.CC.DD</code>) and uses them to create the MAC address with a constant prefix (e.g. <code>0A:58:AA:BB:CC:DD</code>).</li> <li>IPv6: Computes a SHA256 checksum from the IPv6 string and uses the first four bytes for the MAC address with the <code>0A:58</code> constant prefix(e.g. <code>0A:58:SHA[0]:SHA[1]:SHA[2]:SHA[3]</code>).</li> </ul> <p>Although unlikely, we need to implement logic that ensures that the MAC address requested through the <code>NetworkSelectionElement</code> does not conflict with any other configured address on the UDN (including addresses consumed by OVN-Kubernetes).</p> <p>OVN-Kubernetes already persists the IP and MAC addresses in the <code>k8s.ovn.org/pod-networks</code> annotation for each pod:</p> <pre><code>// PodAnnotation describes the assigned network details for a single pod network. (The\n// actual annotation may include the equivalent of multiple PodAnnotations.)\ntype PodAnnotation struct {\n// IPs are the pod's assigned IP addresses/prefixes\nIPs []*net.IPNet\n// MAC is the pod's assigned MAC address\nMAC net.HardwareAddr\n// Gateways are the pod's gateway IP addresses; note that there may be\n// fewer Gateways than IPs.\nGateways []net.IP\n\n// GatewayIPv6LLA is the IPv6 Link Local Address for the pod's gateway, that is the address\n// that will be set as gateway with router advertisements\n// generated from the gateway router from the node where the pod is running.\nGatewayIPv6LLA net.IP\n\n// Routes are additional routes to add to the pod's network namespace\nRoutes []PodRoute\n\n// TunnelID assigned to each pod for layer2 secondary networks\nTunnelID int\n\n// Role defines what role this network plays for the given pod.\n// Expected values are:\n// (1) \"primary\" if this network is the primary network of the pod.\n//     The \"default\" network is the primary network of any pod usually\n//     unless user-defined-network-segmentation feature has been activated.\n//     If network segmentation feature is enabled then any user defined\n//     network can be the primary network of the pod.\n// (2) \"secondary\" if this network is the secondary network of the pod.\n//     Only user defined networks can be secondary networks for a pod.\n// (3) \"infrastructure-locked\" is applicable only to \"default\" network if\n//     a user defined network is the \"primary\" network for this pod. This\n//     signifies the \"default\" network is only used for probing and\n//     is otherwise locked for all intents and purposes.\n// At a given time a pod can have only 1 network with role:\"primary\"\nRole string\n}\n</code></pre> <p>This annotation will be used to build an initial cache of allocated addresses at startup, which will then be updated dynamically at runtime and used for conflict detection. A similar approach is required for IP address conflict detection. When a conflict is detected the pod should not start and an appropriate event should be emitted.</p> <p>When the <code>NetworkSelectionElement</code> contains an <code>IPAMClaimReference</code> the referenced IPAMClaim should reflect the IP allocation status including error reporting through the newly introduced <code>Conditions</code> status field. In the opposite scenario where the <code>NetworkSelectionElement</code> does not specify the <code>IPAMClaimReference</code> the IP allocation is not persisted when the pod is removed.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#testing-details","title":"Testing Details","text":"<p>The following scenarios should be covered in testing:</p> <ul> <li>VM workloads import into OVN-Kubernetes with no changes to the instances network configuration.</li> <li>Imported VM workloads can live-migrate to another node without any additional traffic disruption.</li> <li>'v1.multus-cni.io/default-network' cannot be changed after the pod was created.</li> <li>It should be possible to configure the pods MAC or the IP address without configuring the other.</li> <li>When <code>reservedSubnets</code> is configured automatic IP allocation should not use addresses specified in it.</li> <li>It should be possible to configure the pods IP address using the 'v1.multus-cni.io/default-network' even if the address is a part of the <code>reservedSubnets</code>.</li> <li>Requesting an IP address and default gateway IP that is not a part of the networks subnet should fail.</li> <li>Detect MAC and IP address conflicts between the requested addresses for a newly created pods and the addresses that are already allocated in the network.</li> <li>After configuring custom default gateway and management addresses on a Layer2 UDN the previous default IPs can be consumed by workloads(e.g. for 10.0.0.0/16 network create pods with 10.0.0.1 and 10.0.0.2 addresses).</li> <li>Modifying the  default gateway and management addresses on a Layer2 UDN should not be possible after the network was created.</li> </ul> <p>The scenarios mentioned above have to cover both IPv4 and IPv6 IP families.</p>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#documentation-details","title":"Documentation Details","text":""},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<ul> <li> <p>Modifying the 'v1.multus-cni.io/default-network' value after the pod was created could have unpredictable consequences. To mitigate this introduce a Validating Admission Policy described in Implementation Details.</p> </li> <li> <p>By allowing users to specify the IP and MAC addresses for the pods there is a risk of conflicts. To mitigate this OVN-Kubernetes will check that the requested addresses are not currently used in the UDN. There is still a risk that the user picks an address that's consumed by something outside of the UDN but that's beyond what OVN-Kubernetes controls and can check.</p> </li> <li> <p>The dynamic, per-node subnet allocation in Layer3 UDNs, where each node has a unique default gateway and management IP, makes user-specified UDN gateway/management IPs and static pod IP/MAC assignments very complex. This enhancement will not support Layer3 UDNs.</p> </li> <li> <p>BGP support today is limited to cluster UDNs, to ensure a non-NATed traffic for pods with predefined addresses the user has to use a cluster UDN to configure the network. This is a limitation unrelated to this enhancement and it is possible it will be solved in the future.</p> </li> <li> <p>By consuming the 'v1.multus-cni.io/default-network' annotation for altering the primary UDNs pod configuration the user won't be able to use it for configuring the cluster default network attachment. This is acceptable as there is currently no support for modifying the cluster default network through this annotation while using primary UDNs. If there is a requirement in the future another mechanism can be considered.</p> </li> <li> <p>OVN-Kubernetes computes MAC addresses from pod IPs rather than allocating them, which creates potential MAC address conflicts in a potential scenario where a MAC address previously used by a stopped VM gets consumed by OVN-Kubernetes for a dynamically allocated IP. To mitigate these conflicts, users will have to use a different MAC address and recreate the workload. For importing workloads that already use this prefix, a future enhancement could add a field to the Layer2 spec allowing users to specify a custom MAC prefix for the UDN.</p> </li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":""},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#alternatives","title":"Alternatives","text":"<ul> <li> <p>Instead of the Pod network identity approach, we could expand the IPAMClaim API. It currently lacks IP request capabilities, and using IPAMClaim for MAC addresses is confusing. Introducing a new API would mean deprecating the IPAMClaim, while managing upgrades and supporting both solutions for a period of time. This requires significant effort, which is not feasible at this time.</p> </li> <li> <p>As described in the NetworkSelectionElement annotation section, using the <code>v1.multus-cni.io/default-network</code> annotation means Multus strictly requires this annotation's value to reference an existing NAD. An alternative to the proposed approach would be to reference the NAD that defines the primary network. It was discarded as it would require OVN-Kubernetes to modify the CNI handling logic because multus would target the CNI requests towards the custom network. Additionally it would require users to determine the exact NAD name and namespace for every primary UDN pod needing custom MAC, IP, or IPAMClaim.</p> </li> </ul>"},{"location":"okeps/okep-5233-preconfigured-udn-addresses/#references","title":"References","text":"<ul> <li> <p>IPAMClaim CRD doc - <code>IPAM allocation on behalf of other entities</code> section</p> </li> <li> <p>IPAMClaim status conditions pull request: https://github.com/k8snetworkplumbingwg/ipamclaims/pull/9</p> </li> </ul>"},{"location":"okeps/okep-5259-no-overlay/","title":"OKEP-5259: No-overlay Mode For Layer-3 Networks using BGP","text":"<ul> <li>Issue: #5259</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#problem-statement","title":"Problem Statement","text":"<p>Currently, OVN-Kubernetes uses Geneve as its encapsulation method on the overlay network for east-west traffic; this adds overhead and reduces throughput. By leveraging OVN-Kubernetes support for BGP, we want to provide a way for users to enable a no-overlay mode, which would disable Geneve encapsulation and use direct routing between nodes for east-west traffic on selected networks.</p> <p>Many environments, particularly on-premise deployments or those with dedicated networking infrastructure, prefer to utilize the underlying physical network's routing capabilities directly. This \"no-overlay\" approach can offer several benefits:</p> <ul> <li>Improved Performance: Eliminates overlay protocol overhead, potentially   leading to lower latency and higher throughput for inter-pod communication.</li> <li>Leverage Existing Network Infrastructure: Integrates more seamlessly with   existing BGP-capable network devices, allowing direct routing to pod IPs.</li> <li>Reduced Resource Consumption: Fewer CPU cycles spent on   encapsulation/decapsulation.</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#terminology","title":"Terminology","text":"<p>See terminology details.</p>"},{"location":"okeps/okep-5259-no-overlay/#goals","title":"Goals","text":"<ul> <li>Support no-overlay mode for the cluster default network (CDN).</li> <li>Support no-overlay mode for Primary layer-3 ClusterUserDefinedNetworks   (CUDNs).</li> <li>The no-overlay mode is only supported for bare-metal clusters.</li> <li>A cluster can have networks operating in overlay and no-overlay modes   simultaneously.</li> <li>Use the BGP feature to exchange routes to pod subnets across the cluster.</li> <li>Allow direct communication without any overlay encapsulation for east-west   traffic.</li> <li>Maintain compatibility with existing OVN-Kubernetes features where applicable.</li> <li>Compatible with both local gateway and shared gateway modes.</li> <li>Do not compromise the existing advertised UDN isolation strict mode.</li> <li>Support a deployment option where no-overlay mode can be enabled with a   full-mesh BGP internal fabric managed by OVN-Kubernetes, allowing users to   leverage no-overlay mode without requiring external BGP infrastructure.</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#future-goals","title":"Future Goals","text":"<ul> <li>When OVN-Kubernetes supports BGP for UserDefinedNetwork CRs, extend no-overlay   mode support for Primary layer-3 UDNs.</li> <li>Support toggling no-overlay mode on/off for an existing network. To support   this functionality, there are two things we need to consider.   First, in no-overlay mode, as there is no Geneve overhead, a network can have   the same MTU size as the provider network. However, overlay networks require a   smaller MTU (default 1400). We cannot modify a pod interface MTU without   restarting the pod, so OVN-Kubernetes must work with other external   controllers to accommodate such a migration.   Secondly, to facilitate a smooth, node-by-node migration, it's crucial to   allow networks to operate in a hybrid no-overlay and overlay mode.</li> <li>Address the advertised UDN isolation loose mode issue when no-overlay mode is   enabled.</li> <li>Support route-reflector BGP topology for the managed cluster internal BGP   fabric. If FRR node instances can act as a BGP route reflector, it enables   creation of an internal route reflector-based BGP network. However, it is not   currently supported by frr-k8s, which we use for integrating OVN-Kubernetes   with FRR. We will need to implement this feature in frr-k8s first before   supporting this.</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#non-goals","title":"Non-Goals","text":"<ul> <li>This enhancement does not aim to change the default behavior of   OVN-Kubernetes, which will continue to use Geneve encapsulation for the   default network and any user-defined networks unless no-overlay mode is   explicitly enabled.</li> <li>This enhancement does not aim to change the existing CUDN lifecycle   management. The user must ensure that the CUDN CRs are correctly managed   according to the existing lifecycle management practices.</li> <li>This enhancement does not aim to implement no-overlay mode with the   centralized OVN architecture, as the BGP feature is only available in a   cluster running in interconnect mode.</li> <li>This enhancement does not aim to implement no-overlay mode for the layer-2   type of networks. The layer-2 type of networks is implemented by connecting   pods to the same OVN distributed logical switch. Pods on different nodes are   connected through a layer-2 segment using Geneve encapsulation. It's quite   challenging to implement a layer-2 network over a layer-3 infrastructure   without an overlay protocol.</li> <li>This enhancement does not aim to integrate no-overlay mode with the EVPN   feature.</li> <li>This enhancement does not aim to support users creating a no-overlay network   by manually creating a NetworkAttachmentDefinition (NAD) CR.</li> <li>This enhancement does not aim to support egress IP or egress service for   no-overlay networks. Both features leverage OVN-Kubernetes's logical routing   policy to steer pod egress traffic. They also rely on overlay tunnels to   transport pod egress traffic to egress nodes.</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#introduction","title":"Introduction","text":"<p>In the OVN-Kubernetes BGP Integration enhancement, no-overlay mode was briefly discussed. In this enhancement, we aim to describe the feature in detail, define the API changes we want to introduce for it, and address a number of concerns with respect to the existing BGP Integration and User-Defined Network Segmentation features.</p> <p>Avoiding Geneve encapsulation and using the provider network for east-west traffic stems from the need to minimize network overhead and maximize throughput. Users who intend to enable BGP on their clusters can leverage BGP-learned routes to achieve this. The goal is to provide users with an API to create networks (default or CUDN), with no-overlay mode enabled, allowing traffic to skip Geneve encapsulation (i.e., the overlay network) and simply make use of the learned routes in the underlay or provider network for inter-node communication.</p> <p>While BGP is the proposed solution for exchanging east-west routing information within a cluster, it is not the only possible option. Future implementations of no-overlay mode may incorporate additional routing technologies and protocols beyond BGP.</p>"},{"location":"okeps/okep-5259-no-overlay/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":""},{"location":"okeps/okep-5259-no-overlay/#story-1-have-the-cluster-default-network-in-the-no-overlay-mode","title":"Story 1: Have the cluster default network in the no-overlay mode","text":"<p>As a cluster admin, I want to enable no-overlay mode for the cluster default network to integrate seamlessly with existing BGP-capable networks, achieve maximum network performance, and reduce overhead.</p>"},{"location":"okeps/okep-5259-no-overlay/#story-2-create-a-cudn-in-the-no-overlay-mode","title":"Story 2: Create a CUDN in the no-overlay mode","text":"<p>As a cluster admin, I want to enable no-overlay mode for a cluster user defined network to integrate seamlessly with existing BGP-capable networks and achieve maximum network performance.</p>"},{"location":"okeps/okep-5259-no-overlay/#story-3-deploy-networks-in-the-no-overlay-mode-without-external-bgp-routers","title":"Story 3: Deploy networks in the no-overlay mode without external BGP routers","text":"<p>As a cluster admin, I want to use no-overlay mode for intra-cluster traffic without advertising pod networks to the external network or depending on external BGP routers.</p>"},{"location":"okeps/okep-5259-no-overlay/#proposed-solution","title":"Proposed Solution","text":"<p>This solution leverages the existing BGP feature to advertise each node's pod subnet via the CNCF project FRR-K8s. FRR routers managed by FRR-k8s then propagate these routes throughout the provider network, enabling direct pod-to-pod traffic without an overlay. A single cluster can simultaneously have networks in both overlay and no-overlay modes.</p> <p>When no-overlay mode is enabled for a network:</p> <ul> <li> <p>By default, when BGP advertisement is enabled, the BGP feature disables SNAT   for pod egress traffic leaving nodes. For users who only want to have   no-overlay mode but do not want to expose pod IPs externally, this behavior is   not ideal. To address this, users will be able to enable SNAT for   pod-to-external traffic. This will be configured via a new <code>outboundSNAT</code>   field in the <code>ClusterUserDefinedNetwork</code> CRD for CUDNs, and the   <code>outbound-snat</code> flag in the <code>[no-overlay]</code> section of the configuration file   for the default network.</p> </li> <li> <p>For east-west traffic:</p> </li> <li>Intra-node traffic remains unchanged, handled by the local OVN bridge.</li> <li>Inter-node traffic will follow the same path as north-south traffic, routed     via the underlay. Pod-to-pod, Pod-to-clusterIP traffic within the same     network will be forwarded by the provider network without encapsulation or     NAT. Pod-to-node egress traffic will be SNATed to the node IP at the source     node to allow a pod to access nodePort services in a different UDN.</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#configuration-file-and-flags","title":"Configuration File and Flags","text":"<p>For the default network transport, the new flag <code>transport</code> will be added to the <code>[default]</code> section of the configuration file:</p> <ul> <li>The <code>transport</code> flag accepts either <code>no-overlay</code> or   <code>geneve</code>, defaulting to <code>geneve</code>. Setting it to <code>no-overlay</code> configures the   default network to operate in no-overlay mode. More overlay transport   methods can be supported in the future. If   <code>transport=no-overlay</code> is set, but   no <code>RouteAdvertisements</code> CR is configured for advertising the default network,   ovnkube-cluster-manager will emit an event and log an error message.</li> </ul> <p>For the default network in no-overlay mode, the new flags <code>outbound-snat</code> and <code>routing</code> will be added to the <code>[no-overlay]</code> section of the configuration file:</p> <ul> <li> <p>The <code>outbound-snat</code> flag is a string flag to configure the SNAT behavior for   outbound traffic from pods on the default network. Supported values are   <code>enabled</code> or <code>disabled</code>. It is required when <code>transport=no-overlay</code>.</p> </li> <li> <p><code>enabled</code>: SNATs pod-to-external traffic to the node IP. This is useful when     pod IPs are not routable on the external network.</p> </li> <li><code>disabled</code>: Does not SNAT pod-to-external traffic. This requires pod IPs to     be routable on the external network.</li> </ul> <p>In no-overlay mode, pod-to-remote-pod traffic is never SNATed, regardless of   this flag's setting. However, traffic to remote nodes, the Kubernetes API   server, and DNS is always SNATed.</p> <ul> <li> <p>The <code>routing</code> flag is a string flag to configure whether the cluster default   network routing configuration is managed by OVN-Kubernetes or users. Supported   values are <code>managed</code> or <code>unmanaged</code>. It is required when   <code>transport=no-overlay</code>.</p> </li> <li> <p><code>managed</code>: OVN-Kubernetes manages the routing for the no-overlay network.     OVN-Kubernetes will generate the BGP configurations (RouteAdvertisements and     FRRConfiguration) to advertise the pod subnets, and set up BGP sessions     between nodes. In the managed routing mode, pod subnets will be advertised     to the default VRF of each node.</p> <p>Please note that the managed routing mode requires nodes to be directly connected at Layer 2. Therefore, the managed routing mode is not suitable for clusters that have nodes in different subnets, as pod subnets are not routable between nodes in different subnets. So normally, this option shall be used with <code>outbound-snat=enabled</code>.</p> <p>However, when the network is in the managed routing mode, users can still advertise the pod network to the external network by creating the RouteAdvertisements and FRRConfiguration CRs manually.</p> <p>More managed routing options can be specified by the flags in the section <code>[bgp-managed]</code> described below.</p> </li> <li> <p><code>unmanaged</code>: Users are responsible for managing the routing, including     creating the RouteAdvertisements and FRRConfiguration CRs to advertise the     pod subnets. It allows users to have more control over the BGP     configuration. Normally, this option shall be used with     <code>outbound-snat=disabled</code>.</p> <p>One day if other routing protocols are integrated with OVN-Kubernetes, users can use this option to manage the routing themselves.</p> </li> </ul> <p>To configure the internal BGP fabric that CDN and CUDNs in managed no-overlay mode will use, the configuration file will have a new section <code>[bgp-managed]</code>, with the following new flags:</p> <ul> <li>The <code>topology</code> flag specifies the BGP topology to use for the cluster internal   BGP fabric, with supported values of <code>full-mesh</code> initially, more topology   options can be added in the future.</li> </ul> <p>This flag is required when the CDN or any CUDN is configured in no-overlay   mode with <code>managed</code> routing. Configuration will fail if not provided. When   set, the <code>ovnkube-cluster-manager</code> generates FRRConfigurations to configure   the BGP topology for the cluster.</p> <p>If set to <code>full-mesh</code>, ovnkube-cluster-manager will generate FRRConfigurations   for all nodes to peer with each other in a full mesh. This topology is simpler   to configure but less scalable for large clusters.</p> <ul> <li>The <code>as-number</code> flag is used when no-overlay networks are configured with   managed routing. It is shared by both the cluster default network and CUDNs.   It specifies the AS number to be used by the BGP speakers on each node for its   default VRF. It's optional. The default AS number is <code>64512</code> if not specified.</li> </ul> <p>For CUDNs, no-overlay mode is configured via their respective CRs. Due to a BGP limitation, CUDNs with overlapping pod subnets cannot be advertised to the same VRF. To enable no-overlay mode for these networks, a VRF-lite configuration is required.</p> <p>Here is an example configuration file snippet for the default network in no-overlay mode:</p> <pre><code>[default]\ntransport = no-overlay\n\n[no-overlay]\noutbound-snat = enabled\nrouting = managed\n\n[bgp-managed]\ntopology = full-mesh\nas-number = 64514\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#api-details","title":"API Details","text":"<p>We introduce new fields to the <code>spec.network</code> section of the ClusterUserDefinedNetwork (CUDN) CRD to control network transport technology:</p> <ul> <li><code>transport</code>: Specifies the transport technology used for the network.</li> <li>Supported values: <code>Geneve</code> or <code>NoOverlay</code></li> <li>It's optional, if not specified, it will be treated as <code>Geneve</code>.</li> <li>More transport methods can be supported in the future.</li> <li><code>noOverlayOptions</code>: Contains configuration for no-overlay mode. This is   required when <code>transport</code> is <code>NoOverlay</code>.</li> <li><code>outboundSNAT</code>: Defines the SNAT behavior for outbound traffic from pods.<ul> <li>Supported values: <code>Enabled</code> or <code>Disabled</code>. It is required when <code>transport</code>   is <code>NoOverlay</code>.</li> <li><code>Enabled</code>: SNATs pod-to-external traffic to the node IP. This is useful   when pod IPs are not routable on the external network.</li> <li><code>Disabled</code>: Does not SNAT pod-to-external traffic. This requires pod IPs   to be routable on the external network.</li> <li>Pod-to-remote-pod traffic is never SNATed in no-overlay mode.</li> </ul> </li> <li><code>routing</code>: Specifies whether the pod network routing configuration is     managed by OVN-Kubernetes or users. It is required when <code>transport</code> is     <code>NoOverlay</code>.<ul> <li>Supported values: <code>Managed</code> or <code>Unmanaged</code>.</li> <li><code>Managed</code>: OVN-Kubernetes manages the routing for the no-overlay network.   OVN-Kubernetes will generate the per-node BGP configurations to advertise   the pod subnets, and set up a full-mesh BGP topology where each node peers   with all other nodes in the cluster. In the managed routing mode, pod   subnets will be advertised to the default VRF of each node. When the   network is in the managed routing mode, users can still advertise the pod   network to the external network by creating the RouteAdvertisements and   FRRConfiguration CRs manually.</li> <li><code>Unmanaged</code>: Users are responsible for managing the routing, including   creating the RouteAdvertisements and FRRConfiguration CRs to advertise the   pod subnets. It allows users to have more control over the BGP   configuration. One day if other routing protocols are integrated with   OVN-Kubernetes, users can use this option to manage the routing   themselves.</li> </ul> </li> </ul> <p>The <code>spec</code> field of a ClusterUserDefinedNetwork CR is immutable. Therefore, the transport configuration cannot be changed after a ClusterUserDefinedNetwork CR is created.</p> <pre><code>type TransportOption string\ntype SNATOption string\ntype RoutingOption string\n\nconst (\n    TransportOptionNoOverlay  TransportOption = \"NoOverlay\"\n    TransportOptionGeneve     TransportOption = \"Geneve\"\n\n    SNATEnabled  SNATOption = \"Enabled\"\n    SNATDisabled SNATOption = \"Disabled\"\n\n    RoutingManaged   RoutingOption = \"Managed\"\n    RoutingUnmanaged RoutingOption = \"Unmanaged\"\n)\n\n// NoOverlayOptions contains configuration options for networks operating in no-overlay mode.\ntype NoOverlayOptions struct {\n    // OutboundSNAT defines the SNAT behavior for outbound traffic from pods.\n    // +kubebuilder:validation:Enum=Enabled;Disabled\n    // +required\n    OutboundSNAT SNATOption `json:\"outboundSNAT\"`\n    // Routing specifies whether the pod network routing is managed by OVN-Kubernetes or users.\n    // +kubebuilder:validation:Enum=Managed;Unmanaged\n    // +required\n    Routing RoutingOption `json:\"routing\"`\n}\n\ntype NetworkSpec struct {\n    ...\n    // Transport describes the transport technology used for the network.\n    // Allowed values are \"NoOverlay\" and \"Geneve\".\n    // - \"NoOverlay\": The network operates in no-overlay mode.\n    // - \"Geneve\": The network uses Geneve overlay.\n    // Defaults to \"Geneve\" if not specified.\n    // +kubebuilder:validation:Enum=NoOverlay;Geneve\n    // +optional\n    Transport TransportOption `json:\"transport,omitempty\"`\n    // NoOverlayOptions contains configuration for no-overlay mode.\n    // It is only allowed when Transport is \"NoOverlay\".\n    // +optional\n    NoOverlayOptions *NoOverlayOptions `json:\"noOverlayOptions,omitempty\"`\n}\n</code></pre> <p>To prevent a user from setting unsupported transport configurations in CUDN accidentally, we will add the following CEL validation rules to the <code>spec.network</code> field of the ClusterUserDefinedNetwork CRD.</p> <pre><code>    network:\n      x-kubernetes-validations:\n      - message: \"transport 'NoOverlay' is only supported for Layer3 primary networks\"\n        rule: \"!has(self.transport) || self.transport != 'NoOverlay' || (self.topology == 'Layer3' &amp;&amp; has(self.layer3) &amp;&amp; self.layer3.role == 'Primary')\"\n      - message: \"noOverlayOptions is required if and only if transport is 'NoOverlay'\"\n        rule: \"(has(self.transport) &amp;&amp; self.transport == 'NoOverlay') == has(self.noOverlayOptions)\"\n</code></pre> <p>A new status condition will be added to the CUDN CR to indicate whether the transport is correctly configured for a CUDN. The UDN controller in the ovnkube-cluster-manager is responsible for setting this condition based on the transport configuration. The UDN controller will watch RouteAdvertisements CR events and validate whether a RouteAdvertisements CR is correctly configured to advertise the pod networks.</p> <p>Here is the definition of the new status condition:</p> <ul> <li><code>TransportAccepted</code>: Indicates whether the transport is   correctly configured.</li> <li><code>True</code>: The transport is correctly configured.</li> <li><code>False</code>: The transport is not correctly configured.</li> <li><code>Unknown</code>: The status of the transport configuration is unknown.</li> <li>This condition is only applicable when <code>spec.network.transport</code> is not set     to <code>Geneve</code>.</li> <li>For a no-overlay CUDN, this condition is set to <code>True</code> when:<ul> <li>A RouteAdvertisements CR is created to advertise the pod subnets of   the CUDN, and its status is <code>Accepted</code>.</li> <li>Otherwise, it will be set to <code>False</code> with an appropriate message.</li> </ul> </li> </ul> <p>If the transport is not configured or configured as <code>Geneve</code>, the UDN controller shall set the <code>TransportAccepted</code> condition like this:</p> <pre><code>status:\n  conditions:\n  - type: TransportAccepted\n    status: \"True\"\n    reason: \"GeneveTransportAccepted\"\n    message: \"Geneve transport has been configured.\"\n</code></pre> <p>If the transport is configured as <code>NoOverlay</code> and the RouteAdvertisements CR is created and its status is <code>Accepted</code>, the UDN controller shall set the <code>TransportAccepted</code> condition like this:</p> <pre><code>status:\n  conditions:\n  - type: TransportAccepted\n    status: \"True\"\n    reason: \"NoOverlayTransportAccepted\"\n    message: \"Transport has been configured as 'no-overlay'.\"\n</code></pre> <p>If no RouteAdvertisements CR is advertising the network, it shall update the CUDN status condition <code>TransportAccepted</code> to <code>False</code> with an appropriate message.</p> <pre><code>status:\n  conditions:\n  - type: TransportAccepted\n    status: \"False\"\n    message: \"No RouteAdvertisements CR is advertising the pod networks.\"\n    reason: \"NoOverlayRouteAdvertisementsIsMissing\"\n</code></pre> <p>Or if the RouteAdvertisements CR exists but its status is not accepted:</p> <pre><code>status:\n  conditions:\n  - type: TransportAccepted\n    status: \"False\"\n    message: \"RouteAdvertisements CR blue advertises the pod subnets, but its status is not accepted.\"\n    reason: \"NoOverlayRouteAdvertisementsNotAccepted\"\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#example-of-a-layer-3-cudn-with-no-overlay-mode-enabled","title":"Example of a layer-3 CUDN with no-overlay mode enabled","text":"<p>A layer-3 CUDN with no-overlay mode enabled should look like this:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: my-cudn\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"red\", \"blue\"]\n  network:\n    topology: Layer3\n    layer3:\n      role: Primary\n      mtu: 1500\n      subnets:\n      - cidr: 10.10.0.0/16\n        hostSubnet: 24\n    transport: \"NoOverlay\"\n    noOverlayOptions:\n      outboundSNAT: \"Disabled\"\n      routing: \"Unmanaged\"\nstatus:\n  conditions:\n  - type: TransportAccepted\n    status: \"True\"\n    reason: \"NoOverlayTransportAccepted\"\n    message: \"Transport has been configured as 'no-overlay'.\"\n</code></pre> <p>Or: if the routing is managed by OVN-Kubernetes:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: my-cudn\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"red\", \"blue\"]\n  network:\n    topology: Layer3\n    layer3:\n      role: Primary\n      mtu: 1500\n      subnets:\n      - cidr: 10.10.0.0/16\n        hostSubnet: 24\n    transport: \"NoOverlay\"\n    noOverlayOptions:\n      outboundSNAT: \"Enabled\"\n      routing: \"Managed\"\nstatus:\n  conditions:\n  - type: TransportAccepted\n    status: \"True\"\n    reason: \"NoOverlayTransportAccepted\"\n    message: \"Transport has been configured as 'no-overlay'.\"\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#implementation-details","title":"Implementation Details","text":"<p>No-overlay mode relies on the BGP feature, which is exclusive to clusters deployed in the single-node zone interconnect mode. Therefore, no-overlay mode can only be enabled in clusters configured with single-node zone interconnect.</p> <p>For networks configured with no-overlay mode, the OVN topology requires minor adjustments. In the current interconnect architecture, a network can span all cluster nodes thanks to the cross-node logical links that OVN establishes between the transit switch and the ovn-cluster-router instances on each node. That is the logical representation of the OVN network, and in practice, these links are implemented through Geneve tunnels between every node in the cluster.</p> <p>To route pod traffic directly over the underlay, the network's logical topology is modified: the transit switch will not be created anymore, thus the <code>ZoneInterconnectHandler</code> in ovnkube-controller will not create a connection to the transit switch. As a result, static routes to remote pod subnets will not be added to the ovn_cluster_router. Instead, traffic will be forwarded using a source-based route, with the next hop determined by the cluster's gateway mode. ovn_cluster_router will route all east-west traffic to the provider network, following the same path as north-south traffic.</p> <p>The following diagram shows the OVN topology of a cluster with three networks:</p> <ul> <li>a CUDN (in green) that uses Geneve encapsulation (default behavior)</li> <li>a default network (in gray) that is operating in no-overlay mode</li> <li>a CUDN (in blue) that is operating in no-overlay mode</li> </ul> <p></p>"},{"location":"okeps/okep-5259-no-overlay/#traffic-path-in-shared-gateway-mode","title":"Traffic Path in Shared Gateway Mode","text":"<p>At the ovn_cluster_router, the pod-to-remote-pod egress traffic will be forwarded to the GatewayRouter via the join switch. Unlike the current route advertisement behavior, routes to remote pod subnets will be imported to the gateway router. Once the traffic reaches the GatewayRouter, it will be forwarded to the nexthop node according to the imported BGP routes.</p> <p>An example traffic path is as follows (assuming pod IP of 10.128.0.10 on node A, destined to remote pod 10.128.2.11 on node B):</p> <p>As the east-west traffic will not go through the transit switch, the static routes to remote pod subnets will not be added to the ovn_cluster_router anymore. The ovn_cluster_router routes all local pod egress traffic to the gateway router according to a source-based route (10.128.0.0/24). The routing table of the default network's ovn_cluster_router for node A would look like:</p> <pre><code>IPv4 Routes\nRoute Table &lt;main&gt;:\n               100.64.0.4                100.64.0.4 dst-ip\n            10.128.0.0/24                100.64.0.4 src-ip\n            10.128.0.0/16                100.64.0.4 src-ip\n</code></pre> <p>And the routing table of the gateway router with learned BGP routes to remote pod subnets:</p> <pre><code>IPv4 Routes\nRoute Table &lt;main&gt;:\n           169.254.0.0/17               169.254.0.4 dst-ip rtoe-GR_nodeA\n            10.128.0.0/16                100.64.0.1 dst-ip\n            10.128.1.0/24                172.18.0.2 dst-ip rtoe-rtoe-GR_nodeA\n            10.128.2.0/24                172.18.0.4 dst-ip rtoe-rtoe-GR_nodeA\n                0.0.0.0/0                172.18.0.1 dst-ip rtoe-rtoe-GR_nodeA\n</code></pre> <pre><code>sequenceDiagram\n    participant Pod1 on Node A\n    participant ovn_cluster_router Node A\n    participant OVN Gateway Router (GR) Node A\n    participant Underlay Physical Network\n    participant Node B\n\n    Pod1 on Node A-&gt;&gt;ovn_cluster_router Node A: Packet to remote Pod2 (10.128.2.11)\n    Note over ovn_cluster_router Node A: route lookup\n    ovn_cluster_router Node A-&gt;&gt;ovn_cluster_router Node A: Match the static source-based route (10.128.0.0/24 100.64.0.4)\n    ovn_cluster_router Node A-&gt;&gt;OVN Gateway Router (GR) Node A: Forward packet to GR via join switch\n    Note over OVN Gateway Router (GR) Node A: route lookup\n    OVN Gateway Router (GR) Node A-&gt;&gt;OVN Gateway Router (GR) Node A: Match BGP route: 10.128.2.0/24 via 172.18.0.3 (Node B's IP)\n    OVN Gateway Router (GR) Node A-&gt;&gt;Underlay Physical Network: Route packet out via breth0\n    Underlay Physical Network-&gt;&gt;Node B: Packet arrives at Node B\n    Note over Node B: Traffic is delivered to Pod2\n</code></pre> <p>And the reply path:</p> <pre><code>sequenceDiagram\n    participant Node B\n    participant Underlay Physical Network\n    participant OVN Gateway Router (GR) Node A\n    participant ovn_cluster_router Node A\n    participant Pod1 on Node A\n\n    Note over Node B: Traffic is delivered from Pod2\n    Node B-&gt;&gt;Underlay Physical Network: Packet leaves Node B\n    Underlay Physical Network-&gt;&gt;OVN Gateway Router (GR) Node A: Route packet in via breth0\n    Note over OVN Gateway Router (GR) Node A: route lookup\n    OVN Gateway Router (GR) Node A-&gt;&gt;OVN Gateway Router (GR) Node A: Match static route: 10.128.0.0/24 via 100.64.0.1 (Node A's pod subnet)\n    OVN Gateway Router (GR) Node A-&gt;&gt;ovn_cluster_router Node A: forwarded via the join switch\n    Note over ovn_cluster_router Node A: route lookup\n    ovn_cluster_router Node A-&gt;&gt;ovn_cluster_router Node A: Match the direct route to local pods\n    ovn_cluster_router Node A-&gt;&gt;Pod1 on Node A: Packet to local Pod1 (10.128.0.5) via the node switch\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#traffic-path-in-local-gateway-mode","title":"Traffic Path in Local Gateway Mode","text":"<p>The pod-to-remote-pod egress traffic will enter the host VRF via the ovn-k8s-mpX interface. Then it will be forwarded to remote nodes according to the host routing table of the VRF.</p> <p>An example traffic path is as follows (assuming pod IP of 10.128.0.10, destined to remote pod 10.128.2.11):</p> <p>The routing table of the default VRF of node A which contains learned BGP routes:</p> <pre><code>10.128.0.0/16 via 10.128.2.1 dev ovn-k8s-mp0\n10.128.0.0/24 dev ovn-k8s-mp0 proto kernel scope link src 10.128.0.2\n10.128.1.0/24 nhid 30 via 172.18.0.2 dev eth0 proto bgp metric 20\n10.128.2.0/24 nhid 25 via 172.18.0.4 dev eth0 proto bgp metric 20\n</code></pre> <pre><code>sequenceDiagram\n    participant Pod1 on Node A\n    participant ovn_cluster_router Node A\n    participant Host VRF/Kernel Node A\n    participant Underlay Physical Network\n    participant Node B\n\n    Pod1 on Node A-&gt;&gt;ovn_cluster_router Node A: Packet to remote Pod2 (10.128.2.11)\n    ovn_cluster_router Node A-&gt;&gt;Host VRF/Kernel Node A: Forward packet to host via ovn-k8s-mp0\n    Note over Host VRF/Kernel Node A: BGP route lookup\n    Host VRF/Kernel Node A-&gt;&gt;Host VRF/Kernel Node A: Match BGP route in host table: 10.128.2.0/24 via 172.18.0.3 (Node B's IP)\n    Host VRF/Kernel Node A-&gt;&gt;Underlay Physical Network: Route packet out via breth0\n    Underlay Physical Network-&gt;&gt;Node B: Packet arrives at Node B\n    Note over Node B: Traffic is delivered to Pod2\n</code></pre> <p>And the reply path:</p> <pre><code>sequenceDiagram\n    participant Node B\n    participant Underlay Physical Network\n    participant Host VRF/Kernel Node A\n    participant ovn_cluster_router Node A\n    participant Pod1 on Node A\n\n    Note over Node B: Traffic is delivered from Pod2\n    Node B-&gt;&gt;Underlay Physical Network: Packet leaves Node B\n    Underlay Physical Network-&gt;&gt;Host VRF/Kernel Node A: Route packet in via breth0\n    Note over Host VRF/Kernel Node A: route lookup\n    Host VRF/Kernel Node A-&gt;&gt;Host VRF/Kernel Node A: Match static route: 10.128.0.0/24 via 100.64.0.1 (Node A's pod subnet)\n    Host VRF/Kernel Node A -&gt;&gt;ovn_cluster_router Node A: forwarded via mp0\n    Note over ovn_cluster_router Node A: route lookup\n    ovn_cluster_router Node A-&gt;&gt;ovn_cluster_router Node A: Match the direct route to local pods\n    ovn_cluster_router Node A-&gt;&gt;Pod1 on Node A: Packet to local Pod1 (10.128.0.5) via the node switch\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#import-routes-to-nodesubnets","title":"Import Routes to NodeSubnets","text":"<p>Changes to BGP behavior are necessary to import node subnet routes into the gateway router and the host routing table of each node. A new <code>transport</code> key will be included in the OVN CNI NetConf (the NAD's spec.config JSON) to indicate the east-west transport protocol of the network. If a network operates in no-overlay mode, this key is set to <code>noOverlay</code>. Additionally, the <code>outboundSNAT</code> key will be included to indicate the SNAT behavior for outbound traffic from pods. This information can then be passed into the NetInfo object and utilized by the route import and gateway controller of ovnkube-controller.</p> <p>Note: Users must not set these fields manually; OVN-Kubernetes manages them.</p> <p>An example OVN-Kubernetes NAD may look like:</p> <pre><code>apiVersion: k8s.cni.cncf.io/v1\nkind: NetworkAttachmentDefinition\nmetadata:\n  name: l3-network\n  namespace: default\nspec:\n  config: |\n    {\n            \"cniVersion\": \"0.3.1\",\n            \"name\": \"l3-network\",\n            \"type\": \"ovn-k8s-cni-overlay\",\n            \"topology\":\"layer3\",\n            \"mtu\": 1500,\n            \"netAttachDefName\": \"default/l3-network\",\n            \"role\": \"primary\",\n            \"transport\": \"noOverlay\",\n            \"outboundSNAT\": \"disabled\"\n    }\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#host-routing-tables","title":"Host Routing Tables","text":"<p>To maintain isolation between UDNs, node BGP speakers do not accept routes to pod subnets by default. However, to route east-west traffic in no-overlay mode, these routes must be imported to the host routing table.</p>"},{"location":"okeps/okep-5259-no-overlay/#gateway-router","title":"Gateway Router","text":"<p>Currently, BGP routes to other pod subnets are not imported into the gateway router, which ensures node-to-pod traffic traverses the OVN logical topology. However, we need the routes to be imported into the gateway router in no-overlay mode. We need to update the route import controller to check whether a network is in no-overlay mode and import the routes from the host routing table if so.</p>"},{"location":"okeps/okep-5259-no-overlay/#configurable-snat-for-egress-traffic","title":"Configurable SNAT for Egress Traffic","text":"<p>By default, the OVN-Kubernetes BGP feature disables SNAT for all pod egress traffic when the pod network is advertised. The only exception is traffic destined to other nodes in the cluster (pod-to-node), which remains SNATed to the node IP to ensure nodePort services can be accessed across networks.</p> <p>However, this default behavior assumes all external destinations can route back to the pod IPs. This assumption fails for destinations reached via non-BGP routers (such as a default gateway). To address this, we introduce configurable SNAT behavior via:</p> <ul> <li><code>outboundSNAT</code> field in the ClusterUserDefinedNetwork CRD for CUDNs</li> <li><code>outbound-snat</code> flag in the <code>[no-overlay]</code> configuration section for the   default network</li> </ul> <p>SNAT Behavior:</p> <ul> <li> <p><code>outboundSNAT: Disabled</code> (or <code>outbound-snat=disabled</code>): Pod egress traffic   is not SNATed. Pod IPs must be routable on the external network.</p> </li> <li> <p><code>outboundSNAT: Enabled</code> (or <code>outbound-snat=enabled</code>): Pod egress traffic   to external destinations is SNATed to the node IP. This is intended for   deployments where pod networks are only advertised to an internal BGP fabric   and are not routable from external networks.</p> </li> </ul> <p>Important: Regardless of this setting, pod-to-remote-pod traffic within the same network is never SNATed, while traffic to remote nodes, the Kubernetes API server, and DNS is always SNATed.</p>"},{"location":"okeps/okep-5259-no-overlay/#shared-gateway-mode","title":"Shared Gateway Mode","text":"<p>When <code>outboundSNAT</code> is disabled, the existing BGP feature behavior is preserved: pod egress traffic is not SNATed (see OKEP-5296 for details).</p> <p>When <code>outboundSNAT</code> is enabled, the gateway router maintains the default SNAT rule for pod-to-external traffic while exempting pod-to-remote-pod traffic. To achieve this, the network controllers in ovnkube-controller create an address set containing the pod subnets for each CUDN and the default network, then add an exemption rule using this address set.</p> <p>The implementation uses the <code>exempted_ext_ips</code> field rather than adding a negative match to the <code>match</code> field because negative matches in OVN SNAT rules are inefficient. Using <code>exempted_ext_ips</code> results in one OpenFlow rule per pod SNAT entry in the OVS bridge, providing better performance.</p> <p>In the following example, <code>10.128.0.0/24</code> is the pod subnet in the local node, <code>10.128.0.0/16</code> is the CIDR of the pod network. <code>outboundSNAT</code> is enabled.</p> <pre><code># Address_Set for the no-overlay pod network CIDR\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b82d\naddresses           : [\"10.128.0.0/16\"]\nexternal_ids        : {ip-family=v4}\nname                : a15397791517117706455\n\n# per-pod SNAT rule in the Gateway Router (exclude pod-network destinations)\n_uuid               : cd9264de-f2c6-4528-8f53-c2ccb5b23c8d\nallowed_ext_ips     : []\nexempted_ext_ips    : [\"a15397791517117706455\"] # Exclude traffic to the pod Network\nexternal_ids        : {}\nexternal_ip         : \"172.18.0.2\"            # Node/GR external IP used for SNAT for the default network, or a Masquerade IP for a CUDN.\nexternal_mac        : []\nexternal_port_range : \"32768-60999\"\ngateway_port        : []\nlogical_ip          : \"10.128.0.7\"            # The local pod IP\nlogical_port        : []\nmatch               : \"\"                      # SNAT all non-exempt destinations (node and external)\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n</code></pre> <p>For UDN, the behavior is similar. The UDN controller will create an address set for the UDN pod subnet and add an exemption rule to the Gateway Router of that UDN.</p> <p>In the following example, <code>10.10.1.0/24</code> is the pod subnet in the local node, <code>10.10.0.0/16</code> is the CIDR of the pod network. <code>outboundSNAT</code> is enabled.</p> <pre><code># Address_Set for the no-overlay UDN pod network CIDR\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b85d\naddresses           : [\"10.10.0.0/16\"]\nexternal_ids        : {ip-family=v4}\nname                : a15397791517117706478\n\n# per-pod SNAT rule in the Gateway Router (exclude pod-network destinations)\n_uuid               : 5f72b7a5-e7fc-4d14-8dbf-70d5ba304452\nallowed_ext_ips     : []\nexempted_ext_ips    : [\"a15397791517117706478\"] # Exclude traffic to the pod Network\nexternal_ids        : {\"k8s.ovn.org/network\"=cluster_udn_e2e-test-bgp-1, \"k8s.ovn.org/topology\"=layer3}\nexternal_ip         : \"169.254.0.12\"            # Masquerade IP for the CUDN\nexternal_mac        : []\nexternal_port_range : \"32768-60999\"\ngateway_port        : []\nlogical_ip          : \"10.10.1.0/24\"            # The local pod subnet\nlogical_port        : []\nmatch               : \"\"                        # SNAT all non-exempt destinations (node and external)\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#local-gateway-mode","title":"Local Gateway Mode","text":"<p>When <code>outboundSNAT</code> is disabled, the existing BGP feature behavior is preserved: pod egress traffic is not SNATed (see OKEP-5296 for details).</p> <p>When <code>outboundSNAT</code> is enabled for the default network, pod egress traffic is forwarded to the host kernel by the ovn-cluster-router, where it is then SNATed to the node IP using nftables rules.</p> <p>In the following example, <code>10.128.0.0/24</code> is the pod subnet in the local node, <code>10.128.0.0/16</code> is the CIDR of the pod network. <code>outboundSNAT</code> is enabled for the network.</p> <pre><code>table ip nat {\n    chain postrouting {\n        type nat hook postrouting priority 100; policy accept;\n        # skip SNAT for pod-to-pod traffic\n        ip saddr 10.128.0.0/24 ip daddr 10.128.0.0/16 return\n        ip saddr 10.128.0.0/24 masquerade\n    }\n}\n</code></pre> <p>For UDN, before the pod egress traffic entering the host kernel, the ovn-cluster-router will perform a SNAT first to change source IP to the UDN masquerade IP. So instead of changing the nftables rules. The UDN controller in the ovnkube-controller will modify the SNAT rule in the ovn-cluster-router.</p> <p>In the following example, <code>10.10.1.0/24</code> is the pod subnet in the local node, <code>10.10.0.0/16</code> is the CIDR of the pod network. <code>outboundSNAT</code> is enabled for the network.</p> <pre><code># Address_Set for the no-overlay pod network CIDR\n_uuid               : a718588e-0a9b-480e-9adb-2738e524b85d\naddresses           : [\"10.10.0.0/16\"]\nexternal_ids        : {ip-family=v4}\nname                : a15397791517117706478\n\n# per-pod SNAT rule in the ovn-cluster-router (exclude pod-network destinations)\n_uuid               : 5f72b7a5-e7fc-4d14-8dbf-70d5ba304452\nallowed_ext_ips     : []\nexempted_ext_ips    : [\"a15397791517117706478\"] # Exclude traffic to the pod Network\nexternal_ids        : {\"k8s.ovn.org/network\"=cluster_udn_e2e-test-bgp-1, \"k8s.ovn.org/topology\"=layer3}\nexternal_ip         : \"169.254.0.12\"\nexternal_mac        : []\nexternal_port_range : \"32768-60999\"\ngateway_port        : []\nlogical_ip          : \"10.10.1.0/24\"\nlogical_port        : rtos-cluster_udn_e2e.test.bgp.1_ovn-worker\nmatch               : \"\"                        # SNAT all non-exempt destinations (node and external)\noptions             : {stateless=\"false\"}\npriority            : 0\ntype                : snat\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#udn-traffic-isolation","title":"UDN Traffic Isolation","text":"<p>Currently, for inter-UDN pod-to-pod traffic, OVN-Kubernetes supports 2 modes: loose and strict. In the strict mode, inter-UDN traffic is blocked by ACL rule defined in the node switch. In no-overlay mode, this isolation mode will still operate in the same way.</p> <p>In the loose mode, inter-UDN traffic is allowed to be routed by external routers. However, loose mode can be broken when multiple CUDNs are advertised to the same VRF. Because routes to pod subnets are learned by the host routing table and then imported into the gateway router, OVN-Kubernetes may route inter-UDN traffic directly, which violates the intended loose-mode behavior. We may address this limitation in the future by generating additional import-filter rules via FRR-K8s. One workaround is to use either eBGP on their external router, or iBGP with <code>next-hop-self</code> to ensure that inter-UDN traffic is routed by an external router instead of the node.</p>"},{"location":"okeps/okep-5259-no-overlay/#workflow","title":"Workflow","text":""},{"location":"okeps/okep-5259-no-overlay/#enable-no-overlay-mode-for-the-default-network-with-unmanaged-routing","title":"Enable No-overlay Mode for the Default Network with Unmanaged Routing","text":"<ol> <li>The frr-k8s pods shall be deployed on each node.</li> <li> <p>The cluster admin shall create a base FRRConfiguration CR that is used for    generating the per-node FRRConfiguration instances by OVN-Kubernetes.</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: external-rr\n  namespace: frr-k8s-system\n  labels:\n    network: default\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      # the external BGP route reflector\n      - address: 172.20.0.2\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: filtered\n            prefixes:\n            # accept pod subnets of the default network explicitly\n            - prefix: 10.128.0.0/16\n              ge: 24\n</code></pre> </li> <li> <p>The cluster admin shall create a RouteAdvertisements CR to advertise the    default network. Make sure the RouteAdvertisements CR selects the    FRRConfiguration defined in the previous step.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default\nspec:\n  advertisements:\n  - PodNetwork\n  # Select the FRRConfiguration defined in step-2 with the custom label.\n  frrConfigurationSelector:\n    matchLabels:\n      network: default\n  networkSelectors:\n  - networkSelectionType: DefaultNetwork\n  # The empty nodeSelector selects all nodes. We don't support a network in an overlay and no-overlay hybrid mode.\n  nodeSelector: {}\n</code></pre> </li> <li> <p>The cluster admin enables no-overlay mode by running ovn-kubernetes with    config file below:</p> <pre><code>[default]\ntransport = no-overlay\n\n[no-overlay]\nrouting = unmanaged\noutbound-snat = disabled\n</code></pre> </li> <li> <p>The default network controller in ovnkube-controller will create the OVN    virtual topology for the default network. Neither the transit switch nor the    static routes to remote pod subnets will be created.</p> </li> <li> <p>The default network controller in ovnkube-cluster-manager shall watch the    RouteAdvertisements CRs. It shall ensure at least one RouteAdvertisements CR    is correctly configured to advertise the pod networks and will validate the    RouteAdvertisements CR status. If the RouteAdvertisements CR is missing or    its status is not accepted, it shall report event and log an error message.</p> </li> <li> <p>The BGP feature is responsible for exchanging BGP routes to remote pod    subnets between nodes. Ultimately, these routes will be imported into the    gateway router.</p> </li> </ol>"},{"location":"okeps/okep-5259-no-overlay/#enable-no-overlay-mode-for-the-default-network-with-managed-routing","title":"Enable No-overlay Mode for the Default Network with Managed Routing","text":"<ol> <li> <p>The frr-k8s pods shall be deployed on each node.</p> </li> <li> <p>In the Managed routing mode, the default network controller in    ovnkube-cluster-manager will create the RouteAdvertisements CR. A new    FRRConfiguration controller will be responsible for creating the base    FRRConfiguration CR which sets up the cluster internal BGP fabric. The    cluster admin only needs to start ovn-kubernetes with the following flags:</p> <pre><code>[default]\ntransport = no-overlay\n\n[no-overlay]\nrouting = managed\noutbound-snat = enabled\n\n[bgp-managed]\ntopology = full-mesh\nas-number = 64514\n</code></pre> </li> <li> <p>The default network controller in ovnkube-cluster-manager will create a base    FRRConfiguration CR in the <code>frr-k8s-system</code> namespace, and a    RouteAdvertisements CR that advertises the default network. The generated    FRRConfiguration will configure BGP speakers on each node to advertise the    pod subnets and set up a full-mesh iBGP topology where each node peers with    all other nodes in the cluster.</p> </li> </ol> <p>When nodes are added or removed, the FRRConfiguration controller shall update    the base FRRConfiguration CR, and the per-node FRRConfiguration instances    accordingly.</p> <p>For example, if the cluster has 3 nodes with pod subnets <code>10.128.0.0/24</code>,    <code>10.128.1.0/24</code>, and <code>10.128.2.0/24</code>. The generated RouteAdvertisements CR    will look like:</p> <pre><code>```yaml\napiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: ovnk-managed-&lt;hash&gt;\nspec:\n  advertisements:\n  - PodNetwork\n  frrConfigurationSelector:\n    matchLabels:\n      k8s.ovn.org/managed-internal-fabric: bgp # Selects the base FRRConfiguration CR managed by OVN-Kubernetes\n  networkSelectors:\n  - networkSelectionType: DefaultNetwork\n  nodeSelector: {}\n```\n\nAnd the generated base FRRConfiguration CR for a full mesh BGP topology\nwill look like:\n\n```yaml\napiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: ovnk-managed-&lt;hash&gt;\n  namespace: frr-k8s-system\n  labels:\n    k8s.ovn.org/managed-internal-fabric: bgp # This label is set on FRRConfigurations managed by OVN-Kubernetes to configure the internal fabric\nspec:\n  bgp:\n    routers:\n    - asn: 64514\n      neighbors:\n      # BGP peers with other nodes\n      - address: &lt;node1-ip&gt;\n        asn: 64514\n        toReceive:\n          # Allow to receive the routes to remote pod subnets.\n          # This is no-overlay specific behavior.\n          allowed:\n            prefixes:\n            - ge: 24\n              le: 24\n              prefix: 10.128.0.0/16\n      - address: &lt;node2-ip&gt;\n        asn: 64514\n        toReceive:\n          allowed:\n            prefixes:\n            - ge: 24\n              le: 24\n              prefix: 10.128.0.0/16\n      - address: &lt;node3-ip&gt;\n        asn: 64514\n        toReceive:\n          allowed:\n            prefixes:\n            - ge: 24\n              le: 24\n              prefix: 10.128.0.0/16\n```\n</code></pre> <p>The following steps are the same as steps 5 to 7 in the previous section.</p>"},{"location":"okeps/okep-5259-no-overlay/#create-a-clusteruserdefinednetwork-in-no-overlay-mode-with-unmanaged-routing","title":"Create a ClusterUserDefinedNetwork in No-overlay Mode with Unmanaged Routing","text":"<p>Here is a configuration example:</p> <ol> <li>The frr-k8s pods shall be deployed on each node.</li> <li> <p>A cluster admin wants to enable no-overlay mode for the blue network by    creating the following ClusterUserDefinedNetwork CR.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: blue\n  labels:\n    network: blue\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"ns1\", \"ns2\"]\n  network:\n    topology: Layer3\n    layer3:\n      role: Primary\n      # The UDN MTU shall not be larger than the provider network's MTU.\n      mtu: 1500\n      subnets:\n      - cidr: 10.10.0.0/16\n        hostSubnet: 24\n    transport: \"NoOverlay\"\n    noOverlayOptions:\n      outboundSNAT: \"Disabled\"\n      routing: \"Unmanaged\"\n</code></pre> </li> <li> <p>The cluster admin has created an FRRConfiguration CR to peer with an external    BGP router <code>182.18.0.5</code>.</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: blue\n  namespace: frr-k8s-system\n  # A custom label\n  labels:\n    network: blue\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      - address: 182.18.0.5\n        asn: 64512\n        disableMP: true\n        holdTime: 1m30s\n        keepaliveTime: 30s\n        passwordSecret: {}\n        port: 179\n        toAdvertise:\n          allowed:\n            mode: filtered\n        toReceive:\n          # Allow to accept the routes to remote pod subnets.\n          allowed:\n            mode: filtered\n            prefixes:\n            - ge: 24\n              le: 24\n              prefix: 10.10.0.0/16\n</code></pre> </li> <li> <p>The cluster admin advertises the CUDN pod network. In this example, the    <code>targetVRF</code> is not set, meaning routes to the pod networks will be advertised    to the default VRF.</p> </li> </ol> <p>If users want to advertise the pod networks to a specific VRF or deploy    VRF-lite they can set the <code>targetVRF</code> field in the RouteAdvertisements CR.</p> <pre><code>```yaml\napiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: blue\nspec:\n  # nodeSelector must be empty, since we don't support a network in an overlay and no-overlay hybrid mode.\n  nodeSelector: {}\n  frrConfigurationSelector:\n    matchLabels:\n    # Select the FRRConfiguration defined in step-3\n    network: blue\n  networkSelectors:\n  - networkSelectionType: ClusterUserDefinedNetwork\n    clusterUserDefinedNetworkSelector:\n      networkSelector:\n        matchLabels:\n          # Select the CUDN defined in step-2\n          network: blue\n  advertisements:\n  - PodNetwork\n```\n</code></pre> <ol> <li> <p>The UDN controller in ovnkube-controller will create the OVN virtual topology    for the UDN. Neither the transit switch nor the static routes to remote pod    subnets will be created.</p> </li> <li> <p>The UDN controller in ovnkube-cluster-manager will watch the    RouteAdvertisements events to check whether at least one RouteAdvertisements    CR is correctly configured to advertise the pod networks and its status is    <code>Accepted</code>. The UDN controller will update the status condition    <code>TransportAccepted</code> accordingly.</p> </li> <li> <p>The BGP feature is responsible for exchanging BGP routes to remote pod    subnets between nodes. Ultimately, these routes will be imported into the    gateway router.</p> </li> </ol>"},{"location":"okeps/okep-5259-no-overlay/#create-a-clusteruserdefinednetwork-in-no-overlay-mode-with-managed-routing","title":"Create a ClusterUserDefinedNetwork in No-overlay Mode with Managed Routing","text":"<ol> <li> <p>The frr-k8s pods shall be deployed on each node.</p> </li> <li> <p>In the Managed routing mode, a new FRRConfiguration controller will be    responsible for creating the base FRRConfiguration CR which sets up the    cluster internal BGP fabric. The cluster admin only needs to start    ovn-kubernetes with the following flags:</p> <pre><code>[bgp-managed]\ntopology = full-mesh\nas-number = 64514\n</code></pre> </li> </ol> <p>The generated FRRConfiguration will be the same as the one described in the    managed default network section.</p> <ol> <li> <p>A cluster admin wants to enable no-overlay mode for the blue network by    creating the following ClusterUserDefinedNetwork CR.</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterUserDefinedNetwork\nmetadata:\n  name: blue\n  labels:\n    network: blue\nspec:\n  namespaceSelector:\n    matchExpressions:\n    - key: kubernetes.io/metadata.name\n      operator: In\n      values: [\"ns1\", \"ns2\"]\n  network:\n    topology: Layer3\n    layer3:\n      role: Primary\n      # The UDN MTU shall not be larger than the provider network's MTU.\n      mtu: 1500\n      subnets:\n      - cidr: 10.10.0.0/16\n        hostSubnet: 24\n    transport: \"NoOverlay\"\n    noOverlayOptions:\n      outboundSNAT: \"Enabled\"\n      routing: \"Managed\"\n</code></pre> </li> <li> <p>The UDN controller in ovnkube-cluster-manager will create a    RouteAdvertisements CR that advertises the blue network.</p> </li> </ol> <p>The generated RouteAdvertisements CR will look like:</p> <pre><code>```yaml\napiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: ovnk-managed-&lt;hash&gt;\nspec:\n  advertisements:\n  - PodNetwork\n  frrConfigurationSelector:\n    matchLabels:\n      k8s.ovn.org/managed-internal-fabric: bgp # Selects the base FRRConfiguration CR managed by OVN-Kubernetes\n  networkSelectors:\n  - networkSelectionType: ClusterUserDefinedNetwork\n    clusterUserDefinedNetworkSelector:\n      networkSelector:\n        matchLabels:\n          network: blue\n  nodeSelector: {}\n```\n</code></pre> <p>The following steps are the same as steps 5 to 7 in the previous section.</p>"},{"location":"okeps/okep-5259-no-overlay/#deployment-considerations","title":"Deployment Considerations","text":""},{"location":"okeps/okep-5259-no-overlay/#bgp-topology","title":"BGP Topology","text":"<p>The selected <code>FRRConfiguration</code> CR determines the deployment mode for the no-overlay network. There will be no new field added to the FRRConfiguration. The goal is to exchange the routes to the pod subnets across the cluster. The possible BGP topologies are varied. Users shall be able to choose any topology that is most suitable for their use case. However, frr and frr-k8s may not support some BGP features. Here we list some common deployment options.</p>"},{"location":"okeps/okep-5259-no-overlay/#ebgp-peering-with-external-routers","title":"eBGP Peering with External Routers","text":"<p>A common deployment model is to use eBGP to peer each node with an external BGP speaker, such as a Top-of-Rack (ToR) switch. This approach is often preferred in enterprise environments as it provides clear Autonomous System (AS) boundaries and allows for more advanced routing policies (e.g., AS path prepending, MEDs, and filtering based on AS paths).</p> <p>In this topology, the FRR instance on each node establishes an eBGP session with its upstream router. The upstream routers are then responsible for advertising the node subnet routes to the rest of the network. It is also possible for different groups of nodes (e.g. nodes in different racks) to belong to different Autonomous Systems.</p> <pre><code>graph LR\n    subgraph AS 65002 [AS 65002: Rack 1]\n        direction LR\n        NodeA(Node A)\n        NodeB(Node B)\n    end\n    subgraph AS 65003 [AS 65003: Rack 2]\n        direction LR\n        NodeC(Node C)\n    end\n    subgraph AS 65001 [AS 65001: External Network]\n        direction LR\n        ToR1(ToR Switch 1)\n        ToR2(ToR Switch 2)\n    end\n    NodeA &lt;-- eBGP --&gt; ToR1\n    NodeB &lt;-- eBGP --&gt; ToR1\n    NodeC &lt;-- eBGP --&gt; ToR2\n    ToR1  &lt;-- iBGP --&gt; ToR2\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#full-mesh-ibgp-across-the-cluster","title":"Full-Mesh iBGP across the Cluster","text":"<p>The FRR instance on each node maintains a full-mesh BGP peer relationship with all other nodes across the cluster. In this mode, users can enable no-overlay mode without relying on external BGP routes. Enabling <code>outboundSNAT</code> for the no-overlay network allows pod egress traffic to be SNATed to the node IP, enabling communication with external destinations even when the pod network is not advertised externally.</p> <p>This is particularly useful in smaller clusters or in environments where external BGP speakers are not available. However, it is important to note that a full-mesh iBGP setup can lead to increased CPU and memory consumption on the nodes, as each node must maintain a session with every other node.</p> <pre><code>graph LR\n    subgraph AS 64512 [ AS 64512 ]\n      direction LR\n      A(Node A)\n      B(Node B)\n      C(Node C)\n      A &lt;-- iBGP --&gt; B\n      B &lt;-- iBGP --&gt; C\n      C &lt;-- iBGP --&gt; A\n    end\n</code></pre> <p>Here's an example FRRConfiguration:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  labels:\n    network: default\n  name: receive-all\n  namespace: frr-k8s-system\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      # All nodes are added as neighbors to setup a full-mesh BGP topology\n      neighbors:\n      - address: 192.168.111.20\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: all\n      - address: 192.168.111.21\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: all\n      - address: 192.168.111.22\n        asn: 64512\n        disableMP: true\n        toReceive:\n          allowed:\n            mode: all\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#external-route-reflectors-for-larger-clusters","title":"External Route Reflectors for Larger Clusters","text":"<p>In a large cluster, a full-mesh BGP setup leads to more CPU and memory consumption on the nodes. Instead of every node peering with every other node, nodes peer only with external BGP route reflectors. This significantly reduces the number of BGP sessions each node needs to maintain, improving scalability.</p> <pre><code>graph TD\n    subgraph AS 64512 [ AS 64512 ]\n        direction LR\n        subgraph \"Route Reflector\"\n            RR1(Route Reflector 1)\n        end\n        subgraph \"Kubernetes Nodes\"\n            NodeA(Node A)\n            NodeB(Node B)\n            NodeC(Node C)\n        end\n    end\n    NodeA &lt;-- iBGP --&gt; RR1\n    NodeB &lt;-- iBGP --&gt; RR1\n    NodeC &lt;-- iBGP --&gt; RR1\n</code></pre> <p>Users can also force the BGP routes to use the route reflector as the next hop by setting the <code>next-hop-self force</code> option on the iBGP neighbors. Such configuration can be used together with the UDN isolation loose mode to prevent inter-UDN traffic from being routed by the node.</p> <p>Here is an example of how to configure the iBGP neighbors on FRR routers:</p> <pre><code>  neighbor 172.18.0.2 activate\n  neighbor 172.18.0.2 route-reflector-client\n  neighbor 172.18.0.2 next-hop-self force\n</code></pre>"},{"location":"okeps/okep-5259-no-overlay/#feature-compatibility","title":"Feature Compatibility","text":""},{"location":"okeps/okep-5259-no-overlay/#multiple-external-gateways-meg","title":"Multiple External Gateways (MEG)","text":"<p>MEG cannot work with advertised networks, therefore MEG cannot work with networks operating in the no-overlay mode either.</p>"},{"location":"okeps/okep-5259-no-overlay/#egress-ip","title":"Egress IP","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5259-no-overlay/#services","title":"Services","text":"<p>MetalLB will still be used in order to advertise services across the BGP fabric.</p>"},{"location":"okeps/okep-5259-no-overlay/#egress-service","title":"Egress Service","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5259-no-overlay/#egress-firewall","title":"Egress Firewall","text":"<p>Full support.</p>"},{"location":"okeps/okep-5259-no-overlay/#egress-qos","title":"Egress QoS","text":"<p>Full support.</p>"},{"location":"okeps/okep-5259-no-overlay/#network-policyanp","title":"Network Policy/ANP","text":"<p>Full support.</p>"},{"location":"okeps/okep-5259-no-overlay/#ipsec","title":"IPsec","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5259-no-overlay/#multicast","title":"Multicast","text":"<p>Not supported.</p> <p>OVN does not support routing multicast traffic to external networks. OVN logical routers only support forwarding multicast traffic between logical switches. It cannot exchange PIM messages with external routers. Additionally, FRR-K8s does not currently support configuring the FRR multicast functions (IGMP/MLD, PIM, etc.). Therefore, multicast is not supported in no-overlay mode.</p>"},{"location":"okeps/okep-5259-no-overlay/#userdefinednetworks-connect","title":"UserDefinedNetworks Connect","text":"<p>Not supported.</p>"},{"location":"okeps/okep-5259-no-overlay/#testing-details","title":"Testing Details","text":"<ul> <li>E2E Testing Details   The E2E tests shall cover the following combinations:</li> <li>Gateway Modes: LGW and SGW</li> <li>IP Modes: IPv4, IPv6, and dual-stack</li> <li>BGP Topologies: iBGP with an external route reflector, full-mesh iBGP,     iBGP+eBGP combined and eBGP only.</li> <li>BGP VRF-lite with multiple CUDNs</li> <li>test suite: conformance, control-plane</li> <li>API Testing Details</li> <li>ClusterUserDefinedNetwork CR reports expected status conditions</li> <li>No-overlay mode cannot be enabled for UserDefinedNetworks</li> <li>Scale Testing Details</li> <li>The impact of the number of the imported BGP routes</li> <li>Performance Testing Details</li> <li>Test pod-to-pod throughput and latency</li> <li>Cross-Feature Testing Details - coverage for interaction with other features</li> <li>UDN isolation between UDNs in no-overlay mode<ul> <li>Default mode</li> <li>Loose mode</li> </ul> </li> <li>NetworkPolicy</li> <li>EgressFirewall</li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-5259-no-overlay/#risks","title":"Risks","text":"<p>In the no-overlay mode, east-west traffic relies on the BGP network. Therefore, internal and external BGP speaker outages may impact cluster networking.</p> <p>The no-overlay mode inherits all limitations from the BGP integration feature. Consequently, multicast is not supported in no-overlay mode.</p> <p>As OVN is no longer delivering pod-to-pod traffic end-to-end, it will necessitate BGP knowledge for debugging.</p>"},{"location":"okeps/okep-5259-no-overlay/#known-limitations","title":"Known Limitations","text":"<ul> <li> <p>Features that rely on OVN to deliver pod-to-pod traffic, such as IPSec,   EgressIP and EgressService, are not supported in no-overlay mode.   OVN-Kubernetes will report an event/warning.</p> </li> <li> <p>When users configure egress IP or egress service for no-overlay networks,   OVN-Kubernetes won't block the configuration or set a status condition to the   EgressIP or EgressService CR. As we believe such feature compatibility issues   are not no-overlay specific, it should be designed and addressed   comprehensively in future enhancements.</p> </li> <li> <p>The UDN loose mode isolation will be broken if multiple no-overlay UDNs are   advertised to the same VRF. Before this issue is resolved, if the user wants   to use loose mode with no-overlay, then they should either use eBGP on their   external router, or iBGP with next-hop-self to ensure that inter-UDN traffic   is routed by an external router instead of the node. However, the drawback of   this workaround is that all the intra-UDN traffic will be routed by the   external router too, which may add extra latency and increase the load on the   external router.</p> </li> <li> <p>The managed routing mode uses a full-mesh BGP topology where each node peers   with all other nodes in the cluster. This approach does not scale well for   large clusters, as the number of BGP sessions grows quadratically (N*(N-1)/2   sessions for N nodes), leading to increased CPU and memory consumption on each   node. For large-scale deployments, users should consider using the unmanaged   routing mode with external route reflectors or other scalable BGP topologies.</p> </li> <li> <p>Changing the transport type between Geneve and no-overlay for the cluster   default network is not supported. Users must create a new cluster with the   desired transport type configured at installation time.</p> </li> </ul>"},{"location":"okeps/okep-5259-no-overlay/#ovn-kubernetes-version-skew","title":"OVN Kubernetes Version Skew","text":"<p>To be discussed.</p>"},{"location":"okeps/okep-5259-no-overlay/#alternatives","title":"Alternatives","text":"<p>N/A</p>"},{"location":"okeps/okep-5259-no-overlay/#references","title":"References","text":"<ol> <li>OKEP-5296: OVN-Kubernetes BGP Integration</li> <li>OKEP-5193: User Defined Network Segmentation</li> </ol>"},{"location":"okeps/okep-5296-bgp/","title":"OKEP-5296: OVN-Kubernetes BGP Integration","text":"<ul> <li>Issue: #5296</li> </ul>"},{"location":"okeps/okep-5296-bgp/#networking-glossary","title":"Networking Glossary","text":"Term Definition BGP (Border Gateway Protocol) A standardized exterior gateway protocol used to exchange routing and reachability information between autonomous systems (ASes) on the Internet. BGP makes routing decisions based on path attributes and policies. BGPd A daemon (background service) that implements the BGP protocol, commonly found in routing software suites like FRRouting (FRR) or Quagga. It manages BGP sessions, route advertisements, and policy enforcement. BFD (Bidirectional Forwarding Detection) A network protocol that quickly detects faults in the path between two forwarding devices. It enables sub-second failure detection, often used with BGP or OSPF for faster convergence. iBGP (Internal BGP) A BGP session between routers within the same autonomous system (AS). It is used to distribute external routes internally, usually requiring a full mesh or route reflectors. eBGP (External BGP) A BGP session between routers in different autonomous systems. It is the primary method for routing between networks on the Internet. OSPF (Open Shortest Path First) A link-state interior gateway protocol (IGP) used to distribute IP routing information within a single autonomous system. It calculates the shortest path to each destination using Dijkstra's algorithm and converges quickly."},{"location":"okeps/okep-5296-bgp/#problem-statement","title":"Problem Statement","text":"<p>OVN-Kubernetes currently has no native routing protocol integration, and relies on a Geneve overlay for east/west traffic, as well as third party operators to handle external network integration into the cluster. The purpose of this enhancement is to introduce BGP as a supported routing protocol with OVN-Kubernetes. The extent of this support will allow OVN-Kubernetes to integrate into different BGP user environments, enabling it to dynamically expose cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN.</p>"},{"location":"okeps/okep-5296-bgp/#goals","title":"Goals","text":"<ul> <li>To provide a user facing API to allow configuration of iBGP or eBGP peers, along typical BGP configuration including   communities, route filtering, etc.</li> <li>Support for advertising Egress IP addresses.</li> <li>To enable BFD to BGP peers.</li> <li>To allow east/west traffic without encapsulation.</li> <li>ECMP routing support within OVN for BGP learned routes.</li> <li>Support for advertising user-defined networks via BGP as long as there is no subnet overlap over the default VRF.</li> <li>Allowing for VRF-Lite type of VPN where the user maps interfaces on the host to user-defined VRFs/networks and   advertises VPN routes via BGP sessions over said VRFs.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#non-goals","title":"Non-Goals","text":"<ul> <li>Running separate BGPd instances per VRF network.</li> <li>Providing any type of API or operator to automatically connect two Kubernetes clusters via L3VPN.</li> <li>Replacing the support that MetalLB provides today for advertising service IPs.</li> <li>Support for any other type of BGP speaker other than FRR.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#future-goals","title":"Future Goals","text":"<ul> <li>Support EVPN configuration and integration with a user\u2019s DC fabric, along with MAC-VRFs and IP-VRFs.</li> <li>Support iBGP with route reflectors.</li> <li>Potentially advertising other IP addresses, including the Kubernetes API VIP across the BGP fabric.</li> <li>Optimize Layer 2 network routing.</li> <li>Allow selecting only a subset of nodes to advertise BGP.</li> <li>Support other routing protocols like OSPF.</li> <li>Support disabling Geneve as an overlay transport and either using a pure routed, no-overlay topology as well as an EVPN VXLAN overlay.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#introduction","title":"Introduction","text":"<p>There are multiple driving factors which necessitate integrating BGP into OVN-Kubernetes. They will be broken down into sections below, describing each use case/requirement. Additionally, implementing BGP paves the way for full EVPN support in the future, which is the choice of networking fabric in the modern data center. For purposes of this document, the external, physical network of the cluster which a user administers will be called the \u201cprovider network\u201d.</p>"},{"location":"okeps/okep-5296-bgp/#importing-routes-from-the-provider-network","title":"Importing Routes from the Provider Network","text":"<p>Today there is no API for a user to be able to configure routes into OVN. In order for a user to change how egress traffic is routed, the user leverages local gateway mode. This mode forces traffic to hop through the Linux networking stack, and there a user can configure routes inside of the host via NM State to control egress routing. This manual configuration would need to be performed and maintained across nodes and VRFs within each node.</p> <p>Additionally, if a user chooses to not manage routes within the host for local gateway mode, or the user chooses shared gateway mode, then by default traffic is always sent to the default gateway. The only other way to affect egress routing is by using the Multiple External Gateways (MEG) feature. With this feature the user may choose to have multiple different egress gateways per namespace to send traffic to.</p> <p>As an alternative, configuring BGP peers and which route-targets to import would eliminate the need to manually configure routes in the host, and would allow dynamic routing updates based on changes in the provider\u2019s network.</p>"},{"location":"okeps/okep-5296-bgp/#exporting-routes-into-the-provider-network","title":"Exporting Routes into the Provider Network","text":"<p>There exists a need for provider networks to learn routes directly to services and pods today in Kubernetes. More specifically, MetalLB is already one solution where load balancer IPs are advertised by BGP to provider networks. The goal of this RFE is to not duplicate or replace the function of MetalLB. MetalLB should be able to interoperate with OVN-Kubernetes, and be responsible for advertising services to a provider\u2019s network.</p> <p>However, there is an alternative need to advertise pod IPs on the provider network. One use case is integration with 3rd party load balancers, where they terminate a load balancer and then send packets directly to OCP nodes with the destination IP address being the pod IP itself. Today these load balancers rely on custom operators to detect which node a pod is scheduled to and then add routes into its load balancer to send the packet to the right node.</p> <p>By integrating BGP and advertising the pod subnets/addresses directly on the provider network, load balancers and other entities on the network would be able to reach the pod IPs directly.</p>"},{"location":"okeps/okep-5296-bgp/#datapath-performance","title":"Datapath Performance","text":"<p>In cases where throughput is a priority, using the underlay directly can eliminate the need for tunnel encapsulation, and thus reducing the overhead and byte size of each packet. This allows for greater throughput.</p>"},{"location":"okeps/okep-5296-bgp/#multi-homing-link-redundancy-fast-convergence","title":"Multi-homing, Link Redundancy, Fast Convergence","text":"<p>BGP can use multi-homing with ECMP routing in order to provide layer 3 failover. When a link goes down, BGP can reroute via a different path. This functionality can be coupled with BFD in order to provide fast failover. A typical use case is in a spine and leaf topology, where a node has multiple NICs for redundancy that connect to different BGP routers in topology. In this case if a link goes down to one BGP peer or the BGP peer fails, BFD can detect the outage on the order of milliseconds to hundreds of milliseconds and quickly assist BGP in purging the routes to that failed peer. This then causes fast failover and route convergence to the alternate peer.</p>"},{"location":"okeps/okep-5296-bgp/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<ul> <li>As a user I want to be able to leverage my existing BGP network to dynamically learn routes to pods in my Kubernetes   cluster.</li> <li>As a user, rather than having to maintain routes with NM State manually in each Kubernetes node, as well as being   constrained to using local gateway mode for respecting user defined routes; I want to use BGP so that I can dynamically   advertise egress routes for the Kubernetes pod traffic in either gateway mode.</li> <li>As a user where maximum throughput is a priority, I want to reduce packet overhead by not having to encapsulate   traffic with Geneve.</li> <li>As an egress IP user, I do not want to have to restrict my nodes to the same layer 2 segment and prefer   to use a pure routing implementation to handle advertising egress IP movement across nodes.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#proposed-solution","title":"Proposed Solution","text":"<p>OVN-Kubernetes will leverage other CNCF projects that already exist to enable BGP in Linux. FRR will be used as the BGP speaker, as well as provide support for other protocols like BFD. For FRR configuration, the MetalLB project has already started an API to be able to configure FRR: https://github.com/metallb/frr-k8s. While some of the configuration support for FRR may be directly exposed by FRR-K8S API, it may also be the case that some intermediary CRD provided by OVN-Kubernetes is required to integrate OVN-Kubernetes networking concepts into FRR.</p> <p>Functionally, FRR will handle advertising and importing routes and configuring those inside a Linux VRF. OVN-Kubernetes will be responsible for listening on netlink and configuring logical routers in OVN with routes learned by FRR. OVN-Kubernetes will manage FRR configuration through the FRR-K8S API in order to advertise routes outside of the node onto the provider network.</p> <p>There should be no changes required in FRR. FRR-K8S may need extended APIs to cover the OVN-Kubernetes use cases proposed in this enhancement. The lifecycle management of FRR-K8S and FRR is outside the scope of this enhancement and will not be managed by OVN-Kubernetes.</p> <p>OVN will require no changes.</p>"},{"location":"okeps/okep-5296-bgp/#workflow-description","title":"Workflow Description","text":"<p>An admin has the ability to configure BGP peering and choose what networks to advertise. A tenant is able to define networks for their namespace, but requires admin permission in order to expose those networks onto the provider's BGP fabric. A typical workflow will be for a user or admin to create a user-defined network, and then the admin will be responsible to:</p> <ol> <li>If setting up VRF-Lite, do any host modifications necessary via NMState to enslave an IP interface to the    matching network VRF.</li> <li>Configure BGP peering via interacting with the FRR-K8S API for a given set of worker nodes. Also define filters for    what routes should be received from the provider network.</li> <li>Create the OVN-Kubernetes RouteAdvertisements CR to configure what routes and where to advertise them.</li> <li>Verify peering and that routes have been propagated correctly.</li> </ol> <p>For detailed examples, see the BGP Configuration section.</p>"},{"location":"okeps/okep-5296-bgp/#api-extensions","title":"API Extensions","text":"<p>FRR-K8S API will be used in order to create BGP Peering and configure other BGP related configuration. A RouteAdvertisements CRD will be introduced in order to determine which routes should be advertised for specific networks. Additionally, the network CRD will be modified in order to expose a new transport field to determine if encapsulation should be used for east/west traffic.</p>"},{"location":"okeps/okep-5296-bgp/#api-details","title":"API Details","text":""},{"location":"okeps/okep-5296-bgp/#route-advertisements","title":"Route Advertisements","text":"<p>When OVN-Kubernetes detects that a FRR-K8S CR has been created for BGP peering, OVN-Kubernetes will by default advertise the pod subnet for each node, via creating an additive, per node FRR-K8S CR that is managed by OVN-Kubernetes. OVN-Kubernetes CRD:</p> <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default\nspec:\n  nodeSelector: {}\n  frrConfigurationSelector: {}\n  networkSelectors:\n    - networkSelectionType: ClusterUserDefinedNetworks\n      clusterUserDefinedNetworkSelector:\n        networkSelector:\n          matchLabels:\n            bgp: \"enabled\"\n  targetVRF: default\n  advertisements:\n    - PodNetwork\n    - EgressIP\n</code></pre> <p>In the above example, a required networkSelector field is populated to match on CUDNs matching the shown label. Alternatively, a user may specify the DefaultNetwork as the networkSelectionType. If a selector is used that selects the default cluster network, it will be enabled for any namespace using the cluster default network. Additionally, advertisements may be limited to specific nodes via the nodeSelector.</p> <p>In the case where a CUDN network selector is used, the networks will be checked by OVN-Kubernetes to determine if there is any overlap of the IP subnets. If so, an error status will be reported to the CRD and no BGP configuration will be done by OVN-Kubernetes.</p> <p>Multiple CRs may not select the same network. If this happens OVN-Kubernetes will report an error to the RouteAdvertisements CR and will refuse to apply the config.</p> <p>The CRD will support enabling advertisements for pod subnet and egress IPs. Note, MetalLB still handles advertising LoadBalancer IP so there is no conflict of responsibilities here. When the pod network is set to be advertised, there is no longer a need to SNAT pod IPs for this network to the node IP. Therefore, when pod network advertisements are enabled, the traffic from these pods will no longer be SNAT'ed on egress.</p> <p>The \"targetVRF\" key is used to determine which VRF the routes should be advertised in. The default value is \"default\", in which case routes will be leaked into the default VRF. One use case for this would be when a user wants to define a network with a specific IP addressing scheme, and then wants to advertise the pod IPs into the provider BGP network without VPN. Note that by using route leaking with a user-defined network, the network is no longer fully isolated, as now any other networks also leaked or attached to that VRF may reach this user-defined network. If a user attempts to leak routes into a targetVRF for a user-defined network whose IP subnet would collide with another, OVN-Kubernetes will report an error to the RouteAdvertisement status. Alternatively, a user may specify the value \"auto\", which in which case OVN-Kubernetes will advertise routes in the VRF that corresponds to the selected network. Any other values other than \"auto\" or \"default\" will result in an error. There is no support at this time for routing leaking into any other VRF.</p> <p>The frrConfigurationSelector is used in order to determine which FRRConfiguration CR to use for building the OVN-Kubernetes driven FRRConfiguration. OVN-Kubernetes needs to leverage a pre-existing FRRConfiguration to be able to find required pieces of configuration like BGP peering, etc. If more than one FRRConfiguration is found matching the selector, then an error will be propagated to the RouteAdvertisements CR and no configuration shall be done.</p>"},{"location":"okeps/okep-5296-bgp/#implementation-details","title":"Implementation Details","text":""},{"location":"okeps/okep-5296-bgp/#bgp-configuration","title":"BGP Configuration","text":"<p>When OVN-Kubernetes detects that a FRRConfiguration has been created that has a corresponding and valid FRRNodeState, OVN-Kubernetes will then use RouteAdvertisements CR create a corresponding FRRConfiguration. The following examples will use an environment where a user has created an FRRConfiguration:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  creationTimestamp: \"2024-06-11T14:22:37Z\"\n  generation: 1\n  name: metallb-ovn-worker\n  namespace: metallb-system\n  resourceVersion: \"1323\"\n  uid: 99b64be3-4f36-4e0b-8704-75aa5182d89f\n  labels:\n    routeAdvertisements: default\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      - address: 172.18.0.5\n        asn: 64512\n        disableMP: false\n        holdTime: 1m30s\n        keepaliveTime: 30s\n        passwordSecret: {}\n        port: 179\n        toAdvertise:\n          allowed:\n            mode: filtered\n        toReceive:\n          allowed:\n            mode: filtered\n  nodeSelector:\n    matchLabels:\n       kubernetes.io/hostname: ovn-worker\n</code></pre> <p>ovnkube-controller will check that if this FRRConfiguration applies to its node, ovn-worker. For this example, a user-defined network named \"blue\", has been created with a network of 10.0.0.0/16, and a matching vrf exists in the Linux host. The slice of this supernet that has been allocated to node ovn-worker is 10.0.1.0/24.</p>"},{"location":"okeps/okep-5296-bgp/#example-1-advertising-pod-ips-from-a-user-defined-network-over-bgp","title":"Example 1: Advertising pod IPs from a user-defined network over BGP","text":"<p>In this example a user wants to expose network blue outside their OCP cluster so that pod IPs are reachable on the external network. The admin has creates the following RouteAdvertisements CR for the blue tenant: <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default\nspec:\n  advertisements:\n    podNetwork: true\n  networkSelector:\n    matchLabels: \n      k8s.ovn.org/metadata.name: blue\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/hostname: ovn-worker\n  frrConfigurationSelector:\n    matchLabels:\n      routeAdvertisements: default\n</code></pre></p> <p>ovnkube-controller will now see it needs to generate corresponding FRRConfiguration:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: route-advertisements-blue\n  namespace: metallb-system\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      vrf: blue\n      prefixes:\n        - 10.0.1.0/24\n    - asn: 64512\n      neighbors:\n         - address: 172.18.0.5\n           asn: 64512\n           toAdvertise:\n              allowed:\n                 prefixes:\n                    - 10.0.1.0/24\n  raw:\n    rawConfig: |-\n       router bgp 64512\n        address-family ipv4 unicast\n          import vrf blue\n          exit-address-family\n       router bgp 64512 vrf blue    \n        address-family ipv4 unicast\n          import vrf default\n          exit-address-family      \n  nodeSelector:\n     matchLabels:\n        kubernetes.io/hostname: ovn-worker\n</code></pre> <p>In the above configuration generated by OVN-Kubernetes, the subnet 10.0.1.0/24 which belongs to VRF blue, is being imported into the default VRF, and advertised to the 172.18.0.5 neighbor. This is because the targetVRF was defaulted so the routes are leaked and advertised in the default VRF. Additionally, routes are being imported from the default VRF into the blue VRF. At the time of this writing, FRR-K8S does not support importing vrf routes as an API, and thus rawConfig is used. However, when implementing this enhancement every attempt should be made to add support into FRR-K8S to use its API rather than using rawConfig.</p>"},{"location":"okeps/okep-5296-bgp/#example-2-vrf-lite-advertising-pod-ips-from-a-user-defined-network-over-bgp-with-vpn","title":"Example 2: VRF Lite - Advertising pod IPs from a user-defined network over BGP with VPN","text":"<p>In this example, the user provisions a VLAN interface, enslaved to the VRF, which carries the blue network in isolation to the external PE router. This provides a VRF-Lite design where FRR-K8S is going to be leveraged to advertise the blue network only over the corresponding VRF/VLAN link to the next hop PE router. The same is done for the red tenant. Here the user has created an additional FRRConfiguration CR to peer with the PE router on the blue and red VLANs:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: vpn-ovn-worker\n  namespace: metallb-system\n  labels:\n    routeAdvertisements: vpn-blue-red\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      vrf: blue\n      neighbors:\n      - address: 182.18.0.5\n        asn: 64512\n        disableMP: false\n        holdTime: 1m30s\n        keepaliveTime: 30s\n        passwordSecret: {}\n        port: 179\n        toAdvertise:\n          allowed:\n            mode: filtered\n        toReceive:\n          allowed:\n            mode: filtered\n    - asn: 64512\n      vrf: red\n      neighbors:\n         - address: 192.18.0.5\n           asn: 64512\n           disableMP: false\n           holdTime: 1m30s\n           keepaliveTime: 30s\n           passwordSecret: {}\n           port: 179\n           toAdvertise:\n              allowed:\n                 mode: filtered\n           toReceive:\n              allowed:\n                 mode: filtered\n</code></pre> <p>The admin now creates the following RouteAdvertisements CR: <pre><code>apiVersion: k8s.ovn.org/v1\nkind: RouteAdvertisements\nmetadata:\n  name: default\nspec:\n  targetVRF: auto \n  advertisements:\n    podNetwork: true\n  networkSelector:\n    matchExpressions:\n      - { key: k8s.ovn.org/metadata.name, operator: In, values: [blue,red] } \n  nodeSelector:\n    matchLabels:\n      kubernetes.io/hostname: ovn-worker\n  frrConfigurationSelector:\n    matchLabels:\n      routeAdvertisements: vpn-blue-red\n</code></pre></p> <p>In the above CR, the targetVRF is set to auto, meaning the advertisements will occur within the VRF corresponding to the individual networks selected. In this case, the pod subnet for blue will be advertised over the blue VRF, while the pod subnet for red will be advertised over the red VRF. OVN-Kubernetes creates the following FRRConfiguration:</p> <pre><code>apiVersion: frrk8s.metallb.io/v1beta1\nkind: FRRConfiguration\nmetadata:\n  name: route-advertisements-blue\n  namespace: metallb-system\nspec:\n  bgp:\n    routers:\n    - asn: 64512\n      neighbors:\n      - address: 182.18.0.5\n        asn: 64512\n        toAdvertise:\n          allowed:\n            prefixes:\n               - 10.0.1.0/24\n      vrf: blue\n      prefixes:\n        - 10.0.1.0/24\n    - asn: 64512\n      neighbors:\n      - address: 192.18.0.5\n        asn: 64512\n        toAdvertise:\n          allowed:\n            prefixes:\n            - 10.0.1.0/24\n      vrf: red\n      prefixes:\n         - 10.0.1.0/24  \n  nodeSelector:\n     matchLabels:\n        kubernetes.io/hostname: ovn-worker\n</code></pre> <p>OVN-Kubernetes uses the configuration already in place in FRR and the desired RouteAdvertisements to generate the above FRRConfiguration. For filtering or choosing what routes to receive, a user should do that in the FRRConfiguration they declare for peering.</p> <p>Note, VRF-Lite is only available when using Local Gateway Mode in OVN-Kubernetes.</p>"},{"location":"okeps/okep-5296-bgp/#preserving-udn-isolation","title":"Preserving UDN Isolation","text":"<p>By definition, UDNs are expected to provide network isolation. With the ability to advertise UDNs within the same BGP VRF, it opens the door for users on one UDN to try to access another UDN via external routing. This may be undesirable for users who are simply exposing their UDN onto a VRF to allow direct ingress to those pods from outside the cluster, and not necessarily wanting to allow UDNs to communicate. Therefore by default, access between externally routed UDNs will be blocked by ACLs internal to OVN. This will protect users from accidentally exposing their UDNs to other tenants.</p> <p>In the future, an option will be added to allow a cluster admin to specify that UDNs exposed via BGP may be externally routed between each other. Note, this security limitation only applies to pod and cluster IP service addresses . If a group of pods is exposed externally as service (LoadBalancer, NodePort, External IP), then external routing from another UDN will succeed. This is by design as the user has explicitly opened their service up to the outside world.</p> <p>Additionally, when Kubernetes nodes are directly peered with each other and not an external provider BGP router, remote node UDN subnets are not imported in order to prevent issues with isolation, as well as issues with accidentally routing via the underlay for a UDN pod subnet.</p>"},{"location":"okeps/okep-5296-bgp/#bgp-route-advertisements","title":"BGP Route Advertisements","text":""},{"location":"okeps/okep-5296-bgp/#layer-3-topology","title":"Layer 3 Topology","text":"<p>With a Layer 3 network type, each BGP node should advertise the node subnet assigned to it. Since the cluster supernet is divided into per node subnets, it makes sense to advertise those from each node. This will result in direct routing to the node that hosts destination pod. In the future when nodeSelector is supported, all BGP speakers will also need to advertise the cluster supernet. This will allow ECMP routes via any BGP enabled node for reachability to nodes not participating in BGP.</p>"},{"location":"okeps/okep-5296-bgp/#layer-2-topology","title":"Layer 2 Topology","text":"<p>Layer 2 networks are more complicated than Layer 3 due to the nature of a single subnet spanning multiple nodes. In other words the cluster supernet is not subnetted out on a per-node basis. Therefore, every node must advertise the cluster supernet. This results in asynchronous routing, and in practical terms, packets having to endure an extra hop before making it to their destination node. This can result in reply traffic hitting different nodes, and thus conntrack is unreliable. With OVN, Logical Switches will by default drop packets that are CT_INVALID (Logical Routers do not behave this way) and thus the OVN will need to be configured to not drop CT_INVALID packets by setting:</p> <p><pre><code>ovn-nbctl set NB_Global . options:use_ct_inv_match=false\n</code></pre> This change is only required in Local Gateway Mode.</p> <p>In the future, routing optimizations will be implemented which break up contiguous address blocks of the supernet  to each node for assignment. This will be similar to how Layer 3 subnetting works, except there will not be individual subnets, but ranges instead. With this design, each node can advertise their contiguous range as well as the supernet. For example with a supernet of 10.244.0.0/16, a node A may be assigned a block of 256 addresses (10.244.0.0/24). In this case node A would advertise both 10.244.0.0/16 and 10.244.0.0.0/24 routes. This will result in direct routing for pods.</p> <p>Additionally, VMs may migrate on Layer 2 networks. In, which case the VM pod on node A with an address of 10.244.0.5 may migrate to node B. With the existing route advertisement scheme, traffic destined to this VM would first route towards node A and then to node B. To resolve this, when a VM migrates to a node, a /32 route will be advertised from the target node. So node B would advertise (in order of longest prefix):  - 10.244.0.5/32  - 10.244.1.0/24  - 10.244.0.0/16</p> <p>As a VM migrates back to another node or back to its original node, the /32 route will be removed.</p>"},{"location":"okeps/okep-5296-bgp/#egress-ip","title":"Egress IP","text":"<p>Egress IP will be advertised at the node where the Egress IP is assigned as a /32 route. If the Egress IP migrates to a different node, then the new node will be responsible for advertising the Egress IP route. Egress IPs advertised via BGP must share the same nodeSelector in the Egress IP resource and the RouteAdvertisement resource to function correctly.</p>"},{"location":"okeps/okep-5296-bgp/#feature-compatibility","title":"Feature Compatibility","text":""},{"location":"okeps/okep-5296-bgp/#multiple-external-gateways-meg","title":"Multiple External Gateways (MEG)","text":"<p>When using BGP to learn routes to next hops, there can be overlap with gateways detected by the MEG feature. MEG may still be configured along with BFD in OVN, and overlapping routes learned from BGP will be ignored by OVN-Kubernetes. BFD may also be configured in FRR, for immediate purging of learned routes by FRR.</p> <p>A user can also configure RouteAdvertisements for namespaces affected by MEG. Since MEG directly uses pod IPs (in disable-snat-multiple-gws mode), the external gateway needs to know where to route pod IPs for ingress and egress reply traffic. Traditionally this is done by the gateway having its own operator to detect pod subnets via kubernetes API. With BGP, this is no longer necessary. A user can simply configure RouteAdvertisements, and the pod subnet routes will be dynamically learned by an external gateway capable of BGP peering.</p>"},{"location":"okeps/okep-5296-bgp/#egress-ip_1","title":"Egress IP","text":"<p>EgressIP feature that is dynamically moved between nodes. By enabling the feature in RouteAdvertisements, OVN-Kubernetes will automatically change FRR-K8S configuration so that the node where an egress IP resides will advertise the IP. This eliminates the need for nodes to be on the same layer 2 segment, as we no longer have to rely on gratuitous ARP (GARP).</p>"},{"location":"okeps/okep-5296-bgp/#services","title":"Services","text":"<p>MetalLB will still be used in order to advertise services across the BGP fabric.</p>"},{"location":"okeps/okep-5296-bgp/#egress-service","title":"Egress Service","text":"<p>Full support.</p>"},{"location":"okeps/okep-5296-bgp/#egress-firewall","title":"Egress Firewall","text":"<p>Full support.</p>"},{"location":"okeps/okep-5296-bgp/#egress-qos","title":"Egress QoS","text":"<p>Full Support.</p>"},{"location":"okeps/okep-5296-bgp/#network-policyanp","title":"Network Policy/ANP","text":"<p>Full Support.</p>"},{"location":"okeps/okep-5296-bgp/#direct-pod-ingress","title":"Direct Pod Ingress","text":"<p>Direct pod ingress is already enabled for the default cluster network. With direct pod ingress, any external entity can talk to a pod IP if it sends the packet to the node where the pod lives. Previously, support for direct pod ingress on user-defined networks was not supported. RouteAdvertisements with this enhancement may select user-defined networks and enable pod network advertisements. Therefore, it only makes sense to also accept direct pod ingress for these user-defined networks as well, especially since we know the selected subnets will not overlap in IP address space.</p>"},{"location":"okeps/okep-5296-bgp/#ingress-via-ovs-bridge","title":"Ingress via OVS bridge","text":"<p>Modifications will need to be made so that for packets arriving on br-ex, flows are added to steer the selected pod subnets to the right OVN GR patch port. Today the flow for the default cluster network looks like this:</p> <pre><code>[root@ovn-worker ~]# ovs-ofctl show breth0\nOFPT_FEATURES_REPLY (xid=0x2): dpid:00000242ac120003\nn_tables:254, n_buffers:0\ncapabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP\nactions: output enqueue set_vlan_vid set_vlan_pcp strip_vlan mod_dl_src mod_dl_dst mod_nw_src mod_nw_dst mod_nw_tos mod_tp_src mod_tp_dst\n 1(eth0): addr:02:42:ac:12:00:03\n     config:     0\n     state:      0\n     current:    10GB-FD COPPER\n     speed: 10000 Mbps now, 0 Mbps max\n 2(patch-breth0_ov): addr:ca:2c:82:e9:06:42\n     config:     0\n     state:      0\n     speed: 0 Mbps now, 0 Mbps max\n\n\n[root@ovn-worker ~]# ovs-ofctl dump-flows breth0 |grep 10.244\n cookie=0xdeff105, duration=437.393s, table=0, n_packets=4, n_bytes=392, idle_age=228, priority=109,ip,in_port=2,dl_src=02:42:ac:12:00:03,nw_src=10.244.1.0/24 actions=ct(commit,zone=64000,exec(load:0x1-&gt;NXM_NX_CT_MARK[])),output:1\n cookie=0xdeff105, duration=437.393s, table=0, n_packets=0, n_bytes=0, idle_age=437, priority=104,ip,in_port=2,nw_src=10.244.0.0/16 actions=drop\n cookie=0xdeff105, duration=437.393s, table=1, n_packets=4, n_bytes=392, idle_age=228, priority=15,ip,nw_dst=10.244.0.0/16 actions=output:2\n</code></pre> <p>Packets matching the pod IP are forwarded directly to OVN, where they are forwarded to the pod without SNAT. Additional flows will need to be created for additional networks. For shared gateway mode, the reply packet will always return via OVS and follow a symmetrical path. This is also true if local gateway mode is being used in combination with Multiple External Gateways (MEG). However, if local gateway mode is used without MEG, then the reply packet will be forwarded into the kernel networking stack, where it will routed out br-ex via the kernel routing table.</p>"},{"location":"okeps/okep-5296-bgp/#ingress-via-a-secondary-nic","title":"Ingress via a Secondary NIC","text":"<p>If packets enter into the host via a NIC that is not attached to OVS, the kernel networking stack will forward it into the proper VRF, where it will be forwarded via ovn-k8s-mp0 of the respective user-defined network. Today when these packets ingress ovn-k8s-mp0 into OVN, they are SNAT'ed to the ovn-k8s-mp0 address (.2 address). Reasons for this include: 1. To ensure incoming traffic that may be routed to another node via geneve would return back to this node. This is undesirable    and unnecessary for networks where only their respective per-node pod subnet is being advertised to the BGP fabric. 2. If running shared gateway mode, the reply packet would be routed via br-ex with the default gateway configured in    ovn_cluster_router.</p> <p>For local gateway mode, changes will need to be made to skip the SNAT as it is not necessary and provides pods with the true source IP of the sender.</p> <p>Additionally, modifications will need to be made in the kernel routing table to leak routes to the pod subnets from each user-defined VRF into the default VRF routing table.</p>"},{"location":"okeps/okep-5296-bgp/#testing-details","title":"Testing Details","text":"<ul> <li>E2E upstream with a framework (potentially containerlab.dev to simulate a routed spine and leaf   topology with integration using OVN-Kubernetes.</li> <li>Testing using transport none for some networks, and Geneve for non-BGP enabled networks.</li> <li>Testing to cover BGP functionality including MEG, Egress IP, Egress QoS, etc.</li> <li>Scale testing to determine impact of FRR-K8S footprint on large scale deployments.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#documentation-details","title":"Documentation Details","text":"<ul> <li>ovn-kubernetes.io will be updated with a BGP user guide</li> <li>Additional dev guides will be added to the repo to show how the internal design of BGP is implemented.</li> </ul>"},{"location":"okeps/okep-5296-bgp/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<p>Using BGP, especially for east/west transport relies on the user's BGP network. Therefore, BGP control plane outages in a user's network may impact cluster networking. Furthermore, when changing a network's transport there will be some amount of traffic disruption. Finally, this entire feature will depend on correct BGP peering via FRR-K8S configuration, as well as settings to match the proper communities, route filtering, and other BGP configuration within the provider's BGP network.</p> <p>In addition, by dynamically learning routes and programming them into OVN, we risk a customer accidentally introducing potentially hundreds or even thousands of routes into different OVN logical routers. As a mitigation, we can recommend using communities or route aggregation to customers to limit RIB size. We will also need to scale test the effect of many routes inside OVN logical routers.</p> <p>Another risk is integration with MetalLB. We need to ensure that MetalLB is still able to function correctly with whatever OVN-Kubernetes is configuring within FRR.</p> <p>BGP has a wide range of configurations and configuration options that are supported by FRR today. There is a big risk of scope creep to try to support all of these options during the initial development phase. During the initial release the number of options supported will be limited to mitigate this issue, and the main focus will be on stability of a default environment without enabling these extra options/features.</p> <p>Reliance on FRR is another minor risk, with no presence from the OVN-Kubernetes team involved in that project.</p>"},{"location":"okeps/okep-5296-bgp/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":"<p>BGP will be delivered in version 1.1.0.</p>"},{"location":"okeps/okep-5296-bgp/#alternatives","title":"Alternatives","text":"<p>None</p>"},{"location":"okeps/okep-5296-bgp/#references","title":"References","text":"<p>None</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/","title":"OKEP-5494: Model Context Protocol for Troubleshooting OVN-Kubernetes","text":""},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#problem-statement","title":"Problem Statement","text":"<p>Diagnosing an issue in OVN-Kubernetes network plugin is complex because it has many layers (Kubernetes, OVN-Kubernetes, OVN, OpenvSwitch, Kernel - especially Netfilter elements). Usually the person troubleshooting an issue has to approach it in a layered fashion and has to be fully aware of all the debugging tools each layer has to offer to then be able to pin point where the problem is. That is time consuming and requires years of expertise on each depth of the stack and across all features. Sometimes it also involves working with the layered community project teams like OVN and OVS since they hold more knowledge about their domain than engineers working on the OVN-Kubernetes plugin. Each troubleshooting session to understand where the packet is getting blackholed or dropped takes a lot of time to solve (unless it's trivial).</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#goals","title":"Goals","text":"<p>The goal of this enhancement is to try to improve \"troubleshooting time\", \"tool usage/typing time\" and \"erase the need to know how to use each of those tools to a parameter level detail\" by exposing these tools using Model Context Protocol (MCP) and leveraging the backend Model's (Claude Sonnet4, Gemini2.5Pro, GPT-5, etc) knowledge and context to troubleshoot issues on live clusters or locally on containers with end user's databases loaded. This can go a long way in speeding up bug triages.</p> <ul> <li>Phase1 targets only ovn-kubernetes community members as target audience</li> <li>Build an OVN-Kubernetes MCP Server(s) that exposes all the tools required   to troubleshoot OVN-Kubernetes network plugin<ul> <li>This MCP Server(s) must also be aware of where to run these tools to   troubleshoot the issue in question on a cluster (i.e Which node? Which   pod? Which container?)</li> </ul> </li> <li>Add support to load a MCP Server against not just a real-time cluster   but also against a simulated environment constructed from information bundle   gathered or other debugging information extracted from a live cluster.   Examples are must-gather and   sos-reports and how tools like   omc or xsos   can be leveraged</li> <li>Ensure that the tools we expose have read only permissions. Whether this means   we restrict the tools itself with read rights only OR expose only read actions   via the tools is to be discussed in proposal section.</li> <li>Support troubleshooting all features for OVN-Kubernetes<ul> <li>We must have extensive failure scenario and chaos tests injected to see   how good LLM is able to troubleshoot</li> <li>We must form a list of all common ovn-kubernetes bugs we have hit across   features (example all possible ways of running into staleSNATs)</li> <li>We must add benchmarking tests for evaluating the quality of troubleshooting   specific to CNI Networking.</li> </ul> </li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#future-goals","title":"Future Goals","text":"<ul> <li>Phase2 targets end-users and others using OVN-Kubernetes to troubleshoot   issues</li> <li>Expand the MCP server to not just provide tools but also relevant context   around OVN-Kubernetes implementation</li> <li>Creating a RAGing system with all internal docs and integrating that with   the prompt client for better results (would require improving our docs   first). This is especially important for the LLM to know how OVN-Kubernetes   implements each traffic flow and feature and what constructs are created   underneath into the layers. This might not be needed for older features   like EgressIPs that has plenty of online resources but for newer features   like RouteAdvertisements, the LLM may not know much without providing that   extra context.</li> <li>Investigate if there is potential for converting it also into a remediation   system (this would need write access)</li> <li>Work with the Layered project community teams (OVN, OpenvSwitch, Kernel)   for each of those layers to also own an MCP server since they know their   stack better. So instead of 1 OVN-Kubernetes MCP Server it would be a   bunch of servers each owned by specific upstream community projects for   better maintenance and ownership</li> <li>The same MCP Server could also potentially be used as a   \"OVN-Kubernetes network plugin - know it better\" chatbot but that is not   the initial goal here.</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#future-stretch-goals","title":"Future-Stretch-Goals","text":"<ul> <li>Phase3 includes this getting productized and shipped to end-users using   OVN-Kubernetes in some far fetched future to run it on production to   troubleshoot. But this would require:<ul> <li>Having a better overall architecture for the wholesome agentic AI   troubleshooting solution on a cluster</li> <li>Solving the compute problem of how and where to run the model</li> <li>Having an air tight security vetting.</li> </ul> </li> </ul> <p>Running this stack in production is out-of-scope for this OKEP; it is a   future-stretch goal contingent on security and testing milestones.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#non-goals","title":"Non-Goals","text":"<ul> <li>We will not be solving the problem around which LLM should be used. Most of   the good ones (based on community member experience) were proprietary (claude sonnet4   and gemini2.5pro) but testing all LLMs and coming up with which works the   best is not in scope<ul> <li>By not solving this problem as part of this enhancement, we risk having   to deal with \"long-context windows causing hallucinations\" but that's   where RAGing mentioned in future goals could help.</li> </ul> </li> <li>We will also not be developing our own model to teach and maintain it<ul> <li>By not solving this problem, we also risk relying on proprietary models   knowing and learning what we want them to learn but having no control on   how fast they learn it. Again RAGing could help here.</li> <li>So the quality here will heavily depend on how good the brain LLM is   which basically won't be in our control much.</li> </ul> </li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#introduction","title":"Introduction","text":"<p>An engineer troubleshooting OVN-Kubernetes usually uses the following set of CLI tools in a layered fashion:</p> <ul> <li>Kubernetes and OVN-Kubernetes layer - this cluster state information is   gathered as part of must-gather for offline troubleshooting<ul> <li>kubectl commands like list, get, describe, logs, exec, events to know   everything about the Kubernetes API state of the feature and to know what   the ovnkube pods were doing through the logs they generate during that   window</li> <li>ovnkube-trace which executes ovn-trace and ovs ofproto trace and   detrace commands (this tool doesn't support all scenarios - it's not   maintained well)</li> <li>Future-tool - ovnkube CLI (need to ask NVIDIA what's the status here)<ul> <li>A place for K8s state-database syncer should/could potentially exist</li> </ul> </li> </ul> </li> <li>OVN Layer - OVN databases are gathered as part of must-gather for   offline troubleshooting<ul> <li>ovn-nbctl commands that are executed on the ovnkube node pods to   understand what OVN-Kubernetes created into northbound database via   libovsdbclient transactions</li> <li>ovn-sbctl commands that are executed on the ovnkube node pods to   understand what northd created into southbound database</li> <li>ovn-trace and detrace commands that are executed on the ovnkube node   pods to understand simulated packet flow tracing based on flows in   southbound database</li> <li>ovn-appctl -t ovn-controller ct-zone-list to list all the conntrack   zone to OVN construct mapping for better understanding how the conntrack   commit of the packets happened</li> </ul> </li> <li>OpenvSwitch Layer - openvswitch database is usually gathered as part   of sos-report for offline troubleshooting<ul> <li>ovs-ofctl dump-flows to debug specially the breth0 openflows   that OVN-Kubernetes creates on the gateway of each node</li> <li>ovs-appctl dpctl/dump-flows to trace live packets run on a specific   node's ovs container (KIND) or on the node (OpenShift)</li> <li>ovs-appctl ofproto/trace and detrace to run an ovs trace of the   packet based on the openflows</li> <li>ovs-appctl dpctl/dump-conntrack to know all the conntrack zones used   for a specific connection</li> <li>ovs-vsctl commands to list interfaces and bridges</li> <li>retis to see packet drops in ovs</li> </ul> </li> <li>Netfilter/Kernel Layer - this information is usually gathered as part   of sos-report for offline troubleshooting<ul> <li>ip util commands like ip r or ip a or ip rule list for   debugging VRFs, BGP learnt routes or the routes OVN-Kubernetes creates   on the node or custom routes that end user's add</li> <li>nft list ruleset to understand what rules were created by   OVN-Kubernetes specially in routingViaHost=true gateway mode</li> <li>iptables-save to list all iptables (Given iptables is deprecated,   I think we can skip this tool though for now 50% of ovn-kubernetes is   still on IPT)</li> <li>conntrack -L or -E on the host itself</li> <li>ip xfrm policy and ip xfrm state when using IPSEC</li> </ul> </li> <li>TCPDUMP - external open source tools, can't be used for offline   troubleshooting<ul> <li>tcpdump is used for packet capture and analysis</li> <li>pwru is used to know the kernel drop reason</li> <li>libreswan <code>ipsec-stateus</code> and <code>ipsec-trafficstatus</code> commands</li> <li>frr frr router config and routes learnt by BGP</li> </ul> </li> </ul> <p>Ideally speaking, there are metrics and events that via alerts also go to the dashboard which is probably what most end-users use to troubleshoot. So when we do the phase3 we would need to reconsider this stack of troubleshooting entirely for including other aspects like OVN-Kubernetes troubleshooting dashboard that the observability team created or the various packet drop tools observability team already exposes. But for the scope of this enhancement, for now, we will consider these above set of tools as MVP.</p> <p>As we can see, that's a lot of tools! So remembering the syntax for each of these tools always and executing them one-by-one and gathering the information at each layer, analysing them, and then moving to the next layer takes time for a human. Always during a remote analysis of bug report the part that takes the longest is the RCA by combing through all the data - same goes for troubleshooting a cluster (which is slightly easier when we have access to the cluster than analysing offline data). The fix is usually the easiest part (there are exceptions).</p> <pre><code>                    OVN-Kubernetes Architecture &amp; Troubleshooting Tools\n                              (Per Node Components)\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           ovnkube-node pod                                 \u2502\n\u2502                                                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         kubectl exec/logs      \u2502\n\u2502  \u2502  ovnkube        \u2502\u25c4\u2500\u2500\u25ba\u2502      NBDB       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ovn-nbctl show/list    \u2502\n\u2502  \u2502  controller     \u2502    \u2502  (northbound)   \u2502                                \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n\u2502           \u2502                       \u2502                                        \u2502\n\u2502           \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n\u2502           \u2502              \u2502     northd      \u2502                               \u2502\n\u2502           \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u2502           \u2502                       \u2502                                        \u2502\n\u2502           \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502      SBDB       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ovn-sbctl show/list   \u2502\n\u2502                          \u2502  (southbound)   \u2502         ovn-trace/detrace     \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u2502                                   \u2502                                        \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n\u2502                          \u2502 ovn-controller  \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ovn-appctl ct-zone    \u2502\n\u2502                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502\n\u2502                                   \u2502                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502       OVS       \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ovs-vsctl list\n                          \u2502   (database)    \u2502         ovs-appctl dpctl/\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         ovs-ofctl dump-flows\n                                   \u2502                  retis (packet drops)\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502   OVS bridge    \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ovs-appctl ofproto/trace\n                          \u2502   br-int/breth0 \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                          \u2502      NIC        \u2502\n                          \u2502   (physical)    \u2502\n                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502         Host Network        \u2502\n                    \u2502                             \u2502\n                    \u2502  ip route/addr/rule \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ip commands\n                    \u2502  nft ruleset        \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 nft list ruleset\n                    \u2502  iptables rules     \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 iptables-save\n                    \u2502  conntrack zones    \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 conntrack -L/-E\n                    \u2502                             \u2502\n                    \u2502  Network interfaces \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tcpdump/pwru\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Problem: Engineers must know WHERE to run WHICH tool on WHICH component\n             to troubleshoot issues across this distributed architecture\n</code></pre> <p>This enhancement aims to solve this pain point of reducing the time taken to execute these tools and analyse these results using MCP Servers and LLMs.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#user-stories","title":"User Stories","text":"<p>As an OVN-Kubernetes developer, I want to troubleshoot my stack without needing to know every tool's parameter fields by-heart or by spending time looking it up each time I need to troubleshoot a feature so that I can spend my time efficiently. I just want to tell in plain english what I want and for the MCP server to execute those specific commands.</p> <p>As an OVN-Kubernetes engineer, I want to troubleshoot my stack without needing to analyse each flow output of these tools when I need to troubleshoot a feature so that I can spend my time efficiently. I just want to tell in plain english what I want and for the LLM to help me analyze the output of the commands executed by the MCP Server. I understand that I will need to verify the reasoning thoroughly before accepting the RCA from AI.</p> <p>As a new engineer joining the OVN-Kubernetes team, I want to retrieve specific information from different parts of the stack without having knowledge of the topology or tooling of the stack.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#proposed-solution","title":"Proposed Solution","text":"<p>We build a Golang MCP Server (could be split into a set of MCP Servers in the future) that exposes these tools in read only fashion and the LLM backend that has the required context will analyse the results of the execution and provide a response back to the prompter who has to verify it thoroughly. This MCP Server code will be in a new repo in ovn-kubernetes org called ovn-kubernetes-mcp.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#example-workflow-for-an-end-user","title":"Example Workflow for an end-user","text":"<ol> <li>An end-user can use any MCP Client to start a troubleshooting session via    prompting. The client connects with all the available servers (in our case    the OVN-Kubernetes MCP Server and maybe in the future all layered community    MCP Severs) and gathers their available tools and presents this information    to the LLM along with their schemas. Example: Using Cursor AI as your MCP    Client</li> <li>LLM will be able use its intelligence and analyze the end-user query and    choose the appropriate tools. Example: Using Claude Sonnet4 as your LLM    model</li> <li>MCP Client then receives the LLM's tool call and routes it to the    corresponding MCP Server which executes the tool and client then relays    the response back to the LLM</li> <li>LLM again uses its intelligence to analyze the responses</li> <li>LLM provides a RCA back to the end user</li> </ol> <p>The steps 2, 3 and 4 is repeated by the LLM and it intelligently does a step-by-step layered troubleshooting exercise to find the root cause.</p> <p>We may also include predefined and tested prompt steps in the documentation of our repo for helping end-users to give the LLM good context around OVN-Kubernetes and OVN and OVS. For example, provide the latest OVN man pages so that it has the current knowledge of OVN DB schemas and tools usage. Some standard preparation prompt can be maintained in the mcp-server repo as reference.</p> <pre><code>OVN-Kubernetes Troubleshooting MCP Architecture\n===============================================\n\nEngineer Query: \"Pod A can't reach Pod B on different nodes, consistent connection drops\"\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    MCP SERVERS (Layer-Specific Tools)                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Kubernetes/K8s     \u2502 \u2502     OVN Layer        \u2502 \u2502   OpenvSwitch        \u2502 \u2502   Netfilter/         \u2502 \u2502   TCPDUMP/Debug      \u2502\n\u2502    MCP Server        \u2502 \u2502    MCP Server        \u2502 \u2502   Layer MCP          \u2502 \u2502   Kernel MCP         \u2502 \u2502    MCP Server        \u2502\n\u2502                      \u2502 \u2502                      \u2502 \u2502   Server             \u2502 \u2502   Server             \u2502 \u2502                      \u2502\n\u2502 Tools:               \u2502 \u2502 Tools:               \u2502 \u2502                      \u2502 \u2502                      \u2502 \u2502 Tools:               \u2502\n\u2502 \u2022 kubectl_get        \u2502 \u2502 \u2022 ovn_nbctl_show     \u2502 \u2502 Tools:               \u2502 \u2502 Tools:               \u2502 \u2502 \u2022 tcpdump_capture    \u2502\n\u2502 \u2022 kubectl_describe   \u2502 \u2502 \u2022 ovn_nbctl_list     \u2502 \u2502 \u2022 ovs_ofctl_flows    \u2502 \u2502 \u2022 ip_route_show      \u2502 \u2502 \u2022 tcpdump_analyze    \u2502\n\u2502 \u2022 kubectl_logs       \u2502 \u2502 \u2022 ovn_sbctl_show     \u2502 \u2502 \u2022 ovs_appctl_dpctl   \u2502 \u2502 \u2022 ip_addr_show       \u2502 \u2502 \u2022 pwru_trace         \u2502\n\u2502 \u2022 kubectl_exec       \u2502 \u2502 \u2022 ovn_sbctl_list     \u2502 \u2502 \u2022 ovs_ofproto_trace  \u2502 \u2502 \u2022 ip_rule_show       \u2502 \u2502                      \u2502\n\u2502 \u2022 kubectl_events     \u2502 \u2502 \u2022 ovn_trace          \u2502 \u2502 \u2022 ovs_appctl_conntr  \u2502 \u2502 \u2022 nft_list_ruleset   \u2502 \u2502                      \u2502\n\u2502 \u2022 ovnkube_trace      \u2502 \u2502 \u2022 ovn_detrace        \u2502 \u2502 \u2022 ovs_vsctl_show     \u2502 \u2502 \u2022 iptables_save      \u2502 \u2502                      \u2502\n\u2502                      \u2502 \u2502 \u2022 ovn_controller_ct  \u2502 \u2502 \u2022 retis_capture      \u2502 \u2502 \u2022 conntrack_list     \u2502 \u2502                      \u2502\n\u2502                      \u2502 \u2502                      \u2502 \u2502                      \u2502 \u2502 \u2022 conntrack_events   \u2502 \u2502                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                                \u2502 All tools aggregated\n                                \u25bc\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           MCP CLIENT                                         \u2502\n\u2502                    (OVN-K8s Troubleshoot AI)                                 \u2502\n\u2502                                                                              \u2502\n\u2502 Unified Tool Interface:                                                      \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502 \u2502 Layer 1 (K8s):    kubectl_*, ovnkube_trace                               \u2502 \u2502\n\u2502 \u2502 Layer 2 (OVN):    ovn_nbctl_*, ovn_sbctl_*, ovn_trace_*                  \u2502 \u2502\n\u2502 \u2502 Layer 3 (OVS):    ovs_ofctl_*, ovs_appctl_*, ovs_vsctl_*, retis_*        \u2502 \u2502\n\u2502 \u2502 Layer 4 (Kernel): ip_*, nft_*, iptables_*, conntrack_*                   \u2502 \u2502\n\u2502 \u2502 Layer 5 (Debug):  tcpdump_*, pwru_*                                      \u2502 \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n                        \u2502 Present unified interface\n                        \u25bc\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        LLM (OVN-K8s Expert)                                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Doing multiple MCP Servers has clear advantages like each layer owning their toolset, independent development and maintenance, granular RBAC, reusability, one server can't affect the other. Given we would need to align with different layered communities later on for a fully supported set of servers from their side which might take several releases, for the phase1 here, we plan to build our own monolithic server that could then be split into multiple servers later on. A single unified server is simpler, faster to iterate on.</p> <p>So our current implementation design looks like this:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                              MCP Client                                     \u2502\n\u2502                        \"Debug pod connectivity issues\"                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                                 MCP Protocol\n                                       \u2502\n                                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        OVN-Kubernetes MCP Server                             \u2502\n\u2502                                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Kubernetes    \u2502  \u2502  Live Cluster   \u2502  \u2502      Offline Bundle          \u2502  \u2502\n\u2502  \u2502     Layer       \u2502  \u2502   Execution     \u2502  \u2502       Execution              \u2502  \u2502\n\u2502  \u2502                 \u2502  \u2502                 \u2502  \u2502                              \u2502  \u2502\n\u2502  \u2502 \u2022 kubectl get   \u2502  \u2502 kubectl exec    \u2502  \u2502 Offline artifacts parser     \u2502  \u2502\n\u2502  \u2502 \u2022 kubectl desc  \u2502  \u2502                 \u2502  \u2502 tools.                       \u2502  \u2502\n\u2502  \u2502 \u2022 kubectl logs  \u2502  \u2502 Direct API      \u2502  \u2502 Example: xsos and omc        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502                      Tool Categories                                 \u2502    \u2502\n\u2502  \u2502                                                                      \u2502    \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502    \u2502\n\u2502  \u2502  \u2502 OVN Layer   \u2502  \u2502 OVS Layer   \u2502  \u2502Kernel Layer  \u2502  \u2502External     \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502             \u2502  \u2502             \u2502  \u2502              \u2502  \u2502Tools        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 ovn-nbctl   \u2502  \u2502 ovs-ofctl   \u2502  \u2502 ip route     \u2502  \u2502 tcpdump     \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 ovn-sbctl   \u2502  \u2502 ovs-appctl  \u2502  \u2502 nft list     \u2502  \u2502 pwru        \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 ovn-trace   \u2502  \u2502 ovs-vsctl   \u2502  \u2502 conntrack    \u2502  \u2502 retis       \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 ovn-detrace \u2502  \u2502 ovs-dpctl   \u2502  \u2502 iptables-save\u2502  \u2502             \u2502 \u2502    \u2502\n\u2502  \u2502  \u2502 ovn-appctl  \u2502  \u2502 ovs-ofproto \u2502  \u2502              \u2502  \u2502             \u2502 \u2502    \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502                      Security &amp; RBAC                                 \u2502    \u2502\n\u2502  \u2502                                                                      \u2502    \u2502\n\u2502  \u2502 \u2022 ovn-troubleshooter ClusterRole (read-only)                         \u2502    \u2502\n\u2502  \u2502 \u2022 Command parameter validation                                       \u2502    \u2502\n\u2502  \u2502 \u2022 Node-specific targeting required                                   \u2502    \u2502\n\u2502  \u2502 \u2022 Write operations blocked                                           \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                       \u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502           \u2502           \u2502\n                           \u25bc           \u25bc           \u25bc\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502 Node1  \u2502  \u2502 Node2  \u2502  \u2502 NodeN  \u2502\n                      \u2502        \u2502  \u2502        \u2502  \u2502        \u2502\n                      \u2502ovnkube-\u2502  \u2502ovnkube-\u2502  \u2502ovnkube-\u2502\n                      \u2502node pod\u2502  \u2502node pod\u2502  \u2502node pod\u2502\n                      \u2502        \u2502  \u2502        \u2502  \u2502        \u2502\n                      \u2502 ovn-nb \u2502  \u2502 ovn-nb \u2502  \u2502 ovn-nb \u2502\n                      \u2502 ovn-sb \u2502  \u2502 ovn-sb \u2502  \u2502 ovn-sb \u2502\n                      \u2502 ovs    \u2502  \u2502 ovs    \u2502  \u2502 ovs    \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nData Flow Examples:\n\u251c\u2500 Live: kubectl exec ovnkube-node-xyz -c nb-ovsdb -- ovn-nbctl show\n\u251c\u2500 Live: kubectl debug node/worker-1 -- ovs-ofctl dump-flows br-int\n\u251c\u2500 Live: kubectl debug node/worker-1 -- ip route show table all\n\u251c\u2500 Offline: Parse must-gather/ovn-kubernetes/ovn-northbound.db\n\u251c\u2500 Offline: Parse sos-report/node-1/openvswitch/ovs-ofctl_dump-flows\n\u2514\u2500 Offline: Parse sos-report/node-1/networking/ip_route\n</code></pre> <p>Note: Some of the layers like Kubernetes and Offline Debugging have existing servers like kubernetes-mcp-server and mustgather-mcp-server can be re-used together with ovn-kubernetes-mcp server for holistic end user experience. However kubernetes-mcp-server exposes <code>kubectl-exec</code> which has security implications (although they also have a read-only mode where only read commands are exposed).</p> <p>Note2: Feeding container logs to LLM will make the context window full pretty fast. We need to investigate a method to ensure we are filtering out relevant logs to feed.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#implementation-details-of-the-ovn-kubernetes-mcp-server","title":"Implementation Details of the OVN-Kubernetes MCP Server","text":"<p>See the Alternatives section for other ideas that were discarded.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#chosen-approach-direct-cli-tool-exposure-idea1","title":"Chosen Approach: Direct CLI Tool Exposure (Idea1)","text":"<p>The initial implementation takes a pragmatic approach by directly exposing the existing CLI tools (<code>ovn-nbctl</code>, <code>ovn-sbctl</code>, <code>ovs-vsctl</code>, etc.) as MCP tools. While this approach may seem less elegant than creating higher-level wrapper abstractions to connect to the database (see discarded alternatives), it offers the fastest path to value for OVN-Kubernetes engineers who are already familiar with these tools but want to leverage LLM assistance for command construction and output analysis. The CLI-based approach is also optimal for exposing the complete troubleshooting toolkit across all layers of the stack. Most other discarded ideas only addressed OVSDB access and reusability concerns while failing to provide the holistic set of tools needed for comprehensive root cause analysis like running packet traces.</p> <p>The MCP server acts as a secure execution bridge, translating natural language troubleshooting requests into appropriate CLI commands.</p> <p>Advantages:</p> <ul> <li>Fastest Time to Value: Leverages existing tools that engineers already   know</li> <li>Zero Deployment Overhead: All required CLI binaries are already present   in the pods and nodes within the cluster</li> <li>Comprehensive Coverage: Only approach that provides access to the   complete troubleshooting toolkit across all stack layers</li> <li>Security Controls: Enables read-only access enforcement by exposing   only the tools with read-only access (get/list)</li> <li>No version compatibility issues between the MCP server tools and the   cluster's installed versions when running on live cluster environments</li> </ul> <p>Trade-offs:</p> <ul> <li>Limited Reusability: Somewhat specific to OVN-Kubernetes deployment   patterns and can't be reused in other layers like OVS</li> </ul> <p>This approach was selected as the optimal balance between security, functionality, and development effort.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#security-model-and-rbac-constraints","title":"Security Model and RBAC Constraints","text":"<p>No matter how we approach this implementation, it is impossible to totally secure the execution of networking troubleshooting tools at this stage. Most networking tools require privileged access to function properly, creating inherent security trade-offs. The goal is therefore to minimize blast radius through layered controls rather than achieve perfect isolation.</p> <p>Kubernetes Layer Security:</p> <ul> <li><code>kubectl exec</code> and <code>kubectl debug node</code> operations require cluster-admin level   privileges by design. Avoid exposing generic exec in the K8s layer.   Prefer direct Kubernetes API reads (<code>get</code>, <code>list</code>, <code>logs</code>, <code>events</code>).</li> <li>Alternative approach using custom debug containers   (<code>kubectl debug node --image=&lt;custom-image&gt;</code>) with volume mounts to database   files reduces some attack surface but remains intrusive</li> <li>Mitigation: Expose only Kubernetes API read operations (<code>get</code>, <code>list</code>,   <code>logs</code>, <code>events</code>); remove any generic exec tool in this layer.</li> <li>Constraint: MCP Server service account requires elevated privileges   (including <code>pods/exec</code>) despite being conceptually a \"troubleshooter\" role</li> </ul> <p>At first, for the kubernetes layer, we thought of leveraging the opensource kubernetes-mcp-server. So whatever security posture they use for now, can be adopted. They have a <code>read-only</code> mode and a mode where write can be done via kubectl exec. Later, after getting reviews, this enhancement has changed the approach and pivotted towards opting into a more secure approach of adding a tool that is a wrapper on top of <code>kubectl-exec</code> without directly exposing kubectl-exec. So instead of relying on <code>kubernetes-mcp-server</code>, our <code>ovn-kubernetes-mcp</code> will take the more secure approach of only using <code>kubectl_exec</code> as an implementation detail but not directly expose it. Downside of this is that we will need to also account for get or list resources command being duplicated into <code>ovn-kubernetes-mcp</code>. So we would need to implement the tools we need also on the kubernetes layer ourselves.</p> <p>OVN/OVS Database Layer Security:</p> <ul> <li>Unix socket-based database access prevents using SSL certificate-based   authentication and authorization</li> <li>Database connections inherit the security context of the container executing   the commands</li> <li>Mitigation: Command parameter validation to ensure only read-only   database operations (<code>show</code>, <code>list</code>, <code>dump</code>) while blocking modification   commands (<code>set</code>, <code>add</code>, <code>remove</code>)</li> <li>Long-term Path: Requires RBAC-enabled CLI execution even against local   unix socket.</li> </ul> <p>Host/Kernel Layer Security:</p> <ul> <li>Kernel-level networking tools (<code>ip</code>, <code>nft</code>, <code>conntrack</code>) inherently require   root system access</li> <li>Current tooling lacks granular RBAC capabilities - tools are typically   all-or-nothing from a privilege perspective</li> <li>External Tools Note: Tools like <code>tcpdump -i any</code> can be highly intrusive   as they capture all network traffic on the host, requiring careful   consideration of privacy and performance impact while being chosen for   execution.</li> <li>Short-term Mitigation: Strict command allowlisting exposing only read   operations (<code>ip route show</code>, <code>nft list ruleset</code>) while blocking modification   commands (<code>ip route add</code>, <code>nft add rule</code>)</li> <li>Long-term Path: Requires RBAC-enabled wrapper tools from upstream   layered community teams (example netfilter, kernel networking)</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#distributed-execution-context","title":"Distributed Execution Context","text":"<p>MCP Server will only be supported on interconnect mode.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#ovn-kubernetes-architecture-challenge","title":"OVN-Kubernetes Architecture Challenge","text":"<p>In OVN-Kubernetes interconnect architecture, each node maintains its own local instances of critical databases and services:</p> <ul> <li>Northbound Database: Local OVN northbound database per node</li> <li>Southbound Database: Local OVN southbound database per node</li> <li>OpenVSwitch Database: Node-specific OVS database and flow tables</li> <li>Host Networking: Node-specific routing tables, conntrack zones, and   kernel state</li> </ul> <p>This distributed architecture means that troubleshooting commands must be executed on the specific node where the relevant data resides.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#node-targeted-command-execution","title":"Node-Targeted Command Execution","text":"<p>Node Selection Strategy: All tools requiring node-specific data accept a <code>node_name</code> parameter as a required argument. The MCP server uses this parameter to:</p> <ol> <li>Pod Selection: Locate the appropriate <code>ovnkube-node</code> pod running on the    specified node using <code>pod.spec.nodeName</code> matching</li> <li>Container Targeting: Route OVN database commands to the correct    container within the ovnkube-node pod (e.g., <code>nb-ovsdb</code>, <code>sb-ovsdb</code>    containers)</li> <li>Execution Context: Execute host-level commands via    <code>kubectl debug node/&lt;node_name&gt; --image &lt;&gt;</code> for direct host access</li> </ol> <p>LLM Responsibility: The LLM must determine the appropriate target node(s) based on the troubleshooting context:</p> <ul> <li>Pod-specific issues: Use the node where the problematic pod is scheduled   (<code>kubectl get pod -o wide</code>)</li> <li>Network flow analysis: Target nodes along the packet path (source node,   destination node, gateway nodes)</li> <li>Cluster-wide analysis: Potentially execute commands across multiple   nodes for correlation</li> </ul> <p>We need to account for testing the LLM Responsibility side which is not something we can guarantee but something we are offloading to the LLM that we won't solve.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#deployment-strategy","title":"Deployment Strategy","text":""},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#flexible-deployment-modes","title":"Flexible Deployment Modes","text":"<p>The MCP server is designed for flexible deployment without requiring elaborate cluster infrastructure. Multiple deployment modes support different use cases and security requirements:</p> <p>CLI Tool Mode (Simplest):</p> <ul> <li>Run the MCP server binary directly on a machine with <code>kubectl</code> access   to the target cluster</li> <li>Server uses existing cluster credentials and executes commands via standard   CLI tools</li> <li>Suitable for both live cluster troubleshooting</li> <li>Offline data based troubleshooting analysis would still need corresponding   parser tools to be run locally where the debug artifact files are hosted</li> <li>No cluster deployment required - operates entirely through external API   access</li> </ul> <p>Debug Container Mode:</p> <ul> <li>Package the MCP server as a container image that the LLM can select for   <code>kubectl debug node --image=&lt;mcp-server-image&gt;</code> operations</li> <li>This custom debug image contains the MCP server binary along with all   necessary troubleshooting tools</li> <li>Reduces blast radius compared to using default debug images with full host   access</li> <li>The LLM chooses this image when it needs to execute commands requiring   direct host access</li> </ul> <p>Future Considerations:</p> <ul> <li>More elaborate deployment patterns (DaemonSet, Deployment) can be considered   when we think of use cases beyond ovn-kubernetes developers</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#testing-strategy","title":"Testing Strategy","text":"<p>Unit Testing - MCP Server Tools:</p> <ul> <li>Straightforward validation of individual tool execution and parameter   handling. Use mcp-inspector.</li> <li>Mock cluster responses to test command routing and error handling</li> <li>Verify security controls and command allowlisting functionality</li> </ul> <p>Integration Testing - The Complex Challenge:</p> <ul> <li>Real Failure Scenario Reproduction: Design test scenarios based on past   bugs and commonly occurring incidents</li> <li>Chaos Engineering Integration: Implement controlled failure injection to   create realistic troubleshooting scenarios</li> <li>LLM Reasoning Validation: The most critical and challenging aspect -   verifying that the LLM can produce meaningful root cause analysis from tool   outputs</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#scenario-based-test-design","title":"Scenario-Based Test Design","text":"<p>Historical Incident Replay:</p> <ul> <li>Collect must-gather and sos-report bundles from past bugs</li> <li>Use these as offline test datasets to validate LLM troubleshooting accuracy</li> <li>Build regression test suite ensuring consistent analysis quality over time</li> </ul> <p>Synthetic Failure Scenarios:</p> <p>Some examples include: * Network policy and EgressIP misconfigurations * Pod connectivity failures across nodes * Gateway flow issues and routing problems * OVN database inconsistencies specially around EgressIPs</p> <p>LLM Capability Assessment:</p> <ul> <li>Measure accuracy of root cause identification - depends on how much   OVN-Kubernetes feature specific context it has</li> <li>Evaluate quality of troubleshooting step recommendations</li> <li>Test correlation of multi-layer data analysis</li> <li>Validate handling of incomplete or missing data scenarios</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#success-metrics","title":"Success Metrics","text":"<ul> <li>Accuracy: Percentage of correct root cause identifications in known   failure scenarios</li> <li>Completeness: Coverage of troubleshooting steps recommended vs. manual   expert analysis</li> <li>Efficiency: Time reduction compared to manual troubleshooting workflows</li> <li>Safety: Verification that only read-only operations are executed as   intended</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#documentation-details","title":"Documentation Details","text":"<p>OKEP for the MCP Server will be on https://ovn-kubernetes.io/ . All end-user documentation will be on the new repo's docs folder.</p> <ul> <li>Getting Started Guide: MCP client setup and initial configuration</li> <li>Troubleshooting Scenarios: Common use cases and example natural language   queries</li> <li>Tool Reference: Available tools and their capabilities across all stack   layers</li> <li>Security Model: Warning around security considerations</li> <li>Deployment Options: CLI mode vs. debug container mode setup instructions</li> <li>Offline Analysis: Must-gather and sos-report analysis workflows</li> </ul>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#alternative-implementation-ideas","title":"Alternative Implementation Ideas","text":""},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#idea0-using-existing-kubernetes-mcp-server-with-generic-bash-execution","title":"Idea0: Using Existing kubernetes-mcp-server with Generic Bash Execution","text":"<p>Approach: Use the existing kubernetes-mcp-server's <code>pods_exec</code> tool to run arbitrary bash commands like <code>ovn-nbctl show</code> or <code>ip route</code> directly through <code>kubectl exec</code> or <code>kubectl debug</code> sessions, without building any specialized tooling.</p> <p>Rationale: This approach would provide immediate access to all CLI tools without any development effort, leveraging the LLM's knowledge of command syntax to construct appropriate bash commands.</p> <p>Why Discarded:</p> <ul> <li>Security Risk: Allowing arbitrary bash command execution creates   significant security vulnerabilities. There's no protection against   destructive commands like <code>ovn-nbctl set</code> operations that could modify live   database state.</li> <li>Lack of Access Control: No way to enforce read-only operations or   validate command parameters before execution.</li> <li>Separation of Concerns: The LLM should focus on analysis and   troubleshooting logic, not on understanding the security implications of   direct system access.</li> <li>Blast Radius: Any compromise or LLM hallucination could potentially   execute dangerous commands on production systems.</li> </ul> <p>The fundamental principle that \"each layer knows best about how/what tools to allow with proper read access\" makes a controlled wrapper approach essential rather than direct bash execution.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#idea1-chosen-approach-direct-cli-tool-exposure","title":"Idea1: Chosen Approach: Direct CLI Tool Exposure","text":"<p>See the proposed solution section.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#idea2-libovsdb-client-golang-wrapper","title":"Idea2: libovsdb-client Golang Wrapper","text":"<p>Approach: Build a Golang MCP server using <code>NewOVSDBClient</code> to directly query OVSDB instances with proper RBAC controls implemented through OVSDB's native access control mechanisms.</p> <p>Advantages: * High Reusability: Could be shared across OVN, OVS, and OVN-Kubernetes   projects * Native RBAC: Leverages OVSDB's built-in role-based access controls * Structured Output: Returns structured data rather than CLI text parsing</p> <p>Why Discarded: * Deployment Model: Would require running as a DaemonSet on each node or   shipping binaries to ovnkube-node pods * Scope Limitation: Only addresses database access, missing ovn and ovs   flow trace simulation and host networking tools</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#idea3-ovsdb-client-binary-wrapper","title":"Idea3: ovsdb-client Binary Wrapper","text":"<p>Approach: Create a wrapper around the existing <code>ovsdb-client</code> binary (owned by the OVS team) to provide structured database access.</p> <p>Advantages: * High Reusability: Could be shared across OVN, OVS, and OVN-Kubernetes   projects * Native RBAC: Leverages OVSDB's built-in role-based access controls</p> <p>Why Discarded: * Ownership: We could argue this wrapper better belongs in the openvswitch   community then in OVN-Kubernetes org * Scope Limitation: Only addresses database access, missing ovn and ovs   flow trace simulation and host networking tools * In future once we reach out to OVN and OVS communities to see what   they plan to do, we could revisit it.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#idea4-direct-database-access-wrapper","title":"Idea4: Direct Database Access Wrapper","text":"<p>Approach: Build wrapper for direct database read operations bypassing CLI and client tools entirely.</p> <p>Advantages: * High Reusability: Could be shared across OVN, OVS, and OVN-Kubernetes   projects</p> <p>Why Discarded: Building from scratch when there's no real need to - existing CLI tools already provide all necessary functionality with proven reliability.</p>"},{"location":"okeps/okep-5494-ovn-kubernetes-mcp-server/#known-risks-and-limitations","title":"Known Risks and Limitations","text":"<ul> <li>AI! We can trust it only as much as we can throw it.<ul> <li>Quality of this troubleshooter depends on the LLM's intelligence</li> <li>Quality of the MCP Server itself is however in our own hands and can be   enhanced based on user experience</li> </ul> </li> <li>Security! We know that we cannot fully eliminate the risk</li> <li>Performance/Scalability: MCPs are a relatively new concept. So aspects like   how many tools could we expose per server and upto what point it scales etc   are unknowns. We will need to try and test it in our PoCs as we develop this.</li> <li>With sending bulky logs and debug details we also have danger of context     window running out. We need to ensure we filter out relevant logs.</li> <li>Token consumption and context window bloating.</li> <li>Other potential problems we need to rule out during testing:<ul> <li>Poor tool selection: LLMs struggle to choose the right tool from too many options</li> <li>Parameter hallucination: Agents invoke tools with incorrect or fabricated parameters</li> <li>Misinterpretation: Responses from tools are more likely to be misunderstood</li> <li>Attention spreading: The model's attention gets distributed thinly across many options</li> </ul> </li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/","title":"OKEP-5552: Dynamic UDN Node Allocation","text":"<ul> <li>Issue: #5552</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#problem-statement","title":"Problem Statement","text":"<p>When scaling UDNs, the control-plane cost of rendering a topology is high. This is the core limiting factor to being able to scale to 1000s of UDNs. While there are plans to also improve network controller performance with UDNs, there is still valuable savings to be had by not rendering UDNs on nodes where they are not needed.</p> <p>An example use case where this makes sense is when a Kubernetes cluster has its node resources segmented per tenant. In this case, it only makes sense to run the tenant network (UDN) on the nodes where a tenant is allowed to run pods. This allows for horizontal scaling to much higher number of overall UDNs running in a cluster.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#goals","title":"Goals","text":"<ul> <li>To dynamically allow the network to only be rendered on specific nodes.</li> <li>To increase overall scalability of the number UDNs in a Kubernetes cluster with this solution.</li> <li>To increase the efficiency of ovnkube operations on nodes where a UDN exists, but is not needed.</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#non-goals","title":"Non-Goals","text":"<ul> <li>To fully solve control plane performance issues with UDNs. There will be several other fixes done to address that    outside of this enhancement.</li> <li>To provide any type of network security guarantee about exposing UDNs to limited subset of nodes.</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#future-goals","title":"Future Goals","text":"<ul> <li>Potentially enabling this feature on a per UDN basis, rather than globally.</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#introduction","title":"Introduction","text":"<p>The purpose of this feature is to add a configuration knob that users can turn on which will only render UDNs on nodes where pods exist on that UDN. This feature will allow for higher overall UDN scale and less per-node control plane resource usage under conditions where clusters do not have pods on every node, with connections to all UDNs. For example, if I have 1000 UDNs and 500 nodes, if a particular node only has pods connected to say 200 of those UDNs, then my node is only responsible for rendering 200 UDNs instead of 1000 UDNs as it does today.</p> <p>This can provide significant control plane savings, but comes at a cost. Using the previous example, if a pod is now launched in UDN 201, the node will have to render UDN 201 before the pod can be wired. In other words, this introduces a one time larger pod latency cost for the first pod wired to the UDN. Additionally, there are more tradeoffs with other feature limitations outlined later in this document.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>Story 1: Segment groups of nodes per tenant</p> <p>As a cluster admin, I plan to dedicate groups of nodes to either a single tenant or small group of tenants. I plan to create a CUDN per tenant, which means my network will only really need to exist on this group of nodes. I would like to be able to limit this network to only be rendered on that subset of nodes. This way I will be able to have less resource overhead from OVN-Kubernetes on each node, and be able to scale to a higher number of UDNs in my cluster.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#proposed-solution","title":"Proposed Solution","text":"<p>The proposed solution is to add a configuration knob to OVN-Kubernetes, \"--dynamic-udn-allocation\", which will enable this feature. Once enabled, NADs derived from CUDNs and UDNs will only be rendered on nodes where there is a pod scheduled in that respective network. Additionally, if the node is scheduled as an Egress IP Node for a UDN, this node will also render the UDN.</p> <p>When the last pod on the network is deleted from a node, OVNK will not immediately tear down the UDN. Instead, OVNK will rely on a dead timer to expire to conclude that this UDN is no longer in use and may be removed. This timer will also be configurable in OVN-Kubernetes as \"--udn-deletion-grace-period\".</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#api-details","title":"API Details","text":"<p>There will be no API changes. There will be new status conditions introduced in the section below.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#implementation-details","title":"Implementation Details","text":"<p>In OVN-Kubernetes we have three main controllers that handle rendering of networking features for UDNs. They exist as  - Cluster Manager - runs on the control-plane, handles cluster-wide allocation, rendering of CUDN/UDNs  - Controller Manager - runs on a per-zone basis, handles configuring OVN for all networking features  - Node Controller Manager - runs on a per-node basis, handles configuring node specific things like nftables, VRFs, etc.</p> <p>With this change, Cluster Manger will be largely untouched, while Controller Manager and Node Controller Manager will be modified in a few places to filter out rendering UDNs when a pod doesn't exist.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#internal-controller-details","title":"Internal Controller Details","text":"<p>In OVN-Kubernetes we have many controllers that handle features for different networks, encompassed under three controller manager containers. The breakdown of how these will be modified is outlined below:</p> <ul> <li>Cluster Manager</li> <li>UDN Controller \u2014 No change</li> <li>Route Advertisements Controller \u2014 No change</li> <li>Egress Service Cluster \u2014 Doesn't support UDN</li> <li>Endpoint Mirror Controller \u2014 No change</li> <li>EgressIP Controller \u2014 No change</li> <li>Unidling Controller \u2014 No change</li> <li>DNS Resolver \u2014 No change</li> <li>Network Cluster Controller \u2014 Modified to report status and exclude nodes not serving the UDN</li> <li>Controller Manager (ovnkube-controller)</li> <li>Default Network \u2014 No change</li> <li>NAD Controller \u2014 Ignore NADs for UDNs that are not active on this node (no pods for the UDN and not an EIP node)</li> <li>Node Controller Manager</li> <li>Default Network \u2014 No change</li> <li>NAD Controller \u2014 Ignore NADs for UDNs that are not active on this node (no pods for the UDN and not an EIP node)</li> </ul> <p>The resulting NAD Controller change will filter out NADs that do not apply to this node, stopping NAD keys from being enqueued to the Controller Manager/Node Controller Manager's Network Manager. Those Controller Managers will not need to create or run any sub-controllers for nodes that do not have the network. To do this cleanly, NAD Controller will be modified to hold a filterFunc field, which the respective controller manager can set in order to filter out NADs. For Cluster Manager, this function will not apply, but for Controller Manager and Node Controller Manager it will be a function that filters based on if the UDN is serving pods on this node.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#new-podegressip-tracker-controller","title":"New Pod/EgressIP Tracker Controller","text":"<p>In order to know whether the Managers should filter out a UDN, a pod controller and egress IP controller will be used in the Managers to track this information in memory. The pod controller will be a new level driven controller for each manager. For Egress IP, another new controller will be introduced that watches EgressIPs, Namespaces, and NADs in order to track which NAD maps to a node serving Egress IP.</p> <p>When Managers are created, they will start these Pod/EgressIP Tracker Controllers, and set a filterFunc on NAD Controller. The filterFunc will query the aforementioned controllers to determine if the NAD being synced matches the local node. If not, then NADController will not create the UDN controller for that network.</p> <p>Additionally, the Pod/EgressIP Tracker Controllers will expose a callback function, called \"onNetworkRefChange\". When the first pod is detected as coming up on a node + NAD combination, or the node activates as an Egress IP node for the first time, onNetworkRefChange will be triggered, which allows a callback mechanism to be leveraged for events. The Controller Manager and Node Controller Manager will leverage this callback, so that they can trigger NAD Controller to reconcile the NAD for these events. This is important as it provides a way to signal that NADController should remove a UDN controller if it is no longer active, or alternatively, force the NAD Controller to reconcile a UDN Controller if for example, a new remote node has activated.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#other-controller-changes","title":"Other Controller Changes","text":"<p>The Layer3 network controller will need to filter out nodes where the UDN is not rendered. Upon receiving events, they will query a Manager function called NodeHasNAD. Managers will export a Tracker interface, that only contains this method for UDN Controllers to query. The implementation of NodeHasNAD will rely on the Manager querying their pod and egress IP trackers.</p> <p>Upon UDN activation of a remote node, these controllers will need to receive events in order to reconcile the new remote node.  To do this, the corresponding tracker will trigger its callback, \"onNetworkRefChange\". That will trigger the Manager to ask NAD Controller to reconcile the UDN controller belonging to this NAD. Once that Layer 3 UDN controller reconciles, it will walk nodes and determine what needs to be added or removed. It will take the applicable nodes, set their syncZoneICFailed status, then immediately queue the objects to the retry framework with no backoff. This will allow the Zone IC (ZIC) controller to properly configure the transit switch with the remote peers, or tear it down, if necessary.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#status-condition-and-metric-changes","title":"Status Condition and Metric Changes","text":"<p>A new status condition will be added to CUDN/UDN that will indicate how many nodes are selected for a network: <pre><code>status:\n  conditions:\n    - type: NodesSelected\n      status: \"True\"\n      reason: DynamicAllocation\n      message: \"5 nodes rendered with network\"\n      lastTransitionTime: 2025-09-22T20:10:00Z\n</code></pre></p> <p>If the status is \"False\", then no nodes are currently allocated for the network - no pods or egress IPs assigned.</p> <p>Cluster Manager will leverage instances of EgressIP and Pod Trackers in order to use that data for updating this status. The nodes serving a network are defined as a node with at least one OVN networked pod or having an Egress IP assigned to it on a NAD that maps to a UDN or CUDN.</p> <p>Additionally, events will be posted to the corresponding UDN/CUDN when nodes have become active or inactive for a node. This was chosen instead of doing per node status events, as that can lead to scale issues. Using events provides the audit trail, without those scale implications. The one drawback of this approach pertains to UDN deactivation. There is an \"udn-deletion-grace-period\" timer used to delay deactivation of a UDN on a node. This is to prevent churn if a pod is deleted, then almost immediately re-added. Without storing the timestamp in the API, we are relying internally on in memory data. While this is fine for normal operation, if OVN-Kube pod restarts, we lose that context. However, this should be fine as when we restart we have to walk and start all network controllers anyway, so we are not really creating a lot of extra work for OVN-Kube here.</p> <p>A metric will also be exposed which allows the user to track over time how many nodes were active for a particular network.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#testing-details","title":"Testing Details","text":"<ul> <li>Unit Tests will be added to ensure the behavior works as expected, including checking that OVN switches/routers are not created there is no pod/egress IP active on the node, etc.</li> <li>E2E Tests will be added to create a CUDN/UDN with the feature enabled and ensure pod traffic works correctly between nodes.</li> <li>Benchmark/Scale testing will be done to show the resource savings of 1000s of nodes with 1000s of UDNs.</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#documentation-details","title":"Documentation Details","text":"<ul> <li>User-Defined Network feature documentation will be updated with a user guide for this new feature.</li> </ul>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<p>Risks:  * Additional first-pod cold start latency per UDN/node. Could impact pod readiness SLOs.  * Burst reconcile load on large rollouts of pods on inactive nodes.</p> <p>Limitations:  * No OVN central support.  * NodePort/ExternalIP services with external traffic policy mode \"cluster\", will not work when sending traffic to inactive nodes.  * MetalLB must be configured on nodes where the UDN is rendered. This can be achieved by scheduling a daemonset for the designated nodes on the UDN.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#ovn-kubernetes-version-skew","title":"OVN Kubernetes Version Skew","text":"<p>Targeted for release 1.2.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#alternatives","title":"Alternatives","text":"<p>Specifying a NodeSelector in the CUDN/UDN CRD in order to determine where a network should be rendered. This was the initial idea of this enhancement, but was evaluated as less desirable than dynamic allocation. The dynamic allocation provides more flexibility without a user/admin needing to intervene and update a CRD.</p>"},{"location":"okeps/okep-5552-dynamic-udn-node-allocation/#references","title":"References","text":"<p>None</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/","title":"OKEP-5674: DPU Healthcheck","text":"<ul> <li>Issue: #5674</li> </ul>"},{"location":"okeps/okep-5674-dpu-healthcheck/#problem-statement","title":"Problem Statement","text":"<p>With the DPU (trusted) architecture, we have ovnkube running on the host (DPU-Host mode), and ovnkube running in the DPU (DPU mode). Kubelet talks on the host to dpu-host mode, which annotates the VF rep information so that DPU ovnkube can plug the port into OVS and handle all the OVN configuration. The DPU component is managed outside the host kubernetes cluster.</p> <p>The purpose of this addition is to be able to detect when the DPU side goes down, and indicate that the CNI is no longer ready on the node.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#goals","title":"Goals","text":"<ul> <li>Decrease CNI failure time by detecting when the DPU is unhealthy</li> <li>Avoid Kubernetes scheduling new pods via to nodes where the DPU is unhealthy</li> </ul>"},{"location":"okeps/okep-5674-dpu-healthcheck/#non-goals","title":"Non-Goals","text":"<ul> <li>Tainting the node</li> <li>Distinguishing between different types of failure in the DPU</li> </ul>"},{"location":"okeps/okep-5674-dpu-healthcheck/#introduction","title":"Introduction","text":"<p>As a refresher, the architecture of the DPU architecture in \"trusted\" mode is shown below:</p> <pre><code>flowchart TB\n\n  subgraph HOST[Host]\n    direction TB\n    H1[\"Kubelet\"]\n    H2[\"ovn-kubernetes (DPU-Host mode)\"]\n  end\n\n  subgraph DPU[DPU]\n    direction TB\n    D1[\"ovn-kubernetes (DPU mode)\"]\n    D2[OVS]\n    D3[OVN]\n    D1 --&gt; D2\n    D1 --&gt; D3\n  end\n\n  HOST &lt;--&gt; DPU\n</code></pre> <p>Note, \"trusted\" mode here means the DPU side has the kubeconfig of the host cluster, and runs OVNK in the DPU. There is an \"untrusted\" mode, but it is outside the scope of this enhancement and will not use this feature.</p> <p>As can be seen in the diagram, the ovn-kubernetes DPU mode is running down in the DPU, managed outside the purview of the host Kubernetes cluster. Therefore, the Host Kubernetes cluster has no observability or failure detection of this pod. When a DPU-accelerated pod is created, the following happens:</p> <ol> <li>Kubelet invokes Multus which checks that the UDN/NAD has a resourceName field</li> <li>SR-IOV plugin is called and reserves a VF, passes the PCI address in the CNI ADD</li> <li>OVN-Kubernetes DPU-Host receives the CNI ADD, and annotates the pod with the connection details.</li> <li>OVN-Kubernetes DPU waits for the annotation, plugs the VF representor into OVS, and annotates the pod with typical k8s.ovn.org annotations.</li> <li>OVN-Kubernetes DPU-Host sees annotations, and then moves the VF into the pod and configures it</li> <li>DPU-Host sends CNI ADD reply</li> </ol> <p>Now, if the DPU side goes down, DPU-Host will keep waiting on the CNI ADD for 2 minutes. All CNI ADDs will keep failing. From a user's perspective on the Host Kubernetes cluster, they may have no insight into what is running in the DPU from KAPI. We can make this better by:</p> <ol> <li>Detecting when the DPU side has gone down.</li> <li>Failing CNI operations fast as we know when the DPU is down.</li> <li>Triggering Kubelet to report the node as not ready to avoid future pods from being scheduled to this node.</li> </ol>"},{"location":"okeps/okep-5674-dpu-healthcheck/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":"<p>Story 1: Make DPU operations more robust</p> <p>As a cluster admin, I want to leverage DPUs for hardware offload. However, I do not want to compromise my visibility into the cluster and be stuck with components that are failing outside my view. I'd like to have DPU issues propagate up the stack so that I can see when a node has failed. Additionally, I would like Kubelet to no longer schedule pods to this node until it has come back online.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#proposed-solution","title":"Proposed Solution","text":"<p>Kubernetes uses node leases as a mechanism to maintain health within a cluster. This solution proposes that a custom node lease is created and used by OVN-Kubernetes to detect when a DPU is no longer functioning. The DPU OVN-Kubernetes component will update this node lease periodically. DPU-Host will monitor this node lease, and when it expires it will:</p> <ol> <li>Trigger CNI operations to fail fast, CNI is down.</li> <li>Implement the CNI STATUS verb to signal CNI issues to the container runtime.</li> </ol> <p>The container runtime such as CRI-O or containerd will call CNI STATUS. When CNI STATUS returns an error, the runtime will report to Kubelet that <code>NetworkReady=false</code>, and then Kubelet will set the Kubernetes node status as <code>NotReady</code>. Kubernetes will stop scheduling new pods to this node, and after a grace period (default 300 seconds), Kube Node Controller will taint the node, triggering eviction for pods that do not tolerate the taint.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#api-details","title":"API Details","text":"<p>A configuration knob will be introduced in OVN-Kubernetes to set the \"--dpu-node-lease-renew-interval\" time in seconds. The default setting will be 10 seconds. Set this value to 0 to disable. In addition, another config knob \"--dpu-node-lease-duration\" will be exposed that takes a time value in seconds. It will be used as the max time before the DPU is considered dead. The default time will be 40 seconds. These values are similar to what Kubernetes uses for Kubelet node leases.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#implementation-details","title":"Implementation Details","text":"<p>OVN-Kubernetes default node network controller (DNNC) will be updated to update the node lease when the mode is configured to be DPU mode, and the --dpu-node-lease-renew-interval is &gt; 0. OVN-Kubernetes CNI server will be updated to track this lease, and when the DPU node lease expires it will trigger CNI to:</p> <ul> <li>Immediately fail CNI ADD, rather than annotating the pod with the VF and waiting 2 minutes. The CNI ADD will fail with a new message \"DPU Not Ready\".</li> <li>Report STATUS according to the CNI specification.</li> </ul>"},{"location":"okeps/okep-5674-dpu-healthcheck/#dpu-custom-node-lease","title":"DPU Custom Node Lease","text":"<p>OVN-Kubernetes RBAC will be updated to allow for a RoleBinding to allow ovnkube to interact with the Kubernetes <code>lease</code> resource in the ovn-kubernetes namespace.  DPU-Host will be responsible for creating the lease, and the owner reference will be set to the Kubernetes Node object. This will allow automatic clean up if the node is deleted in Kubernetes. Example:</p> <pre><code>apiVersion: coordination.k8s.io/v1\nkind: Lease\nmetadata:\n  name: ovn-dpu-worker-node-1\n  namespace: ovn-kubernetes\n  ownerReferences:\n    - apiVersion: v1\n      kind: Node\n      name: worker-node-1\n      uid: &lt;actual-node-uid&gt; # Critical for GC to work\nspec:\n  holderIdentity: \"ovnkube-dpu-node\"\n  leaseDurationSeconds: 40\n  renewTime: \"2023-10-27T10:00:00Z\"\n</code></pre> <p>ovnkube-node-dpu will be given update and read RBAC permissions in order to update the heartbeat every interval.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#testing-details","title":"Testing Details","text":"<ul> <li>Unit tests will be added to make sure CNI ADD/STATUS function correctly under different DPU conditions.</li> <li>Unit tests will be added to ensure that the DPU custom node lease functions as expected.</li> <li>E2E tests will be added where there is no DPU mode, and DPU-Host mode is just launched. Checking that the node condition gets updated correctly and pods are not scheduled to the node.</li> <li>Future E2E tests will be added upstream once there is a full DPU CI environment to do more comprehensive testing.</li> </ul>"},{"location":"okeps/okep-5674-dpu-healthcheck/#documentation-details","title":"Documentation Details","text":"<p>Documentation will be added describing this feature as well as the overall OVN-Kubernetes Trusted DPU architecture.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":"<p>Only applicable for Trusted DPU Architecture. Zero-trust will be handled later.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#ovn-kubernetes-version-skew","title":"OVN-Kubernetes Version Skew","text":"<p>Targeting version 1.2.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#alternatives","title":"Alternatives","text":"<p>Instead of using the node condition to indicate the CNI is down, cluster-manager could listen for the lease expiry and then taint the node. We have tried tainting the node in the past, and it led to a bunch of issues causing us to revert it: https://github.com/ovn-kubernetes/ovn-kubernetes/pull/2459. The SDN should refrain from tainting nodes, as it can cause system critical pods to not be able to start, including OVN-Kubernetes itself.</p>"},{"location":"okeps/okep-5674-dpu-healthcheck/#references","title":"References","text":"<p>None</p>"},{"location":"okeps/okep-5224-connecting-udns/discarded-alternatives/","title":"Discarded alternatives","text":""},{"location":"okeps/okep-5224-connecting-udns/discarded-alternatives/#network-topology-alternatives-that-were-discarded","title":"Network Topology Alternatives that were discarded","text":"<p>Connecting Layer3 type networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue, green and yellow) across two nodes.</p> <p></p> <p>Idea1: Add a new transit switch between the two ovn_cluster_routers of the networks</p> <p>When the UDNConnect CR gets created selecting these three networks, the controller will make the following changes to the topology: * Create a distributed colored-enterprise_interconnect-switch   (basically a transit-switch type in OVN used for   interconnection) with the <code>connectSubnets</code> value provided on the CR   (default=192.168.0.0/10) * Connect the colored-enterprise_interconnect-switch to the   ovn_cluster_router's of the blue, green and yellow networks on each   node using remote patch ports and tunnelkeys * Add routes towards each connected network's CIDR on the ovn_cluster_router's   of the blue, green and yellow networks on each node.</p> <p></p> <p>So in this idea it will be 1 router per connect API.</p> <p>However this would cause an asymmetry for reply traffic:</p> <p></p> <p>Idea1.1 Add a new transit router between two ovn_cluster_routers of the networks and move the node-network specific routes to the new connect router</p> <p>Ultimately Idea 1.1 is what we went with in the main proposal because its the solution with no asymmetry. See the proposal for more details around this.</p> <p>Idea2: Connecting the ovn_cluster_routers directly</p> <p>When the UDNConnect CR gets created selecting these two networks, the controller will make the following changes to the topology: * Create a logical-router-port on <code>blue_blue.network_ovn_cluster_router</code>   with an IP assigned from <code>connectSubnets</code>. * Create a logical-router-port on <code>green_green.network_ovn_cluster_router</code>   with an IP assigned from <code>connectSubnets</code>. * Create a logical-router-port on   <code>yellow_yellow.network_ovn_cluster_router</code> with an IP assigned from   <code>connectSubnets</code>. These three ports will have their <code>peer</code> field set as each other to allow for direct connectivity.</p> <p></p> <p>The above diagram shows what happens when all 3 networks are connected together.</p> <p>Which idea to pick?</p> Topic Idea1 Idea2 Number of new IPs required for connectSubnets N(nodes)*M(networks) N(nodes)*M(networks) Requires ovnkube-controller local IPAM for per node-network router YES YES Requires node-network router IP storage design (either annotations OR nodeNetworkCRD) YES YES Requires Centralized IPAM slicing per node YES NO - here given each node can re-use the same set of IPs, there is no need for centralized slicing - this is a huge advantage Requires Centralized tunnel-ids allocation YES NO - this is a huge advantage Reply is Symmetric NO - but can be made symmetric with /24 routes if required NO - replies will be assymmetric Scalability of routes /16 will be 1 route on each router per connected network /16 will be 1 route on each router per connected network Scalability of connections Each router get's 1 additional port and the new switch will get M (number of networks) patch ports - so totally N*M patch ports Point-to-point connections means all ovn_cluster_router's now have N*M patch ports Dataplane churn caused for connect/disconnect More isolated changes because adding and disconnecting a network is as simple as adding/removing the port only from that network's router and the central switch. Direct R2R Connection - so disconnecting a network means adding/removing the port from each connected router towards this network. So removing a specific tenant network will involve touching routers of other tenant's networks. Network being part of more than one connectAPI Non-transitivity is automatically taken care of because there is no connection between the indirectly connected networks. Non-transitivity is taken care of because of the absence of routes on the router for the indirect connection to work. Any lingering stale routes could accidentally cause connectivity. Extensibility for adding features Better because adding a transit router or switch opens up adding newer features easily like having hierarchical hubs of connections Direct Connection might put us in a bind with regards to how much topology change we could do in future Throughput Add's an extra hop Better because its a direct connection Networking model Aligns with a vRouter/vSwitch model that users expect Direct R2R Connection Connecting mixed type networks Aligns with a vRouter/vSwitch model that users expect - 1 transit switch per connectAPI Direct R2R Connection Accounting for missing topologies on some nodes This can be accommodated but we'd need to use specific /24 routes.. What about L2? This won't work <p>Idea3: Connect the transit switches of the two UDNs together (either using an interconnect switch/router in the middle OR directly using remote ports)</p> <p></p> <p>This idea was dropped because: * Additional burden for the user and new API change: Given each transit   router today has the same 100.88/16 subnet that is not configurable in   UDNs on day2, if we want to connect transit switches directly together,   we would need support day2 changes by exposing this subnet via the API   and to place an additional constraint of either having unique   transit-subnets in each of the UDNs that have to be connected together.   This being an internal implementation details seems like a hard   requirement for the user to know and configure. * Upgrades and disruption of traffic: Users using UserDefinedNetworks   feature in the older versions will need to make the above disruptive   change of transit switch subnets that will lead to traffic disruption. * Transitivity of interconnections will happen here which is not   desired: If we now wanted to connect yellow with green, yellow would   end up being connected to blue automatically which is not desired, in   the case where it's directly connecting with remote ports.</p> <p>Idea3.1: Using NATs</p> <p>To avoid having to change the transit switch subnet ranges, NAT to a particular UDN IP before sending it into the interconnect router - but I assume we want to preserve srcIPs of the pods. This would also add additional unwanted complexity so this idea was also not pursued.</p> <p>Connecting Layer2 type networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue, green and yellow) across two nodes. NOTE: The topology representation is that of the new upcoming Layer2 topology. The dotted pieces in network colors represent the bits in design and not there yet.</p> <p></p> <p>Idea1: Add a new transit switch between the two transit routers of the networks</p> <p></p> <p>Idea2: Connecting the ovn_cluster_routers directly</p> <p></p> <p>Connecting mixed type (layer3 and/or layer2) networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue(l3), green(l3) and yellow(l2)) across two nodes. NOTE: The yellow topology representation is that of the new upcoming Layer2 topology. The dotted pieces in network colors represent the bits in design and not there yet.</p> <p></p> <p>Idea1: Add a new transit switch between the two transit routers of the networks</p> <p></p> <p>Idea2: Connecting the ovn_cluster_routers directly</p> <p></p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/","title":"OKEP-5224: Connecting UserDefinedNetworks","text":""},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#problem-statement","title":"Problem Statement","text":"<p>A (C)UDN is an isolated island, however, we have users that wish to connect these islands together for multiple reasons. This OKEP proposes a solution to connect multiple UserDefinedNetworks or ClusterUserDefinedNetworks together, better known as \"Connecting (C)UDNs\". This solution will ensure that (C)UDNs remain isolated unless an explicit request to connect them together has been made.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#user-personas","title":"User Personas:","text":"<ul> <li><code>Admin</code>: Cluster administrator who has the highest priviledge access to   the cluster. Admin creates the namespaces for each tenant in the cluster.</li> <li><code>Tenant</code>: A tenant is an end user who owns the resources (workloads,   policies, services, udns) created in one or more namespaces that are   designated to them by admins. They cannot update or mutate the   namespace(s) but they live within the namespace(s) boundary controlled   using RBAC.</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#goals","title":"Goals","text":"<ul> <li>Connecting and disconnecting UDNs and CUDNs within the same cluster   aka intra-cluster-udn-connectivity for admins (can be done as day0 or   day2).</li> <li>Support for connecting primary <code>CUDN&lt;-&gt;UDN</code> across namespaces<sup>[footnote]</sup> which includes:<ul> <li>Support for connecting <code>CUDN&lt;-&gt; CUDN</code> (for <code>Admin</code> role only)</li> <li>Support for connecting <code>CUDN&lt;-&gt;UDN</code> (for <code>Admin</code> role only)</li> <li>Support for connecting <code>UDN&lt;-&gt;UDN</code> (for <code>Admin</code> role only)</li> </ul> </li> <li>Support for connecting <code>Layer3&lt;-&gt;Layer3</code>, <code>Layer2&lt;-&gt;Layer2</code> and     <code>Layer3&lt;-&gt;Layer2</code> type networks.</li> <li>Support for only <code>role:Primary</code> type CUDN, and UDN connections.</li> <li>Support for pods, services (clusterIPs only since expectation today is   that NodePorts and LoadBalancer type services are already exposed   externally and hence reachable across UDNs) and network policies across   the connected UDNs and CUDNs.</li> <li>Ensure the proposed API is expandable for also connecting and   disconnecting UDNs or CUDNs across clusters in the future if need be.</li> </ul> <p>[footnote] Food for thought: Currently, there is no need to consider connecting UDNs within the same namespace since there is no support for more than 1 primary-UDN in a namespace and each pod would have connection to its primary and secondary UDNs in its namespace. However there are use cases for connecting secondary UDNs within a namespace to consider in the future. There is also a future enhancement on multiple primary UDNs which if accepted can automatically leverage the work happening in this enhancement.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#future-goals","title":"Future Goals","text":"<ul> <li>[Tenant usecase] Connecting and disconnecting UDNs owned by the   same tenant in same namespace or across different namespaces they   own, in a self-serve fashion without any admin intervention. A tenant   should be able to connnect the secondary networks that belong to them.   This would most likely be a namespace-scoped API.</li> <li>Tenants being able to request for interconnecting their networks   with other tenants.<ul> <li>We need to understand exact use cases for why we need tenants to   initiate connection requests with other tenants. How will a tenant   know other tenants that exist on the cluster? In cases like finance   tenant wanting to use IT tenant's services, they could open a ticket   to the admin to make that happen.</li> <li>Regardless, the tenant API might need more RBAC considerations if we   get such use cases</li> <li>One other idea is to re-use the same peering semantics that   network policies API uses where both the CRs across both the tenant   namespaces would need to select each other to be permitted   connectivity</li> </ul> </li> <li>Admins being able to connect <code>role:Secondary</code> UDNs and CUDNs. The   reason why this is a future goal is because of the following reasons:<ul> <li>For UDNs, given <code>SecondaryUserDefinedNetworkSelector</code> uses a   combination of namespace selector and network selector, a tenant could   change the labels on the UDN thereby causing security issues. Hence   this type of selector support will come later where we might want to   add a well-known label to be able to leverage selectors.</li> <li>The secondary networks don't have the same level of topology parity   as primary networks today. So there is no support for transit router   or egress gateway today in <code>Layer2</code> secondary networks. Hence we would   need to add the transit router to the secondary network <code>Layer2</code>   topology as well which can come in the future. <code>Layer3</code> networks don't   have the same issue as the required topology parity exists but given   we'd need that change for <code>Layer2</code> might as well do the full secondary   networks connect support in future.</li> </ul> </li> <li>Connecting two UDNs with overlapping subnets won't be supported in the first   phase. This is a valid use case scenario but harder to implement.</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#non-goals","title":"Non-Goals","text":"<ul> <li>Support for selecting NADs directly. Since UDNs is the future, new   features should not be added on NADs.</li> <li>Support for this feature in non-interconnect architecture is out of scope   since we plan to deprecate non-interconnect mode soon.</li> <li>Support for <code>localnet</code> type network connections is out of scope.</li> <li><code>localnet</code> type networks can be connected together using bridges     already</li> <li><code>localnet</code>&lt;-&gt;<code>layer2</code> or <code>localnet</code>&lt;-&gt;<code>layer3</code> will not be covered here.</li> <li>Supporting Live Migration and PersistentIPs across connected UDNs.</li> <li>Connecting and disconnecting UDNs and CUDNs across two clusters   aka inter-cluster-udn-connectivity for admins (can be done as day0 or   day2). The plan is to be able to do this using EVPN protocol. See   enhancement on EVPN to see how it is planned to be leveraged as the   mode to connect different UDNs across clusters. But regardless, the API   bits around how to request connectivity needs more fleshing and EVPN's   primary use case is not about connecting UDNs across clusters.</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#introduction","title":"Introduction","text":"<p>OVN-Kubernetes supports two types of user-defined networks for workload isolation:</p> <ul> <li> <p>UserDefinedNetworks (UDNs): Namespace-scoped networks that tenants can   create to isolate workloads within a namespace.</p> </li> <li> <p>ClusterUserDefinedNetworks (CUDNs): Cluster-scoped networks that admins   can use to connect multiple namespaces as part of the same isolated network.</p> </li> </ul> <p>These networks can be configured with different roles:</p> <ul> <li> <p><code>role:Primary</code>: The network becomes the primary network for workloads.   Workloads attached to a primary UDN/CUDN cannot communicate with workloads   on other UDNs/CUDNs or the default network.</p> </li> <li> <p><code>role:Secondary</code>: Enables multihoming scenarios, allowing pods to be   attached to multiple networks simultaneously.</p> </li> </ul> <p>Currently, workloads on different (C)UDNs are completely isolated from each other. However, there are scenarios where partial or full connectivity between microservices segmented across different (C)UDNs is desirable (see use cases section).</p> <p>This OKEP defines how to implement connectivity between two or more (C)UDNs within a single cluster. The following sections outline the design assumptions and implementation details.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#symmetric-nature-of-udn-connectivity","title":"Symmetric nature of UDN Connectivity","text":"<p>If UDN-A is connected to UDN-B then that means UDN-B is connected to UDN-A. When connecting those networks together it assumes bi-directional relationship.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#non-transitive-nature-of-udn-connectivity","title":"Non-Transitive nature of UDN Connectivity","text":"<p>If UDN-A and UDN-B are connected together and UDN-B and UDN-C are connected together, that does not mean UDN-A is connected to UDN-C. Reasoning is we cannot assume something indirectly - It's always best to let the user express what they'd like to see connected.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#user-storiesuse-cases","title":"User-Stories/Use-Cases","text":""},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-1-allowing-network-expansion-for-a-tenant","title":"Story 1: Allowing network expansion for a tenant","text":"<p>As a cluster admin, I want to connect my newly created UDN-New with an existing UDN-Old so that they can communicate with each other as if being part of the same network.</p> <p>Example: Existing (C)UDN has full occupancy of its address-space and we don\u2019t allow CIDR expansion or mutation of UDN Spec today.</p> <p>The admin created UDN-Old with a defined subnet CIDR range which gets exhausted on a 500th day, now the admin cannot mutate UDN-Old to change or expand its subnet CIDR range. Admin creates a new UDN-New (could be in a totally different cluster) and now wants to interconnect them together.</p> <p></p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-2-allowing-connectivity-between-two-tenants-through-exposed-clusterip-services","title":"Story 2: Allowing connectivity between two tenants through exposed clusterIP services","text":"<p>As a cluster admin, I want to allow tenant-Green to be able to access the clusterIP microservice exposed by tenant-Blue without direct access to workloads behind the service.</p> <p>Example: When separate services owned by different teams need to communicate via APIs like frontend needing to access database through the backend service.</p> <p>The admin created UDN-Blue and UDN-Green that were isolated initially but now wants to allow partial inter-tenant communication. Say data produced by microservice blue needs to be consumed by microservice green to do its task.</p> <p></p> <p>When the two networks have overlapping subnets, using service clusterIP as a way to allow communication is valid use case.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-3-allowing-connectivity-between-two-networks-with-overlapping-subnets-through-exposed-clusterip-services","title":"Story 3: Allowing connectivity between two networks with overlapping subnets through exposed clusterIP services","text":"<p>As a cluster admin, I want to allow tenant-Green to be able to access the clusterIP microservice exposed by tenant-Blue even though tenant-Green and tenant-Blue have overlapping pod subnets.</p> <p>Example: IPAddress Management is hard. It is not possible to predict the future of how all applications will communicate with one another. There is a chance app-type-green and app-type-blue got the same pod subnets and now they need to talk to each other. While allowing direct connection of the two overlapping pod subnet networks is hard, a middleground is to use clusterIPs to communicate.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-4-merging-two-tenants-maybe-even-temporarily","title":"Story 4: Merging two tenants (maybe even temporarily)","text":"<p>As a cluster admin, I want to now connect my existing UDN-Blue and UDN-Green so that they can communicate with each other as if being part of the same network.</p> <p>Example: The admin created UDN-Blue and UDN-Green that were isolated initially but now wants to treat them as being part of the same network. Pods cannot be rewired given they are part of two different UDNs. Say when different teams within the same organization operate as separate tenants but collaborate on a project requiring unrestricted communication for a period of time and then want to disconnect from each other later.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-5-same-udn-across-multiple-clusters-must-be-connected-by-default","title":"Story 5: Same UDN across multiple clusters must be connected by default","text":"<p>As a cluster admin, I want to connect my workloads which are part of the same UDN but are in different clusters. </p> <p>Example: The user has split their workloads into multiple clusters thereby the same UDN could be fragmented across different clusters. These pods should be able to communicate with each like it would be if they were on the same cluster but be isolated from other workloads.</p> <p>NOTE: Cross cluster UDN connecting is not targeted in this OKEP, but is covered here since connecting UDNs within the cluster is the first step here.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-6-connecting-mixed-type-networks-together-layer2-network-workloads-with-layer3-primarydefault-network","title":"Story 6: Connecting mixed type networks together (Layer2 Network workloads with Layer3 Primary/Default Network)","text":"<p>As a cluster admin, I manage isolated tenants (a bunch of interconnected pods) and share them with my users. Every tenant has its own Primary Layer3 UDN to emulate its own cluster network, but isolate it from the other tenants. Northbound/internet traffic from my tenants is always flowing through additional gateways that I manage. Gateways require HA support, which is done using OVN virtual Port type, which is only supported when all parent ports are located on the same switch, hence the UDN used by the Gateways is Layer2 (Primary).</p> <p>To make tenants' traffic flow through the gateways, I need to interconnect Primary Layer3 network with Primary Layer2 network. Here is a diagram illustrating this ask:</p> <p></p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-7-migration-of-connected-tenant-workloads-from-other-platforms-like-nsx-into-ovn-kubernetes","title":"Story 7: Migration of connected tenant workloads from other platforms like NSX into OVN-Kubernetes","text":"<p>As a cluster admin, I manage isolated tenants on my native NSX platform where they are already seggregated into segments using networks. Some of these segments are connected through vRouter for mutual access. I want to migrate such workloads into OVN-Kubernetes platform by leveraging user defined networks. I would need connecting UDNs feature to replicate that same workload architecture. The alternative of placing those workloads into same UDN and using network policies will not work because it is significant architectural change we would need to mandate on each tenant since tenant own their networks.</p> <p>Example: Tenant-frontend has access to Tenant-backend's databaseAPI workloads on NSX through vRouter, I want to migrate both these tenants into OVN-Kubernetes platform keeping that architecture intact.</p> <p></p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#story-8-future-use-case-tenant-self-connection-without-admin-intervention","title":"Story 8: [Future Use Case] Tenant self-connection without admin intervention","text":"<p>As a tenant owner, I want to connect my workloads which are part of two different namespaces (networks) in the same cluster with no admin intervention.</p> <p>Example: If the same tenant owns microservices across 5 namespaces that are 5 different networks, there should be provision for this tenant to connect their networks together without the requirement for an admin to intervene. The reason could be the same as the above admin user stories but this story captures the tenant intent with no admin intervention</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#proposed-solution","title":"Proposed Solution","text":"<p>This section tries to answer: How can an admin declare two (C)UDNs to be connected?</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#config-for-enabling-the-feature","title":"Config for enabling the feature","text":"<p>The whole feature will be behind a config flag called <code>--enable-network-connect</code> which has to be set to true to get this feature.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#api-details","title":"API Details","text":"<p>This enhancement proposes a new CRD to allow admins to request multiple networks to be connected together.</p> <p><code>ClusterNetworkConnect</code> CR will be cluster-scoped resource and admin-only API that will allow admins to select multiple different networks that need to be connected together. It will also allow specifying what level of connectivity is expected. See the API definitions for more details.</p> <p>Admins can create a <code>ClusterNetworkConnect</code> CR which signals the controller to connect the topologies of the UDNs selected by this CR.</p> <p>Sample YAML:</p> <p><pre><code>apiVersion: k8s.ovn.org/v1\nkind: ClusterNetworkConnect\nmetadata:\n  name: colored-enterprise\nspec:\n  networkSelectors: # can match on UDNs and/or CUDNs\n    - networkSelectionType: ClusterUserDefinedNetworks\n      clusterUserDefinedNetworkSelector:\n        networkSelector:\n          matchExpressions:\n          - key: kubernetes.io/metadata.name\n            operator: In\n            values:\n            - blue-network\n            - green-network\n    - networkSelectionType: PrimaryUserDefinedNetworks\n      primaryUserDefinedNetworkSelector:\n        namespaceSelector:\n          matchExpressions:\n          - key: kubernetes.io/metadata.name\n            operator: In\n            values:\n            - yellow\n            - red\n  connectSubnets: # can have at most 1 CIDR for each family type\n  - cidr: \"192.168.0.0/16\"  # 65K total IPs\n    networkPrefix: 24 # 255 IPs per layer3 network = ~100 possible nodes in your cluster\n  - cidr: \"fd01::/64\"\n    networkPrefix: 96\n  connectivity:\n    - PodNetwork\n    - ClusterIPServiceNetwork\n</code></pre> * <code>connectivity</code> field will ensure we can add support for enabling   more types of features for the connected UDNs granularly in the future.   At least one of these options must be set and validations will be added   for which combinations of these options could/should be set together * If <code>PodNetwork</code> is enabled then admin is asking for full   pod2pod connectivity across UDNs * If <code>ClusterIPServiceNetwork</code> is enabled then admin is asking for   clusterIPs to be reachable across UDNs.     * If this flag is set but <code>PodNetwork</code> is not set, then       it means users only want partial connectivity through services and       not full mesh pod connectivity. * <code>connectSubnets</code> field is used to take a configurable subnet from the end   user that will be used to connect the different networks together. The   slice needs to be big enough to accommodate an IP per network per node   for layer3 network connectivity. So if there are M nodes in your cluster   and N_L3 networks you should have 2*(N_L3\u00d7M) IPs for allocation in this slice.   The <code>networkPrefix</code> contains the prefix length allocated for each   connected layer3 network. From this pool each node get's a <code>/31</code> or <code>/127</code> point to   point network for actual topology connectivity.   To connect layer2 networks we don't need per node-network slices, we just   need one <code>/31</code> or <code>127</code> slice per network.   ConnectSubnets value is expected to be configured uniquely across multiple <code>ClusterNetworkConnect</code>   CRs that select the same network i.e. If a given network is selected as part   of more than 1 ClusterNetworkConnect API, each of those ClusterNetworkConnect   APIs must have a unique connect subnet. If a user accidentally configures overlapping values,   we must have a status check to block and report this in the status. See   status validation section for more details.   ConnectSubnets field cannot be mutated to keep things simple in the first   iteration/implementation of this enhancement. Can be revisited in future.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#api-field-details-and-cel-validations","title":"API field details and CEL Validations","text":"<p>API fields are described in detail below along with field validations:</p> <pre><code>// ClusterNetworkConnect enables connecting multiple User Defined Networks\n// or Cluster User Defined Networks together.\n//\n// +genclient\n// +genclient:nonNamespaced\n// +k8s:openapi-gen=true\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n// +kubebuilder:resource:path=clusternetworkconnects,scope=Cluster,shortName=cnc,singular=clusternetworkconnect\n// +kubebuilder:object:root=true\n// +kubebuilder:subresource:status\n// +kubebuilder:printcolumn:name=\"Age\",type=\"date\",JSONPath=\".metadata.creationTimestamp\"\n// +kubebuilder:printcolumn:name=\"Status\",type=string,JSONPath=\".status.status\"\ntype ClusterNetworkConnect struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    // +kubebuilder:validation:Required\n    // +required\n    Spec ClusterNetworkConnectSpec `json:\"spec\"`\n\n    // +optional\n    Status ClusterNetworkConnectStatus `json:\"status,omitempty\"`\n}\n\n// ClusterNetworkConnectSpec defines the desired state of ClusterNetworkConnect.\n// +kubebuilder:validation:XValidation:rule=\"!self.networkSelectors.exists(i, i.networkSelectionType != 'ClusterUserDefinedNetworks' &amp;&amp; i.networkSelectionType != 'PrimaryUserDefinedNetworks')\",message=\"Only ClusterUserDefinedNetworks or PrimaryUserDefinedNetworks can be selected\"\ntype ClusterNetworkConnectSpec struct {\n    // networkSelectors selects the networks to be connected together.\n    // This can match User Defined Networks (UDNs) and/or Cluster User Defined Networks (CUDNs).\n    // Only ClusterUserDefinedNetworkSelector and PrimaryUserDefinedNetworkSelector can be selected.\n    //\n    // +kubebuilder:validation:Required\n    // +required\n    NetworkSelectors types.NetworkSelectors `json:\"networkSelectors\"`\n\n    // connectSubnets specifies the subnets used for interconnecting the selected networks.\n    // This creates a shared subnet space that connected networks can use to communicate.\n    // Can have at most 1 CIDR for each IP family (IPv4 and IPv6).\n    // Must not overlap with:\n    //  any of the pod subnets used by the selected networks.\n    //  any of the transit subnets used by the selected networks.\n    //  any of the service CIDR range used in the cluster.\n    //  any of the join subnet of the selected networks to be connected.\n    //  any of the masquerade subnet range used in the cluster.\n    //  any of the node subnets for choosen by the platform.\n    //  any of other connect subnets for other ClusterNetworkConnects that might be selecting same networks.\n    //\n    // Does not have a default value for the above reason so\n    // that user takes care in setting non-overlapping subnets.\n    // Cannot be mutated once set.\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinItems=1\n    // +kubebuilder:validation:MaxItems=2\n    // +kubebuilder:validation:XValidation:rule=\"self == oldSelf\", message=\"connectSubnets is immutable\"\n    // +required\n    // +kubebuilder:validation:XValidation:rule=\"size(self) != 2 || !isCIDR(self[0].cidr) || !isCIDR(self[1].cidr) || cidr(self[0].cidr).ip().family() != cidr(self[1].cidr).ip().family()\", message=\"When 2 CIDRs are set, they must be from different IP families\"\n    ConnectSubnets []ConnectSubnet `json:\"connectSubnets\"`\n\n    // connectivity specifies which connectivity types should be enabled for the connected networks.\n    //\n    // +kubebuilder:validation:Required\n    // +kubebuilder:validation:MinItems=1\n    // +kubebuilder:validation:MaxItems=2\n    // +kubebuilder:validation:XValidation:rule=\"self.all(x, self.exists_one(y, x == y))\",message=\"connectivity cannot contain duplicate values\"\n    connectivity []ConnectivityType `json:\"connectivity\"`\n}\n\n// +kubebuilder:validation:XValidation:rule=\"isCIDR(self) &amp;&amp; cidr(self) == cidr(self).masked()\", message=\"CIDR must be a valid network address\"\n// +kubebuilder:validation:MaxLength=43\ntype CIDR string\n\n// +kubebuilder:validation:XValidation:rule=\"!has(self.networkPrefix) || !isCIDR(self.cidr) || self.networkPrefix &gt; cidr(self.cidr).prefixLength()\", message=\"networkPrefix must be smaller than CIDR subnet\"\n// +kubebuilder:validation:XValidation:rule=\"!has(self.networkPrefix) || !isCIDR(self.cidr) || (cidr(self.cidr).ip().family() != 4 || self.networkPrefix &lt; 32)\", message=\"networkPrefix must &lt; 32 for ipv4 CIDR\"\ntype ConnectSubnet struct {\n  // CIDR specifies ConnectSubnet, which is split into smaller subnets for every connected network.\n  // This CIDR should be containing 2*((Number of L3 networks*Max Number of Nodes)+Number of L2 networks) IPs.\n  // Example: cidr= \"192.168.0.0/16\", networkPrefix= 24 means that you can connect 256 Layer3 networks OR\n  // 255 Layer3 networks and 128 Layer2 networks\n  //\n  // CIDR also restricts the maximum number of networks that can be connected together\n  // based on what CIDR range is picked. So choosing a large enough CIDR for future use cases\n  // is important.\n  // +required\n  CIDR CIDR `json:\"cidr\"`\n\n  // networkPrefix specifies the prefix length for every connected network.\n  // This prefix length should be equal to or longer than the length of the CIDR prefix.\n  //\n  // For example, if the CIDR is 10.0.0.0/8 and the networkPrefix is 24,\n  // then the connect subnet for each connected layer3 network will be 10.0.0.0/24, 10.0.1.0/24, 10.0.2.0/24 etc.\n  // For each connected layer2 network we will allocate a /networkPrefix range\n  // that is then used to allocate /31 and /127 slices for each layer2 network.\n  // A good practice is to set this to a value that ensures it contains more\n  // than twice the number of maximum nodes planned to be deployed in the cluster\n  // and how frequently nodes are created and destroyed.\n  // To be safe, better to set this to a value that's 4 times the maximum nodes planned\n  // to be deployed in the cluster.\n  // Example - required values;\n  // if you plan to deploy 10 nodes, minimum required networkPrefix is /27 (20+ IPs)\n  // if you plan to deploy 100 nodes, minimum required networkPrefix is /24 (200+ IPs)\n  // if you plan to deploy 1000 nodes, minimum required networkPrefix is /21 (2000+ IPs)\n  // if you plan to deploy 5000 nodes, minimum required networkPrefix is /18 (10000+ IPs)\n  //\n  // However to be safe and in case nodes are created and destroyed at fast rate,\n  // set it to 4 times max nodes of the cluster:\n  // if you plan to deploy 10 nodes, set the networkPrefix to /26 (40+ IPs)\n  // if you plan to deploy 100 nodes, set the networkPrefix to /23 (400+ IPs)\n  // if you plan to deploy 1000 nodes, set the networkPrefix to /20 (4000+ IPs)\n  // if you plan to deploy 5000 nodes, set the networkPrefix to /17 (20000+ IPs)\n  // This field restricts the maximum number of nodes that can be deployed in the cluster\n  // and hence its good to plan this value carefully along with the CIDR.\n  //\n  // +kubebuilder:validation:Minimum=1\n  // +kubebuilder:validation:Maximum=127\n  // +required\n  NetworkPrefix int32 `json:\"networkPrefix\"`\n}\n\n// ConnectivityType represents the different connectivity types that can be enabled for connected networks.\n// +kubebuilder:validation:Enum=PodNetwork;ClusterIPServiceNetwork\ntype ConnectivityType string\n\nconst (\n    // PodNetwork enables direct pod-to-pod communication across connected networks.\n    PodNetwork ConnectivityType = \"PodNetwork\"\n\n    // ClusterIPServiceNetwork enables ClusterIP service access across connected networks.\n    ClusterIPServiceNetwork ConnectivityType = \"ClusterIPServiceNetwork\"\n)\n\n// StatusType represents the status of a ClusterNetworkConnect.\n// +kubebuilder:validation:Enum=Success;Failure\ntype StatusType string\n\nconst (\n    // Success indicates that the ClusterNetworkConnect has been successfully applied.\n    Success StatusType = \"Success\"\n\n    // Failure indicates that the ClusterNetworkConnect has failed to be applied.\n    Failure StatusType = \"Failure\"\n)\n\n// ClusterNetworkConnectStatus defines the observed state of ClusterNetworkConnect.\ntype ClusterNetworkConnectStatus struct {\n    // status is a concise indication of whether the ClusterNetworkConnect\n    // resource is applied with success.\n    // +kubebuilder:validation:Optional\n    Status StatusType `json:\"status,omitempty\"`\n\n    // conditions is an array of condition objects indicating details about\n    // status of ClusterNetworkConnect object.\n    // +patchMergeKey=type\n    // +patchStrategy=merge\n    // +listType=map\n    // +listMapKey=type\n    Conditions []metav1.Condition `json:\"conditions,omitempty\"`\n}\n\n// ClusterNetworkConnectList contains a list of ClusterNetworkConnect.\n// +kubebuilder:object:root=true\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\ntype ClusterNetworkConnectList struct {\n    metav1.TypeMeta `json:\",inline\"`\n    metav1.ListMeta `json:\"metadata,omitempty\"`\n    Items           []ClusterNetworkConnect `json:\"items\"`\n}\n</code></pre>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#validation-field-checks","title":"Validation Field Checks","text":"<p>In the implementation, after parsing the <code>ClusterNetworkConnect</code> object, we must ensure the following:</p> <ul> <li><code>connectSubnets</code> must not conflict with:   1) pod subnets of the selected networks to be connected   2) transit subnets of the selected networks to be connected   3) service CIDR range used in the cluster   4) join subnet of the selected networks to be connected   5) masquerade subnet range used in the cluster   6) node subnets for choosen by the platform. This is the responsibility      of the user and we won't provide validation for node subnets since      fetching that value could vary from platform to platform. We will      cover this in the docs.   If it does, we need to provide validation and feedback to the   end user for the conflict. </li> <li>Since connection of overlapping pod subnet networks is not   allowed in first phase, we must detect the collision and report   the error back to the user via the status</li> <li>Expectation is for users to initiate the connection of networks of same   family types together. If a singlestack IPV6 UDN is tried to be connected   to singlestack IPV4 UDN, then we will have to validate this as a misconfiguration   and have a status error for this.</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#valid-ways-of-connectingselectingdisconnecting-udns","title":"Valid ways of Connecting/Selecting/Disconnecting UDNs","text":"<ul> <li>A network could be selected as part of more than one network connnect, so   what is the expectation when two Connect APIs are designed that select the   exact same set of networks? How do we control validation of such scenarios?   Example, ClusterNetworkConnect-A selects blue and green networks and   ClusterNetworkConnect-B selects blue and red networks:<ul> <li>In such scenarios the same network's router is going to be connected to   more than one <code>connect-router</code>. So blue network will be connected   to both <code>connect-router-A</code> and <code>connect-router-B</code>.</li> <li>However given non-transitive relation and absence of connecting routes,   indirect connection of green and red will not happen.</li> <li>We must add validation in code to ensure if more than 1   ClusterNetworkConnect API selects the same network, then they must   have non-overlapping <code>connectSubnets</code> field configured. If there is a   conflict, we must emit an event on the connectAPI and fail to create   the plumbing for the subsequent case. OVN does not handle blocking   the creation of two ports on the same router with same subnets.   So if that happens, it will cause wrong routing and hence OVN-Kubernetes   must detect this conflict.</li> </ul> </li> <li>A ClusterNetworkConnect could be selecting a subset of networks already selected by   another ClusterNetworkConnect API. What is the expectation in this case?   Example,   ClusterNetworkConnect-A selects blue, green, red and yellow networks and   ClusterNetworkConnect-B selects blue and green networks.<ul> <li>This is going to be considered a user-configuration error oversight.   This same issue also exists for other APIs like <code>RouteAdvertisement</code>   (wonder what we do there?).</li> <li>We expect the rules to be additively implemented but in this case, we   won't optimize for anything since its a hard problem to solve. We will   end up create duplicate connect networks unnecessarily and it will be   an ECMP routing situation on which connect network will be used to   reach the other side.</li> <li>We will not provide any validation around selectors across all connect   APIs to rule out overlap except we will ensure the <code>connectSubnets</code> is   non-overlapping across all connectAPIs that select a given network   which is same as the first case.</li> </ul> </li> <li>Selecting a network can be done as a day0 or day2 operation. So label changes   to existing networks, new networks added and deleted need to be watched   in order to connect and disconnect networks on the fly based on user   configuration changes.</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#statusconditions","title":"StatusConditions","text":"<p>The <code>ClusterNetworkConnect</code> status will include a <code>status</code> field with a concise indication and a <code>conditions</code> array with detailed condition objects. The controller will update these conditions based on validation results.</p> <p>Condition Types</p> <ul> <li>The <code>Accepted</code> type condition will be done from the cluster-manager   which indicates whether the validation checks passed.</li> <li>The <code>Ready-In-Zone-&lt;nodename&gt;</code> type condition will be done from   each node's ovnkube-controller which indicates if all the corresponding   OVN objects were created correctly.</li> <li>Based on whether <code>status</code> of the above two types of conditions is   <code>True</code> or <code>False</code>, the final string <code>status</code> will be marked <code>Success</code> or   <code>Failure</code></li> </ul> <p>Valid Scenario</p> <p>When a <code>ClusterNetworkConnect</code> resource is successfully applied, the following conditions indicate healthy states:</p> <pre><code>status:\n  status: \"Success\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"True\"\n    reason: \"ValidationSucceeded\"\n    message: \"All validation checks passed successfully\"\n    lastTransitionTime: \"2023-10-01T09:59:00Z\"\n  - type: \"Ready-In-Zone-ovn-worker\"\n    status: \"True\"\n    reason: \"OVNSetupSucceeded\"\n    message: \"Connect router and all ports created successfully\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n  - type: \"Ready-In-Zone-ovn-worker2\"\n    status: \"True\"\n    reason: \"OVNSetupSucceeded\"\n    message: \"Connect router and all ports created successfully\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <p>Invalid Scenarios</p> <p>When validation fails or errors occur, the following error conditions will be reported:</p> <p>Cluster-Manager Validation Errors: <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"ConnectSubnetExhausted\"\n    message: \"Insufficient IP addresses in connect subnet 192.168.0.0/16.\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre></p> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"OverlappingNetworkSubnets\"\n    message: \"Cannot connect networks 'blue-network' and 'green-network': overlapping\n    pod subnets 10.1.0.0/16 detected.\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"ConnectSubnetConflict\"\n    message: \"Connect subnet 192.168.0.0/16 conflicts with pod subnet 192.168.1.0/24\n    of network 'blue-network'\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"ConnectSubnetOverlap\"\n    message: \"Connect subnet 192.168.0.0/16 overlaps with ClusterNetworkConnect\n    'enterprise-connect' which also selects network 'blue-network'.\n    Each ClusterNetworkConnect selecting the same network must use non-overlapping\n    connect subnets\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"IPFamilyMismatch\"\n    message: \"Cannot connect single-stack IPv4 network 'ipv4-network' with\n    single-stack IPv6 network 'ipv6-network'. Networks must use the same IP family\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"InsufficientNetworks\"\n    message: \"Only 1 network selected, minimum 2 networks required for connection\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"UnsupportedNetworkType\"\n    message: \"Network 'blue-network' has role 'Secondary'. Only 'Primary' role networks are supported\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"UnsupportedNetworkType\"\n    message: \"Network 'localnet-network' has type 'localnet'. Localnet networks are not supported\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"False\"\n    reason: \"UnsupportedNetworkType\"\n    message: \"Network connect feature is not supported when overlay tunneling is disabled\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre> <p>OVN Setup Failures: (Per Zone) <pre><code>status:\n  status: \"Failure\"\n  conditions:\n  - type: \"Accepted\"\n    status: \"True\"\n    reason: \"ValidationSucceeded\"\n    message: \"All validation checks passed successfully\"\n    lastTransitionTime: \"2023-10-01T09:59:00Z\"\n  - type: \"Ready-In-Zone-ovn-worker\"\n    status: \"False\"\n    reason: \"OVNSetupFailed\"\n    message: \"Failed to create connect router ports: tunnel key allocation failed\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n  - type: \"Ready-In-Zone-ovn-worker2\"\n    status: \"False\"\n    reason: \"OVNSetupFailed\"\n    message: \"Failed to install routing policies for network 'blue-network'\"\n    lastTransitionTime: \"2023-10-01T10:00:00Z\"\n</code></pre></p> <p>Accepted Condition Reasons (from cluster-manager):</p> <ul> <li><code>ValidationSucceeded</code> - All validation checks passed successfully</li> <li><code>ConnectSubnetExhausted</code> - Not enough IPs in connect subnet for all network-node combinations</li> <li><code>OverlappingNetworkSubnets</code> - Selected networks have overlapping pod subnets (not supported)</li> <li><code>ConnectSubnetConflict</code> - Connect subnet conflicts with existing network subnets, service CIDR, etc.</li> <li><code>ConnectSubnetOverlap</code> - Connect subnet overlaps with another ClusterNetworkConnect selecting the same network</li> <li><code>IPFamilyMismatch</code> - Attempting to connect networks with different IP families</li> <li><code>InsufficientNetworks</code> - Less than 2 networks selected for connection</li> <li><code>UnsupportedNetworkType</code> - Network has unsupported role (Secondary) or type (localnet) or no-overlay mode enabled</li> </ul> <p>Ready-In-Zone Condition Reasons (from ovnkube-controller):</p> <ul> <li><code>OVNSetupSucceeded</code> - OVN topology setup completed successfully</li> <li><code>OVNSetupFailed</code> - OVN topology setup failed</li> </ul> <p>NOTE: The actual messages and name types may vary in implementation subject to reviews. These snippets only represent some potential examples.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#implementation-details","title":"Implementation Details","text":"<p>The implementation details of the feature are covered in various sub sections below:</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#network-topology","title":"Network Topology","text":"<p>This section covers the topology changes that will be introduced to connect networks together.  NOTE: Besides the finalized proposal we have below for the topology change, there were other ideas which were discarded and listed in the alternatives section.</p> <p>Connecting Layer3 type networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue, green and yellow) across two nodes.</p> <p></p> <p>Let's take a look at how we can connect a blue network, green network and yellow network at the OVN layer.</p> <p>Add a new transit router between the ovn_cluster_routers of the networks</p> <p>When the <code>ClusterNetworkConnect</code> CR gets created selecting these three networks, the controller will make the following changes to the topology: * Create a distributed <code>connect-router-colored-enterprise</code> (basically   a transit-router type in OVN used for interconnection) * Connect the <code>connect-router-colored-enterprise</code> to the   <code>ovn_cluster_router</code>'s of the blue, green and yellow networks on each   node using peer ports and tunnelkeys. We will have a total of N*M   links where N is the number of networks and M is the number of nodes. * The port's will have IPs allocated from within the <code>connectSubnets</code>   value provided on the CR. For connecting N networks on M nodes we   would need 2\u00d7(N\u00d7M) IPs from the <code>connectSubnets</code>. See the controller   design section for more details.</p> <p></p> <p>So in this proposal it will be 1 new transit router per <code>ClusterNetworkConnect</code> API. NOTE: Please keep reading to the Pods and Services sections for details around routes, policies and other OVN object changes.</p> <p>In the OVN database of ovn-worker node, this new router looks like this:</p> <pre><code>sh-5.2# ovn-nbctl show connect-router-colored-enterprise\nrouter bb053a19-99d5-4af8-a8e9-ebcab52908d8 (connect-router-colored-enterprise)\n    port colored-enterprise-to-blue_blue-network-ovn-control-plane\n        mac: \"0a:58:c0:80:00:01\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:1\"\n        networks: [\"192.168.0.1/31\"]\n    port colored-enterprise-to-yellow_yellow-network-ovn-control-plane\n        mac: \"0a:58:c0:80:00:03\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:3\"\n        networks: [\"192.168.2.3/31\"]\n    port colored-enterprise-to-green_green-network-ovn-control-plane\n        mac: \"0a:58:c0:80:00:05\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:5\"\n        networks: [\"192.168.1.5/31\"]\n    port colored-enterprise-to-green_green-network-ovn-worker2\n        mac: \"0a:58:c0:80:00:11\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:11\"\n        networks: [\"192.168.1.17/31\"]\n    port colored-enterprise-to-blue_blue-network-ovn-worker\n        mac: \"0a:58:c0:80:00:07\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:7\"\n        networks: [\"192.168.0.7/31\"]\n    port colored-enterprise-to-blue_blue-network-ovn-worker2\n        mac: \"0a:58:c0:80:00:0d\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:d\"\n        networks: [\"192.168.0.13/31\"]\n    port colored-enterprise-to-yellow_yellow-network-ovn-worker2\n        mac: \"0a:58:c0:80:00:0f\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:f\"\n        networks: [\"192.168.2.15/31\"]\n    port colored-enterprise-to-yellow_yellow-network-ovn-worker\n        mac: \"0a:58:c0:80:00:09\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:9\"\n        networks: [\"192.168.2.9/31\"]\n    port colored-enterprise-to-green_green-network-ovn-worker\n        mac: \"0a:58:c0:80:00:0b\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:b\"\n        networks: [\"192.168.1.11/31\"]\n</code></pre> <p>The new ports added to the <code>ovn_cluster_router</code>'s of the 3 networks:</p> <pre><code>sh-5.2# ovn-nbctl show yellow_yellow.network_ovn_cluster_router\nrouter 01e9f2fc-e351-4c7a-a51a-ba5f2b2fb099 (yellow_yellow.network_ovn_cluster_router)\n    port yellow_yellow-network-ovn-worker-to-colored-enterprise\n        mac: \"0a:58:c0:80:02:08\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:8\"\n        networks: [\"192.168.2.8/31\"]\n---\n_uuid               : c78b105c-d2ee-4ba9-bb25-ec94dada0d15\ndhcp_relay          : []\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:c0:80:02:08\"\nname                : yellow_yellow-network-ovn-worker-to-colored-enterprise\nnetworks            : [\"192.168.2.8/31\"]\noptions             : {}\npeer                : colored-enterprise-to-yellow_yellow-network-ovn-worker --&gt; connects to connect-router\nstatus              : {}\n---\nsh-5.2# ovn-nbctl show blue_blue.network_ovn_cluster_router\nrouter 13b24c4d-f3c3-4811-98a9-98cc996be0d4 (blue_blue.network_ovn_cluster_router)\n    port blue_blue-network-ovn-worker-to-colored-enterprise\n        mac: \"0a:58:c0:80:00:06\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:6\"\n        networks: [\"192.168.0.6/31\"]\n---\n_uuid               : 24aabdb9-0b37-4332-965d-5d09ae0694a8\ndhcp_relay          : []\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:c0:80:00:06\"\nname                : blue_blue-network-ovn-worker-to-colored-enterprise\nnetworks            : [\"192.168.0.6/31\"]\noptions             : {}\npeer                : colored-enterprise-to-blue_blue-network-ovn-worker --&gt; connects to connect-router\nstatus              : {}\n---\nsh-5.2# ovn-nbctl show green_green.network_ovn_cluster_router\nrouter 15532710-213f-4bee-8bc1-9d500a6bc5a9 (green_green.network_ovn_cluster_router)\n    port green_green-network-ovn-worker-to-colored-enterprise\n        mac: \"0a:58:c0:80:01:0a\"\n        ipv6-lla: \"fe80::858:c0ff:fe80:a\"\n        networks: [\"192.168.1.10/31\"]\n---\n_uuid               : 99c4bff1-17c9-4d17-826c-c069f2a3cb6b\ndhcp_relay          : []\nenabled             : []\nexternal_ids        : {}\ngateway_chassis     : []\nha_chassis_group    : []\nipv6_prefix         : []\nipv6_ra_configs     : {}\nmac                 : \"0a:58:c0:80:01:0a\"\nname                : green_green-network-ovn-worker-to-colored-enterprise\nnetworks            : [\"192.168.1.10/31\"]\noptions             : {}\npeer                : colored-enterprise-to-green_green-network-ovn-worker --&gt; connects to connect-router\nstatus              : {}\n</code></pre> <p>Connecting Layer2 type networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue, green and yellow) across two nodes. NOTE: The topology representation is that of the new upcoming Layer2 topology. See enhancement on new Layer2 topology for more details on the new design.</p> <p></p> <p>Add a new transit router between the transit routers of the networks</p> <p>When the <code>ClusterNetworkConnect</code> CR gets created selecting these three networks, the controller will make the following changes to the topology: * Create a distributed <code>connect-router-colored-enterprise</code> (basically a   transit-router type in OVN used for interconnection). However note that   for pure layer2 network connections, we don't actually need a transit   router since there won't be any remote ports we need to create for   layer2 unlike in case of layer3. When we go from a pure layer2 to mixed   type connections we will need the remote ports (see the next section). * Connect the <code>connect-router-colored-enterprise</code> to the <code>transit-router</code>\u2019s   of the blue, green and yellow networks using patch ports and   tunnelkeys. In case of Layer2, we will have N links where N is the   number of networks. * The port's will have IPs allocated from within the <code>connectSubnets</code>   value provided on the CR. For connecting N networks on M nodes we   would need 2\u00d7N IPs from the <code>connectSubnets</code>. See the controller   design section for more details.</p> <p></p> <p>Connecting mixed type (layer3 and/or layer2) networks together</p> <p>The below diagram shows the overlay part of the OVN Topology that OVN-Kubernetes creates for 3 UDN networks (blue(l3), green(l3) and yellow(l2)) across two nodes.</p> <p></p> <p>Add a new transit router between the ovn_cluster_router's and transit-routers of the networks</p> <p>When the <code>ClusterNetworkConnect</code> CR gets created selecting these three networks, the controller will make the following changes to the topology: * Create a distributed <code>connect-router-colored-enterprise</code> (basically   a transit-router type in OVN used for interconnection). Remote ports   will be used only in layer3 type interconnection and not in layer2. * Connect the <code>connect-router-colored-enterprise</code> to the   <code>transit-router</code> of the yellow network and <code>ovn_cluster_router</code>'s of the   blue and green networks on each node using patch ports and tunnelkeys.   In case of mixed connection type, we will have N_L3\u00d7M+N_L2 links where   N_L3 is the number of Layer3 networks and N_L2 is the number of Layer2   networks * The port's will have IPs allocated from within the <code>connectSubnets</code>   value provided on the CR. For connecting N_L3 and N_L2 networks on M nodes   we would need 2\u00d7(N_L3\u00d7M+N_L2) IPs from the <code>connectSubnets</code>. See the   controller design section for more details.</p> <p></p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#pods","title":"Pods","text":"<p>In order to ensure pods in different connected networks can talk to each other we will need to add appropriate routes on the routers.</p> <ul> <li>Create policies on each connected network's <code>ovn_cluster_router</code> (L3)   or <code>transit-router</code> (L2) that steers the traffic towards the other   networks' subnet to the <code>connect-router</code>.</li> <li>On the interconect <code>connect-router</code> we will add specific per   network-node subnet routes that steers the traffic to the correct node.</li> </ul> <p>Layer3</p> <p>These routes will look like this in the L3 blue, green, yellow sample topology:</p> <ul> <li>Add logical router policies to <code>blue_blue.network_ovn_cluster_router</code>   that steers traffic towards the green and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>green_green.network_ovn_cluster_router</code>   that steers traffic towards the blue and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>yellow_yellow.network_ovn_cluster_router</code>   that steers traffic towards the blue and green UDN's subnet to connect   router on all nodes</li> <li>On the <code>connect-router-colored-enterprise</code> add routes to specific   node subnet slices of the blue, green and yellow ovn_cluster_routers on the   respective nodes.</li> </ul> <pre><code>sh-5.2# ovn-nbctl lr-policy-list green_green.network_ovn_cluster_router\nRouting Policies\n      9001 inport == \"rtos-green_green.network_ovn-worker\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute              192.168.0.11\n      9001 inport == \"rtos-green_green.network_ovn-worker\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute              192.168.0.11\n---\nsh-5.2# ovn-nbctl lr-policy-list blue_blue.network_ovn_cluster_router\nRouting Policies\n      9001 inport == \"rtos-blue_blue.network_ovn-worker\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute               192.168.0.7\n      9001 inport == \"rtos-blue_blue.network_ovn-worker\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute               192.168.0.7\n---\nsh-5.2# ovn-nbctl lr-policy-list yellow_yellow.network_ovn_cluster_router\nRouting Policies\n      9001 inport == \"rtos-yellow_yellow.network_ovn-worker\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute               192.168.0.9\n      9001 inport == \"rtos-yellow_yellow.network_ovn-worker\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute               192.168.0.9\n---\nsh-5.2# ovn-nbctl lr-route-list connect-router-colored-enterprise \nIPv4 Routes\nRoute Table &lt;main&gt;:\n           103.103.0.0/24              192.168.0.12 dst-ip\n           103.103.1.0/24               192.168.0.0 dst-ip\n           103.103.2.0/24               192.168.0.6 dst-ip\n           104.104.0.0/24               192.168.0.4 dst-ip\n           104.104.1.0/24              192.168.0.10 dst-ip\n           104.104.2.0/24              192.168.0.16 dst-ip\n           105.105.0.0/24              192.168.0.14 dst-ip\n           105.105.1.0/24               192.168.0.2 dst-ip\n           105.105.2.0/24               192.168.0.8 dst-ip\n</code></pre> <p>Layer2</p> <p>These routes will look like this in the L3 blue, green, yellow sample topology:</p> <ul> <li>Add logical router policies to <code>blue_blue.network_transit_router</code>   that steers traffic towards the green and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>green_green.network_transit_router</code>   that steers traffic towards the blue and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>yellow_yellow.network_transit_router</code>   that steers traffic towards the blue and green UDN's subnet to connect   router on all nodes</li> <li>On the <code>connect-router-colored-enterprise</code> add routes to specific   node subnet slices of the blue, green and yellow transit_routers. We   don't need per-node routes like with layer3, since transit router is   distributed across all nodes and layer2 is a flat network</li> </ul> <pre><code>sh-5.2# ovn-nbctl lr-policy-list green_green.network_transit_router\nRouting Policies\n      9001 inport == \"trtos-green_green.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute               192.168.0.7\n      9001 inport == \"trtos-green_green.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute               192.168.0.7\n---\nsh-5.2# ovn-nbctl lr-policy-list blue_blue.network_transit_router\nRouting Policies\n      9001 inport == \"trtos-blue_blue.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute              192.168.0.11\n      9001 inport == \"trtos-blue_blue.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute              192.168.0.11\n---\nsh-5.2# ovn-nbctl lr-policy-list yellow_yellow.network_transit_router\nRouting Policies\n      9001 inport == \"trtos-yellow_yellow.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute               192.168.0.9\n      9001 inport == \"trtos-yellow_yellow.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute               192.168.0.9\n---\nsh-5.2# ovn-nbctl lr-route-list connect-router-colored-enterprise\nIPv4 Routes\nRoute Table &lt;main&gt;:\n           103.103.0.0/16               192.168.0.4 dst-ip\n           104.104.0.0/16               192.168.0.0 dst-ip\n           105.105.0.0/16               192.168.0.2 dst-ip\n</code></pre> <p>Mixed</p> <p>These routes will look like this in the mixed blue, green, yellow sample topology:</p> <ul> <li>Add logical router policies to <code>blue_blue.network_ovn_cluster_router</code>   that steers traffic towards the green and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>green_green.network_ovn_cluster_router</code>   that steers traffic towards the blue and yellow UDN's subnet to connect   router on all nodes</li> <li>Add logical router policies to <code>yellow_yellow.network_transit_router</code>   that steers traffic towards the blue and green UDN's subnet to connect   router on all nodes</li> <li>On the <code>connect-router-colored-enterprise</code> add routes to specific   node subnet slices of the blue, green ovn_cluster_router on the respective   nodes and 1 global route to the distributed yellow transit_router</li> </ul> <pre><code>sh-5.2# ovn-nbctl lr-policy-list blue_blue.network_ovn_cluster_router\nRouting Policies\n      9001 inport == \"rtos-blue_blue.network_ovn-worker\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute              192.168.0.17\n      9001 inport == \"rtos-blue_blue.network_ovn-worker\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute              192.168.0.17\n---\nsh-5.2# ovn-nbctl lr-policy-list green_green.network_ovn_cluster_router\nRouting Policies\n      9001 inport == \"rtos-green_green.network_ovn-worker\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute              192.168.0.13\n      9001 inport == \"rtos-green_green.network_ovn-worker\" &amp;&amp; ip4.dst == 105.105.0.0/16         reroute              192.168.0.13\n---\nsh-5.2# ovn-nbctl lr-policy-list yellow_yellow.network_transit_router\nRouting Policies\n      9001 inport == \"trtos-yellow_yellow.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 103.103.0.0/16         reroute              192.168.0.15\n      9001 inport == \"trtos-yellow_yellow.network_ovn_layer2_switch\" &amp;&amp; ip4.dst == 104.104.0.0/16         reroute              192.168.0.15\n---\nsh-5.2# ovn-nbctl lr-route-list connect-router-colored-enterprise\nIPv4 Routes\nRoute Table &lt;main&gt;:\n           103.103.0.0/24               192.168.0.12 dst-ip\n           103.103.1.0/24               192.168.0.10 dst-ip\n           103.103.2.0/24               192.168.0.4 dst-ip\n           104.104.0.0/24               192.168.0.6 dst-ip\n           104.104.1.0/24               192.168.0.0 dst-ip\n           104.104.2.0/24               192.168.0.8 dst-ip\n           105.105.0.0/16               192.168.0.2 dst-ip\n</code></pre> <p>Overlapping Subnets</p> <p>Direct pod to pod connectivity of two networks is not supported in phase1. Conflict overlap detection and notification via API must be implemented.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#services","title":"Services","text":"<p>Case1: <code>ClusterIPServiceNetwork</code> is not set</p> <p>no-op. Services connectivity is not supported across connected UDNs.</p> <p>Case2: <code>ClusterIPServiceNetwork</code> and <code>PodNetwork</code> are set</p> <p>If the <code>ClusterIPServiceNetwork</code> is explicitly requested, then it means the end user wants the services of both networks to be connected. In this case, we need to modify the services controller to program the OVN load balancers on the local network switches also for connected networks so that once the DNAT to the backend happens it takes the same path as the policies and routes outlined in the Pods section.</p> <p>Example, if we have blue and green clusterIP services:</p> <pre><code>$ k get svc -n blue\nNAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice-blue   ClusterIP   10.96.210.158   &lt;none&gt;        80/TCP    57m\nk get svc -n green\nNAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE\nservice-green   ClusterIP   10.96.69.71   &lt;none&gt;        80/TCP    58m\nk get endpointslice -n blue\nNAME                 ADDRESSTYPE   PORTS   ENDPOINTS                 AGE\nservice-blue-czpg2   IPv4          8080    103.103.0.5,103.103.2.5   58m\nservice-blue-l26v9   IPv4          8080    10.244.2.3,10.244.1.6     58m\nk get endpointslice -n green\nNAME                  ADDRESSTYPE   PORTS   ENDPOINTS                 AGE\nservice-green-ngrrc   IPv4          8080    10.244.2.5,10.244.1.8     58m\nservice-green-rs2wv   IPv4          8080    104.104.2.4,104.104.1.4   58m\n---\nsh-5.2# ovn-nbctl ls-lb-list green_green.network_ovn-worker\nUUID                                    LB                  PROTO      VIP               IPs\nab3397dd-6c4f-41bd-9239-60c73c25aaee    green_green.netw    tcp        10.96.69.71:80    104.104.1.4:8080,104.104.2.4:8080\nsh-5.2# ovn-nbctl ls-lb-list blue_blue.network_ovn-worker\nUUID                                    LB                  PROTO      VIP                 IPs\n04aa1e0b-bfec-4270-94f0-4b6d511b63a3    blue_blue.networ    tcp        10.96.210.158:80    103.103.0.5:8080,103.103.2.5:8080\n</code></pre> <p>We need the controller to also add <code>ab3397dd-6c4f-41bd-9239-60c73c25aaee</code> to <code>blue_blue.network_ovn-worker</code> and <code>04aa1e0b-bfec-4270-94f0-4b6d511b63a3</code> to <code>green_green.network_ovn-worker</code> for the blue pods to be able to talk to green service and vice versa. The switches will DNAT the traffic and the policies and routes we added in the Pods section will take care of routing the DNAT-ed traffic to the correct network.</p> <pre><code>sh-5.2# ovn-nbctl ls-lb-add blue_blue.network_ovn-worker ab3397dd-6c4f-41bd-9239-60c73c25aaee\nsh-5.2# ovn-nbctl ls-lb-add green_green.network_ovn-worker 04aa1e0b-bfec-4270-94f0-4b6d511b63a3\nsh-5.2# ovn-nbctl ls-lb-list blue_blue.network_ovn-worker\nUUID                                    LB                  PROTO      VIP                 IPs\n04aa1e0b-bfec-4270-94f0-4b6d511b63a3    blue_blue.networ    tcp        10.96.210.158:80    103.103.0.5:8080,103.103.2.5:8080\nab3397dd-6c4f-41bd-9239-60c73c25aaee    green_green.netw    tcp        10.96.69.71:80      104.104.1.4:8080,104.104.2.4:8080\nsh-5.2# ovn-nbctl ls-lb-list green_green.network_ovn-worker\nUUID                                    LB                  PROTO      VIP                 IPs\n04aa1e0b-bfec-4270-94f0-4b6d511b63a3    blue_blue.networ    tcp        10.96.210.158:80    103.103.0.5:8080,103.103.2.5:8080\nab3397dd-6c4f-41bd-9239-60c73c25aaee    green_green.netw    tcp        10.96.69.71:80      104.104.1.4:8080,104.104.2.4:8080\n</code></pre> <p>Case3: <code>ClusterIPServiceNetwork</code> is set and <code>PodNetwork</code> is not set</p> <p>This means user is asking for partial connectivity between UDNs through serviceCIDRs. They don't want direct connectivity between all their applications.</p> <p>We will use ACLs in <code>Tier0</code> to allow traffic to service CIDR and drop all <code>connected_udn_subnet</code> pod traffic. These ACLs will be added on the local node switches for layer3 networks and the distributed flat switch for layer2 networks. They are applied in the <code>from-lport</code> direction for traffic originating from the pod's ports.</p> <p><pre><code>_uuid               : e88434bf-70c2-4853-810e-ecb2a64aaaa7\naction              : pass\ndirection           : from-lport\nexternal_ids        : {}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.dst==10.96.0.0/16\"\nmeter               : []\nname                : []\noptions             : {}\npriority            : 5000\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n---\nsh-5.2# ovn-nbctl list address-set connected_udn_subnets\n_uuid               : d340f83e-0adb-4f8c-9196-1e36ddcda7e9\naddresses           : [\"103.103.0.0/16\", \"104.104.0.0/16\", \"105.105.0.0/16\"]\nexternal_ids        : {}\nname                : connected_udn_subnets\n---\n_uuid               : cfe7b1cd-25cc-4f4f-8d15-f7b978ef43a1\naction              : drop\ndirection           : from-lport\nexternal_ids        : {}\nlabel               : 0\nlog                 : false\nmatch               : \"ip4.src==$connected_udn_subnets &amp;&amp; ip4.dst==$connected_udn_subnets &amp;&amp; ct.new\"\nmeter               : []\nname                : []\noptions             : {}\npriority            : 4950\nsample_est          : []\nsample_new          : []\nseverity            : []\ntier                : 0\n---\nsh-5.2# ovn-nbctl acl-list green_green.network_ovn-worker2\nfrom-lport  5000 (ip4.dst==10.96.0.0/16) pass --&gt; new ACL in tier0\nfrom-lport  4999 (ip4.src==$connected_udn_subnets &amp;&amp; ip4.dst==$connected_udn_subnets &amp;&amp; ct.new) drop --&gt; new ACL in tier0\n  to-lport  1001 (ip4.src==104.104.0.2) allow-related --&gt; tier2 (allow healthchecks from kubelet)\n---\nsh-5.2# ovn-nbctl acl-list blue_blue.network_ovn-worker2\nfrom-lport  5000 (ip4.dst==10.96.0.0/16) pass --&gt; new ACL in tier0\nfrom-lport  4999 (ip4.src==$connected_udn_subnets &amp;&amp; ip4.dst==$connected_udn_subnets &amp;&amp; ct.new) drop --&gt; new ACL in tier 0\n  to-lport  1001 (ip4.src==103.103.1.2) allow-related --&gt; tier2 (allow healthchecks from kubelet)\n</code></pre> These ACLs will be having <code>apply-after-lb=false</code> so the service DNAT will happen after the ACLs are applied. Similarly for the response the unDNAT will happen before ACLs for the replies. This guarantees traffic destined directly to the pods in other connected networks will be dropped but if its destined to a clusterIP service, then after the ACL stage, we will hit the load balancing stage in the pipeline where the DNAT to backend IP will happen. Remaining workflow is same as the Pods section where the traffic will be routed to the <code>connect-router</code>.</p> <p>These ACLs take precedence over other ACL features (ANPs, NetPols) and that is the defined behaviour since if full connectivity for PodNetwork is not requested, then network policies are a no-op and need not be honored for the cross-network connection case.</p> <p>NOTE: Since we always have a allow-related ACL on our switches: <pre><code>  to-lport  1001 (ip4.src==103.103.1.2) allow-related\n</code></pre> ideally we should not need the <code>ct.new</code> match in the drop ACL since ct session should automatically be maintained by OVN and replies shouldn't be dropped. However while doing the PoC I don't see this working as expected and when I remove the <code>ct.new</code> match, drop seems to be happening that too at an unexpected places in the pipeline which needs more investigation. During implementation phase, if we fix that behaviour then we can remove the ct.new match from the ACL.</p> <p>Traffic Path:</p> <pre><code>Onward Traffic Flow:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 podA \u2502\u2500\u2500\u2500\u25b6\u2502 UDNA switch  \u2502\u2500\u2500\u2500\u25b6\u2502 UDNA router \u2502\u2500\u2500\u2500\u25b6\u2502 connect router\u2502\u2500\u2500\u2500\u25b6\u2502 UDNB router \u2502\u2500\u2500\u2500\u25b6\u2502 UDNB switch  \u2502\u2500\u2500\u2500\u25b6\u2502 podB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              (DNAT to podB)         (LRPs)               (LRSRs)              (LRSR)\n\nResponse Traffic Flow:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 podA \u2502\u25c0\u2500\u2500\u2500\u2502 UDNA switch  \u2502\u25c0\u2500\u2500\u2500\u2502 UDNA router \u2502\u25c0\u2500\u2500\u2500\u2502 connect router\u2502\u25c0\u2500\u2500\u2500\u2502 UDNB router \u2502\u25c0\u2500\u2500\u2500\u2502 UDNB switch  \u2502\u25c0\u2500\u2500\u2500\u2502 podB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              (unSNAT to CIP)        (LRSR)                 (LRSR)             (LRP)\n</code></pre> <p>Overlapping Subnets</p> <p>We will not support connecting networks via services that have overlapping subnets in the first iteration. Proper user feedback indicating an error will be provided in case users try to connect networks with overlapping subnets.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#network-policies-and-admin-network-policies","title":"Network Policies and Admin Network Policies","text":"<p>If <code>PodNetwork</code> is set, then Network Policies and AdminNetworkPolicies (when they get supported on UDNs) that are created in the networks that are part of ClusterNetworkConnect API will have peers that span across all connect networks. So the scope of policies will be within all connected networks.</p> <p>If <code>PodNetwork</code> is not set, then policies won't apply across networks since there is no need to do so.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#controller-design-changes-to-components","title":"Controller Design / Changes to Components","text":"<p>Cluster Manager</p> <p>A new <code>network-connect-controller</code> will be created that watches the <code>ClusterNetworkConnect</code> objects and does the following:</p> <p>1) It is responsible to allocate a range for each network:   * It get's the <code>connectSubnets</code> value from the CNC and allocates     a <code>/networkPrefix</code> subnet for each layer3 connected network.     If its a layer2 type network, then allocator gives out a     <code>/networkPrefix</code> subnet and then from that gives out a <code>/31</code> or     <code>/127</code> slice subnet for each layer2 network. So until that     <code>/networkPrefix</code> subnet is exhausted, all subsequent layer2     networks will use that range.   * These values are annotated on the <code>ClusterNetworkConnect</code>     CRD for it to be then consumed on the <code>ovnkube-controller</code> side.     Sample:     <pre><code>k8s.ovn.org/network-subnets: {\n  layer3_&lt;network1-id&gt;: {\"ipv4\": \"192.168.0.0/24\", \"ipv6\":\"fd01:0:0:0::/64\"},\n  layer3_&lt;network2-id&gt;: {\"ipv4\": \"192.168.1.0/24\", \"ipv6\": \"fd01:0:0:1::/64\"}, \n  layer3_&lt;network5-id&gt;: {\"ipv4\": \"192.168.2.0/24\", \"ipv6\": \"fd01:0:0:2::/64\"},\n  layer3_&lt;network12-id&gt;: {\"ipv4\": \"192.168.3.0/24\", \"ipv6\": \"fd01:0:0:3::/64\"},\n  layer2_&lt;network40-id&gt;: {\"ipv4\": \"192.168.4.0/31\", \"ipv6\": \"fd01:0:0:4::/127\"},\n  layer2_&lt;network103-id&gt;: {\"ipv4\": \"192.168.4.2/31\", \"ipv6\": \"fd01:0:0:4::2/127\"},\n  layer3_&lt;network15-id&gt;: {\"ipv4\": \"192.168.5.0/24\", \"ipv6\": \"fd01:0:0:5::/64\"},\n  layer2_&lt;network2000-id&gt;: {\"ipv4\": \"192.168.4.4/31\", \"ipv6\": \"fd01:0:0:4::4/127\"},\n  ...\n}\n</code></pre>     Ideally speaking, we will need N_L3\u00d7M subnets for N_L3 Layer3     networks and M nodes and N_L2 subnets for Layer2 for N layer2 networks.     So a total of <code>N_L3*M+N_L2</code> subnets and <code>2x(N_L3*M+N_L2)</code> IPs.     However in this method, we plan to let CM give a slice for each     layer3 connected network based on <code>networkPrefix</code> and each     layer2 connected network a <code>/31</code>- so that means we     are going to consume <code>N_L3*M+N_L2</code> subnets and <code>2x(N_L3*M+N_L2)</code> IPs     where M is maximum nodes in your cluster. Users need to be careful because     this <code>networkPrefix</code> can limit the maximum number of nodes in your     cluster. On the node side from this <code>networkPrefix</code>, we will give out     a <code>/31</code> for each node. For layer3 networks, its possible the node-ids     are not contiguous so we will loose out on a few IPs in the middle sparsely.     This method was chosen so that we don't need to annotate a lot of     N*M values on our annotation which will make it bloat a lot     from performance and scale perspective. 2) It is responsible for allocating the global tunnel key for each   <code>connect-router</code>:   * It will use the same instance of global <code>TunnelKeysAllocator</code>     in cluster manager that already allocates the tunnel keys for     existing Layer2 transit routers. This allocator already     preserves the first 4096 IDs which are used as tunnel-keys     for Layer3 and Layer2 transit switches (derived from the networkID     that is allocated by the networkID allocator).     <code>TunnelKeysAllocator</code> will then allocate the new ones for the     connect router dynamically. This ensures we don't reuse     already allocated networkIDs. For each connect API that will     create a connect router, we will need 1 tunnel key per router.   * This value is annotated on the <code>ClusterNetworkConnect</code>     CRD for it to be then consumed on the <code>ovnkube-controller</code> side.     Sample:     <pre><code>k8s.ovn.org/connect-router-tunnel-key: '50000'\n</code></pre> 3) Status conditions will be updated based on feedback of setup  from the <code>ovnkube-controllers</code> in different zones.</p> <p>This controller also needs to watch for NADs to accommodate adds, updates and deletes for the network-subnet.</p> <p>OVN-Kubernetes Controller</p> <p>A new <code>network-connect-controller</code> will be created that watches the <code>ClusterNetworkConnect</code> objects and does the following:</p> <p>1) Based on the network selectors, creates the    necessary network topology changes mentioned    in the above section:    * Creates the <code>connect-router</code>    * Creates the ports connecting each Layer3 <code>ovn_cluster_router</code> and      each Layer2 <code>transit-router</code> to the <code>connect-router</code>    * Adds the necessary logical router policies on each network router    * Adds the necessary logical router static routes on the <code>connect-router</code> 2) Reads the <code>k8s.ovn.org/network-subnets</code>    annotation set by the cluster manager on CNC and allocates    a <code>/31</code> point to point subnet for each layer3 node-network's router port link    and uses the IPs from the <code>/31</code> to create the ports. For layer2 we only    need one <code>/31</code> instead of per node /31's which is already allocated by    CM. This allocator will be a static allocator util that deterministically    always picks the same <code>/31</code> for every <code>node-id</code>. Example:    <pre><code> worker-1 (node-id=0) \u2192 10.2.0.0/31 (10.2.0.0 + 10.2.0.1)\n worker-2 (node-id=1) \u2192 10.2.0.2/31 (10.2.0.2 + 10.2.0.3)\n worker-3 (node-id=2) \u2192 10.2.0.4/31 (10.2.0.4 + 10.2.0.5)\n worker-4 (node-id=5) \u2192 10.2.0.10/31 (10.2.0.10 + 10.2.0.11)\n</code></pre>    We can see that if node-ids are not contiguous, then we will loose    those IPs in the middle. But we determined this is still better than    having annotation patch issues at scale. However this static allocator    totally relies on maxNodes value and node-id not exceeding the range    user has provided. Offset used will be <code>generator.GenerateP2PSubnet(nodeID * 2)</code>.    For layer2 we will just use the annotation value directly. 3) Reads the <code>k8s.ovn.org/connect-router-tunnel-key</code> and uses    the tunnel key to create the <code>connect-router</code> 4) Within each <code>connect-router</code>, we would need N_L3*M tunnel keys to    connect N_L3 Layer3 networks on M nodes and N_L2 tunnel keys to connect    N_L2 Layer2 networks on M nodes. So total of <code>N_L3*M+N_L2</code> keys.    * This will be done statically based on subnets allocated out for each      node-network pair for layer3 networks and for each layer2 network. See the      Deterministic Static TunnelID Allocator approach mentioned below for details. 5) Creates the <code>pass</code> and <code>drop</code> ACLs if only partial service connectivity    is requested 6) If <code>ClusterIPServiceNetwork</code> is set, then once the services    controller of each network creates the load balancers (retries till    its created), this controller will also attach those load balancers    to the switches of the connected networks. This way, no changes are    required to existing services controller. However this means network-connect    controller will need to watch for endpointslice and services    objects.</p> <p>This controller watches for CNCs, NADs, Services, Endpointslices and Nodes to react to events. It will be a level driven controller.</p> <p>Deterministic Static Tunnel ID Allocation Algorithm used by ovnkube-controller</p> <p>The tunnel IDs for connect-router ports are allocated deterministically based on the existing subnet allocations. This ensures consistency across all ovnkube-controller instances without requiring coordination.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#algorithm-overview","title":"Algorithm Overview","text":"<ol> <li>Calculate maxNodes: <code>maxNodes = 2^(32 - NetworkPrefix)</code></li> <li>For NetworkPrefix=24: maxNodes=128</li> <li> <p>For NetworkPrefix=25: maxNodes=64</p> </li> <li> <p>Calculate network index: Based on the subnet's position in the NetworkPrefix range</p> </li> <li> <p>Allocate tunnel keys:</p> </li> <li>Layer3 networks: <code>tunnelKey = networkIndex * maxNodes + nodeID + 1</code></li> <li>Layer2 networks: <code>tunnelKey = networkIndex * maxNodes + subIndex + 1</code></li> </ol>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#example-allocation","title":"Example Allocation","text":"<p>Given NetworkPrefix=24 (maxNodes=128):</p> Network Subnet Type Index Tunnel Key Range network1 192.168.0.0/24 Layer3 0 [1, 128] network2 192.168.1.0/24 Layer3 1 [129, 256] network5 192.168.2.0/24 Layer3 2 [257, 384] network12 192.168.3.0/24 Layer3 3 [385, 512] network40 192.168.4.0/31 Layer2 4 [513] network103 192.168.4.2/31 Layer2 4 [514] network15 192.168.5.0/24 Layer3 5 [641, 768]"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#scale-limitations","title":"Scale Limitations","text":"<p>The maximum number of connectable networks is limited by: - Tunnel key limit: 32767 (reserved: 0 since 0 is invalid tunnelKey) - Available keys: 32766 - Max networks: <code>32766 / maxNodes</code></p> NetworkPrefix maxNodes Max Networks /24 128 255 /25 64 511 /26 32 1023"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#cross-feature-interaction","title":"Cross Feature Interaction","text":""},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#connecting-advertised-udns","title":"Connecting Advertised UDNs","text":"<p>The ACLs that are added to block traffic routing between the two UDNs in strict mode will be removed so that traffic can flow between networks once connected.</p> <p>A side caveat here is instead of the edge router doing the routing, OVN-Kubernetes itself will take care of routing the traffic. If that is not desired, then users should stick with loose isolation mode using BGP and not use this feature.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#connecting-udns-with-no-overlay-mode","title":"Connecting UDNs with no overlay mode","text":"<p>This feature will not be supported if overlay tunnel encapsulation is turned off. Cluster Manager will perform the necessary steps to validate and emit a message to the end user reporting an error if they tried to connect UDNs when overlay is turned off.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#testing-details","title":"Testing Details","text":"<ul> <li>Unit Testing details - All code modules will have necessary unit tests</li> <li>E2E Testing details - Entire feature will have E2E and Integration   testing</li> <li>API Testing details - CEL rules and API validation will have its own   apply test suite</li> <li>Scale Testing details - Bare minimum set of scale testing will be done   for this feature although we probably need to fix UDN Scale first</li> <li>Cross Feature Testing details - coverage for interaction with other   features like BGP and no-overlay mode</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#documentation-details","title":"Documentation Details","text":"<ul> <li>User guide will be updated for the feature on ovn-kubernetes.io   website</li> <li>Blog posts and easy to get started guides will be done on   ovn-kubernetes.io website</li> <li>Developer docs whereever possible will be added</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#risks-known-limitations-and-mitigations","title":"Risks, Known Limitations and Mitigations","text":""},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#scale-and-performance","title":"Scale and Performance","text":"<ul> <li>Adding a /16 route to each router for each connected peer network and   then adding /24 routes for each node-network combo on the transit   router has scale implications specially on clusters with large nodes.   A cluster with 500 nodes and 500 networks will end up with 250000   routes on each transit router distributed half on the node.</li> <li>In future there is a requirement to not create OVN constructs for UDNs   on those nodes where we know there won't be pod's scheduled. This is   to optimize resource consumption on large scale clusters with 1000 nodes   where only 10 nodes are going to host UDNs. Whatever topology we pick   here must work for that future enhancement as changing topologies is not   really encouraged. See Dynamic UDN Node Allocation for more details.</li> <li>The annotations we add to the CNC has cost of marshal and unmarshal   at each ovnkube-controller</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#ovn-kubernetes-version-skew","title":"OVN Kubernetes Version Skew","text":"<p>Targeting release 1.2</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#alternatives","title":"Alternatives","text":""},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#using-bgp-and-routeradvertisement-api","title":"Using BGP and RouterAdvertisement API","text":"<p>If admin wants to connect two UDNs together then they can simply expose the two UDNs using RouterAdvertisement CR and the router acting as gateway outside the cluster will learn the pod subnet routes from both these UDNs thereby acting as the router and being able to connect pods together. Today we add ACLs, but those could be removed to accommodate the connectivity.</p> <p>Pros</p> <ul> <li>No need for a new API to connect UDNs together</li> <li>Implementation changes are also almost nil</li> </ul> <p>Cons</p> <ul> <li>Some users might want to use BGP and UDNs only for the pods to be   directly accessible from external destinations and within the cluster   they might still want to respect segmentation - this is not possible   if we declare exposing UDNs subnets using BGP also means they are now   connected together. So basically the current API didn't account for   BGP to solve connecting UDNs problem.</li> <li>Some users might not want to use BGP at all because that makes   assumptions on having the right fabric at the cluster infrastructure   layer like presence of FRR-K8s which might not be the case always -   then how can they connect their UDNs together? Solution needs to be   OVN-native where-ever possible.</li> <li>Users cannot ask for partial v/s full connectivity across these UDNs</li> <li>example allow only services not pods.</li> <li>Hardware offload not possible since traffic is not curtailed only to   the OVS stack</li> </ul>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#using-alternative-connection-ideas-on-ovn-topology-level","title":"Using alternative connection ideas on OVN topology level","text":"<p>See discarded-alternatives.md for details on alternative topology connection ideas that were considered but discarded.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#using-nodenetwork-cr-instead-of-annotations-on-clusternetworkconnect-api","title":"Using NodeNetwork CR instead of Annotations on ClusterNetworkConnect API","text":"<p>NOTE: Having the <code>NodeNetwork</code> CRD as an internal data storage API might be more ideal to store the above information internally as this needs to be persisted into etcd on reboots. It will simplify the annotations and also help with scale since there are known issues around patching of annotation and waiting for reading and unmarshalling the annotation impacts scale. But this theory is not proven yet. During discussions we concluded it wouldn't be faster because the NetworkConnectController would be the only one watching for these CRs and it will need to process all the data and networks anyway.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#have-clustermanager-give-out-point-to-point-subnet-slices","title":"Have ClusterManager give out point to point subnet slices","text":"<p>Initial design had: <pre><code>    k8s.ovn.org/node-network-subnets: {\n      layer3_&lt;network1-id&gt;_&lt;node1name&gt;: 192.168.0.0/31,\n      layer3_&lt;network1-id&gt;_&lt;node2name&gt;: 192.168.0.2/31,\n      layer3_&lt;network1-id&gt;_&lt;node3name&gt;: 192.168.0.4/31,\n      layer3_&lt;network2-id&gt;_&lt;node1name&gt;: 192.168.0.6/31,\n      layer3_&lt;network2-id&gt;_&lt;node2name&gt;: 192.168.0.8/31,\n      layer3_&lt;network3-id&gt;_&lt;node3name&gt;: 192.168.0.10/31,\n      layer2_&lt;network1-id&gt;: 192.168.0.12/31,\n      layer2_&lt;network2-id&gt;: 192.168.0.14/31,\n      ...\n    }\n</code></pre> this style of annotation on the CNC object, but we decided against it since 5000 nodes (max nodes) x 4096 networks (max networks) could at worst case generate 2*20480000 values in annotation which won't scale well. Each network and node change would cause churn and frequent updates. For scale reasons we went with the hybrid model mentioned in the proposal. We would have needed N_L3\u00d7M subnets for N_L3 Layer3 networks and M nodes and N_L2 subnets for Layer2 for N layer2 networks. So a total of <code>N_L3*M+N_L2</code> subnets and <code>2x(N_L3*M+N_L2)</code> IPs. This had the advantage of the ranges being contiguous and no wasted IPs in connectSubnets range compared to the solution we have today.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#adding-load-balancers-on-transit-connect-routers-for-services","title":"Adding load balancers on transit connect routers for services","text":"<p>Alternative Idea2: Instead of doing the DNAT at the network switches, we could add the load balancers to the connect router.</p> <p>If <code>ClusterIPServiceNetwork</code> is set, then we add routes for each clusterIP that belongs to the other connected networks to be routed to the <code>connect-router</code> or a ClusterIP CIDR route. We would need to then either host all load balancers for all connected networks on the <code>connect-router</code> OR we would need to route the clusterIP to any random node-network pair belonging to the destination network where the DNAT and reroute would happen but that's complicated. If we go with adding load balancers to the connect router, then traffic path looks like this:</p> <pre><code>Forward Path (podA -&gt; podB):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 podA \u2502\u2500\u2500\u2500\u25b6\u2502 UDNA switch  \u2502\u2500\u2500\u2500\u25b6\u2502 UDNA router \u2502\u2500\u2500\u2500\u25b6\u2502 connect router\u2502\u2500\u2500\u2500\u25b6\u2502 UDNB router \u2502\u2500\u2500\u2500\u25b6\u2502 UDNB switch  \u2502\u2500\u2500\u2500\u25b6\u2502 podB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      (LRP)          (DNAT to podB +        (LRSR)\n                                                           LRSR)\n\nReturn Path (podB -&gt; podA):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 podA \u2502\u25c0\u2500\u2500\u2500\u2502 UDNA switch  \u2502\u25c0\u2500\u2500\u2500\u2502 UDNA router \u2502\u25c0\u2500\u2500\u2500\u2502 connect router\u2502\u25c0\u2500\u2500\u2500\u2502 UDNB router \u2502\u25c0\u2500\u2500\u2500\u2502 UDNB switch  \u2502\u25c0\u2500\u2500\u2500\u2502 podB \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                      (LRSR)        (unSNAT to cip +      (need LRPs\n                                                           LRSR)          for reply)\n</code></pre> <p>So in the end we'd need CIP routes and pod subnet routes + add the load balancers to connect router and also add ACLs to drop direct traffic towards pod subnets but only allow replies for service traffic for idea2 v/s for idea1 we don't need the CIP routes an rest remains the same except load balancers are added to the switches instead of routers. However, idea2 won't work because transit routers have same limitations as regular distributed routers in OVN when it comes to connection tracking. Given the Layer2 transit router to Connect router is a distributed port, traffic coming from the layer2 switch -&gt; layer2 router -&gt; connect router cannot be load balanced.</p>"},{"location":"okeps/okep-5224-connecting-udns/okep-5224-connecting-udns/#dependencies","title":"Dependencies","text":"<ul> <li>This enhancement depends on the new Layer2 topology changes.</li> </ul>"},{"location":"troubleshooting/debugging/","title":"Introduction","text":""},{"location":"troubleshooting/debugging/#initial-setup-issues","title":"Initial setup issues.","text":""},{"location":"troubleshooting/debugging/#each-host-should-have-unique-system-ids","title":"Each host should have unique system-ids.","text":"<p>In a OVN cluster, each host should have a unique system-id.  This value is set in external-ids:system-id of the Open_vSwitch table of each host's Open_vSwitch database.  You can fetch its value with:</p> <pre><code>ovs-vsctl get Open_vSwitch . external-ids:system-id\n</code></pre> <p>This is automatically set by OVS startup scripts (that come with packages). But if you installed OVS from source, and don't use a startup script, this will be empty.  If you clone your VM, then you may endup with same system-id for all your VMs - which is a problem.</p>"},{"location":"troubleshooting/debugging/#all-nodes-should-register-with-ovn-sb-database","title":"All nodes should register with OVN SB database.","text":"<p>On the master, run:</p> <pre><code>ovn-sbctl list chassis\n</code></pre> <p>You should see as many records as the number of nodes in your cluster.  The \"name\" column in each record represents the system-id of each host and they should all be unique.</p>"},{"location":"troubleshooting/debugging/#hosts-should-have-unique-node-names","title":"Hosts should have unique node names.","text":"<p>When you run 'ovnkube -init-node', 'ovnkube -init-master', 'ovnkube -init-network-control-manager' or 'ovnkube -init-cluster-manager' commands, it will ask for node names.  This should all be unique. This should also be the same as used by \"kubelet\" on each node. So the names you see when you run the below command should be the same ones that you supply to ovnkube:</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"troubleshooting/debugging/#check-the-presence-of-geneve-or-stt-kernel-module","title":"Check the presence of \"geneve\" or \"stt\" kernel module.","text":"<p>If you cannot ping between pods in 2 nodes, make sure to check whether you have the OVS kernel modules correctly installed by running:</p> <pre><code>lsmod | grep openvswitch\nlsmod | grep geneve  # Or \"stt\" if your encap type was chosen to be \"stt\".\n</code></pre> <p>If you do not see the kernel module, modprobe them</p> <pre><code>modprobe openvswitch\nmodprobe vport-geneve  # or vport-stt for \"stt\".\n</code></pre>"},{"location":"troubleshooting/debugging/#sanity-check-cross-host-ping","title":"Sanity check cross host ping.","text":"<p>Since on each host, a OVS internal device named \"k8s-$NODENAME\" is created with the second IP address in the subnet assigned for that node, you should be able to check cross host connectivity by pinging these internal devices across hosts.</p>"},{"location":"troubleshooting/debugging/#firewall-blocking-overlay-networks","title":"Firewall blocking overlay networks","text":"<p>If your host has a firewall blocking incoming connections as a default policy, you should open up ports to allow overlay networks.  If you use \"geneve\" as the encapsulation type, you should open up UDP port 6081.  If you use \"stt\" as the encapsulation type, you should open up TCP port 7471.</p> <p>You can use the following command to achieve it via iptables.</p> <pre><code># To open up Geneve port.\n/usr/share/openvswitch/scripts/ovs-ctl --protocol=udp \\\n        --dport=6081 enable-protocol\n\n# To open up STT port.\n/usr/share/openvswitch/scripts/ovs-ctl --protocol=tcp \\\n        --dport=7471 enable-protocol\n</code></pre>"},{"location":"troubleshooting/debugging/#check-ovn-northds-log-file","title":"Check ovn-northd's log file.","text":"<p>On the master, look at /var/log/openvswitch/ovn-northd.log to see for any errors with the setup of the OVN central node.</p>"},{"location":"troubleshooting/debugging/#runtime-issues","title":"Runtime issues","text":""},{"location":"troubleshooting/debugging/#check-the-watchers-log-file","title":"Check the watcher's log file.","text":"<p>On the master, check whether ovnkube is running by:</p> <pre><code>ps -ef | grep ovnkube\n</code></pre> <p>Check the watcher's log file at \"/var/log/ovn-kubernetes/ovnkube.log\" to see whether it is creating logical ports whenever a pod is created and for any obvious errors.</p>"},{"location":"troubleshooting/debugging/#check-the-ovn-cni-log-file","title":"Check the OVN CNI log file.","text":"<p>When you create a pod and it gets scheduled on a particular host, the OVN CNI plugin on that host, tries to access the pod's information from the K8s api server.  Specifically, it tries to get the IP address and mac address for that pod.  This information is logged in the OVN CNI log file on each node if you have specified a log file via \"/etc/openvswitch/ovn_k8s.conf\". You can read how to provide a logfile by reading 'man ovn_k8s.conf.5'.</p>"},{"location":"troubleshooting/debugging/#check-the-kubelets-log-file","title":"Check the kubelet's log file.","text":"<p>If there were any issues with downloading upstream CNI plugins, then kubelet will complain about them in its log file.</p>"},{"location":"troubleshooting/debugging/#check-ovn-controllers-log-file","title":"Check ovn-controller's log file.","text":"<p>If you suspect issues on only one of the host, look at the log file of ovn-controller at /var/log/openvswitch/ovn-controller.log to see any obvious error messages.</p>"},{"location":"troubleshooting/ovnkube-trace/","title":"ovnkube-trace","text":"<p>A tool to trace packet simulations for arbitrary UDP or TCP traffic between points in an ovn-kubernetes driven cluster.</p>"},{"location":"troubleshooting/ovnkube-trace/#usage","title":"Usage:","text":"<p>Given the command-line arguments, ovnkube-trace would inspect the cluster to determine the addresses (MAC and IP) of the source and destination and perform <code>ovn-trace</code>, <code>ovs-appctl ofproto/trace</code>, and <code>ovn-detrace</code> from/to both directions.</p> <pre><code>Usage of /tmp/go-build1564673416/b001/exe/ovnkube-trace:\n  -addr-family string\n        Address family (ip4 or ip6) to be used for tracing (default \"ip4\")\n  -dst string\n        dest: destination pod name\n  -dst-ip string\n        destination IP address (meant for tests to external targets)\n  -dst-namespace string\n        k8s namespace of dest pod (default \"default\")\n  -dst-port string\n        dst-port: destination port (default \"80\")\n  -dump-udn-vrf-table-ids\n        Dump the VRF table ID per node for all the user defined networks\n  -kubeconfig string\n        absolute path to the kubeconfig file\n  -loglevel string\n        loglevel: klog level (default \"0\")\n  -ovn-config-namespace string\n        namespace used by ovn-config itself\n  -service string\n        service: destination service name\n  -skip-detrace\n        skip ovn-detrace command\n  -src string\n        src: source pod name\n  -src-namespace string\n        k8s namespace of source pod (default \"default\")\n  -tcp\n        use tcp transport protocol\n  -udp\n        use udp transport protocol\n</code></pre> <p>Currently implemented loglevels are:  * <code>0</code> (minimal output) * <code>2</code> (more verbose output showing results of trace commands)  * and <code>5</code> (debug output)</p>"},{"location":"troubleshooting/ovnkube-trace/#example","title":"Example","text":"<p>In an environment between 2 pods in namespace <code>default</code>, where the pods are named <code>fedora-deployment-7d49fddf69-chmvh</code> and <code>fedora-deployment-7d49fddf69-t4hqw</code>, the goal would be to trace UDP traffic on port 53 between both pods. Each node in the cluster is running in a different interconnect zone. <pre><code># kubectl get pods -o wide\nNAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\nfedora-deployment-7d49fddf69-chmvh   1/1     Running   0          10m   10.244.2.3   ovn-worker2         &lt;none&gt;           &lt;none&gt;\nfedora-deployment-7d49fddf69-t4hqw   1/1     Running   0          10m   10.244.1.6   ovn-worker          &lt;none&gt;           &lt;none&gt;\nfedora-deployment-7d49fddf69-vwjt7   1/1     Running   0          10m   10.244.0.3   ovn-control-plane   &lt;none&gt;           &lt;none&gt;\n</code></pre></p> <p>The command that one would run in this case would be: <pre><code>ovnkube-trace \\\n  -src-namespace default \\\n  -src fedora-deployment-7d49fddf69-chmvh \\\n  -dst-namespace default \\\n  -dst fedora-deployment-7d49fddf69-t4hqw \\\n  -udp -dst-port 53 \\\n  -loglevel 0\n</code></pre></p> <p>The result with loglevel 0 would be for a successful trace: <pre><code># ovnkube-trace -src-namespace default -src fedora-deployment-7d49fddf69-chmvh -dst-namespace default -dst fedora-deployment-7d49fddf69-t4hqw -udp -dst-port 53 -loglevel 0\novn-trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-trace (remote) source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-trace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novn-trace (remote) destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novs-appctl ofproto/trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novs-appctl ofproto/trace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\novn-detrace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\novn-detrace destination pod to source pod indicates success from fedora-deployment-7d49fddf69-t4hqw to fedora-deployment-7d49fddf69-chmvh\n</code></pre></p> <p>In order to see the actual trace output of the <code>ovn-trace</code> and <code>ovs-appctl ofproto/trace</code> commands, one can increase the loglevel to <code>2</code>, and in order to see debug output for the ovnkube-trace application one can raise the loglevel to <code>5</code>: <pre><code># ovnkube-trace -src-namespace default -src fedora-deployment-7d49fddf69-chmvh -dst-namespace default -dst fedora-deployment-7d49fddf69-t4hqw -udp -dst-port 53 -loglevel 2\nI0823 21:33:18.112821 2457963 ovnkube-trace.go:1157] Log level set to: 2\nI0823 21:33:18.857705 2457963 ovnkube-trace.go:693] ovn-trace source pod to destination pod Output:\n# udp,reg14=0x3,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=52888,tp_dst=53\n\ningress(dp=\"ovn-worker2\", inport=\"default_fedora-deployment-7d49fddf69-chmvh\")\n------------------------------------------------------------------------------\n 0. ls_in_check_port_sec (northd.c:8583): 1, priority 50, uuid de664d3a\n    reg0[15] = check_in_port_sec();\n    next;\n 4. ls_in_pre_acl (northd.c:5991): ip, priority 100, uuid d9a60156\n    reg0[0] = 1;\n    next;\n 5. ls_in_pre_lb (northd.c:6178): ip, priority 100, uuid 4f6825b1\n    reg0[2] = 1;\n    next;\n 6. ls_in_pre_stateful (northd.c:6201): reg0[2] == 1, priority 110, uuid 82c039a6\n    ct_lb_mark;\n\nct_lb_mark /* default (use --ct to customize) */\n------------------------------------------------\n 7. ls_in_acl_hint (northd.c:6297): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 0b20013d\n    reg0[8] = 1;\n    reg0[10] = 1;\n    next;\n 9. ls_in_acl_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid 3eec76bb\n    reg8[30..31] = 1;\n    next(8);\n 9. ls_in_acl_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid b327f7af\n    reg8[30..31] = 2;\n    next(8);\n 9. ls_in_acl_action (northd.c:6753): 1, priority 0, uuid b0f710dc\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n15. ls_in_pre_hairpin (northd.c:7786): ip &amp;&amp; ct.trk, priority 100, uuid 88ef5011\n    reg0[6] = chk_lb_hairpin();\n    reg0[12] = chk_lb_hairpin_reply();\n    next;\n19. ls_in_acl_after_lb_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid 01461328\n    reg8[30..31] = 1;\n    next(18);\n19. ls_in_acl_after_lb_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid a22021af\n    reg8[30..31] = 2;\n    next(18);\n19. ls_in_acl_after_lb_action (northd.c:6753): 1, priority 0, uuid 2c98ee3d\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n27. ls_in_l2_lkup (northd.c:9407): eth.dst == { 0a:58:a9:fe:01:01, 0a:58:0a:f4:02:01 }, priority 50, uuid b29511a2\n    outport = \"stor-ovn-worker2\";\n    output;\n\negress(dp=\"ovn-worker2\", inport=\"default_fedora-deployment-7d49fddf69-chmvh\", outport=\"stor-ovn-worker2\")\n---------------------------------------------------------------------------------------------------------\n 0. ls_out_pre_acl (northd.c:5878): ip &amp;&amp; outport == \"stor-ovn-worker2\", priority 110, uuid 8b1bef96\n    next;\n 1. ls_out_pre_lb (northd.c:5878): ip &amp;&amp; outport == \"stor-ovn-worker2\", priority 110, uuid d7c53e71\n    next;\n 3. ls_out_acl_hint (northd.c:6297): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 26b08bb5\n    reg0[8] = 1;\n    reg0[10] = 1;\n    next;\n 5. ls_out_acl_action (northd.c:6764): reg8[30..31] == 0, priority 500, uuid ea58bb8e\n    reg8[30..31] = 1;\n    next(4);\n 5. ls_out_acl_action (northd.c:6764): reg8[30..31] == 1, priority 500, uuid 03897328\n    reg8[30..31] = 2;\n    next(4);\n 5. ls_out_acl_action (northd.c:6753): 1, priority 0, uuid bcfbe611\n    reg8[16] = 0;\n    reg8[17] = 0;\n    reg8[18] = 0;\n    reg8[30..31] = 0;\n    next;\n 9. ls_out_check_port_sec (northd.c:5843): 1, priority 0, uuid 79358872\n    reg0[15] = check_out_port_sec();\n    next;\n10. ls_out_apply_port_sec (northd.c:5848): 1, priority 0, uuid 12ba0dbe\n    output;\n    /* output to \"stor-ovn-worker2\", type \"patch\" */\n\ningress(dp=\"ovn_cluster_router\", inport=\"rtos-ovn-worker2\")\n-----------------------------------------------------------\n 0. lr_in_admission (northd.c:11790): eth.dst == { 0a:58:a9:fe:01:01, 0a:58:0a:f4:02:01 } &amp;&amp; inport == \"rtos-ovn-worker2\" &amp;&amp; is_chassis_resident(\"cr-rtos-ovn-worker2\"), priority 50, uuid e40942af\n    xreg0[0..47] = 0a:58:0a:f4:02:01;\n    next;\n 1. lr_in_lookup_neighbor (northd.c:11956): 1, priority 0, uuid 897d00f0\n    reg9[2] = 1;\n    next;\n 2. lr_in_learn_neighbor (northd.c:11965): reg9[2] == 1 || reg9[3] == 0, priority 100, uuid 234da6ee\n    next;\n12. lr_in_ip_routing_pre (northd.c:12190): 1, priority 0, uuid b1a3a6af\n    reg7 = 0;\n    next;\n13. lr_in_ip_routing (northd.c:10603): reg7 == 0 &amp;&amp; ip4.dst == 10.244.1.0/24, priority 73, uuid 1ed4e720\n    ip.ttl--;\n    reg8[0..15] = 0;\n    reg0 = 100.88.0.4;\n    reg1 = 100.88.0.2;\n    eth.src = 0a:58:a8:fe:00:02;\n    outport = \"rtots-ovn-worker2\";\n    flags.loopback = 1;\n    next;\n14. lr_in_ip_routing_ecmp (northd.c:12285): reg8[0..15] == 0, priority 150, uuid a1ea724a\n    next;\n15. lr_in_policy (northd.c:9741): ip4.src == 10.244.0.0/16 &amp;&amp; ip4.dst == 10.244.0.0/16, priority 102, uuid 1c6af09a\n    reg8[0..15] = 0;\n    next;\n16. lr_in_policy_ecmp (northd.c:12452): reg8[0..15] == 0, priority 150, uuid 3841a2fc\n    next;\n17. lr_in_arp_resolve (northd.c:12665): outport == \"rtots-ovn-worker2\" &amp;&amp; reg0 == 100.88.0.4, priority 100, uuid 792c14a1\n    eth.dst = 0a:58:a8:fe:00:04;\n    next;\n21. lr_in_arp_request (northd.c:13083): 1, priority 0, uuid f21b210a\n    output;\n\negress(dp=\"ovn_cluster_router\", inport=\"rtos-ovn-worker2\", outport=\"rtots-ovn-worker2\")\n---------------------------------------------------------------------------------------\n 0. lr_out_chk_dnat_local (northd.c:14444): 1, priority 0, uuid 2f6e84ed\n    reg9[4] = 0;\n    next;\n 6. lr_out_delivery (northd.c:13129): outport == \"rtots-ovn-worker2\", priority 100, uuid 81cdee53\n    output;\n    /* output to \"rtots-ovn-worker2\", type \"patch\" */\n\ningress(dp=\"transit_switch\", inport=\"tstor-ovn-worker2\")\n--------------------------------------------------------\n 0. ls_in_check_port_sec (northd.c:8583): 1, priority 50, uuid de664d3a\n    reg0[15] = check_in_port_sec();\n    next;\n 5. ls_in_pre_lb (northd.c:5875): ip &amp;&amp; inport == \"tstor-ovn-worker2\", priority 110, uuid 69169a39\n    next;\n27. ls_in_l2_lkup (northd.c:9329): eth.dst == 0a:58:a8:fe:00:04, priority 50, uuid da101703\n    outport = \"tstor-ovn-worker\";\n    output;\n\negress(dp=\"transit_switch\", inport=\"tstor-ovn-worker2\", outport=\"tstor-ovn-worker\")\n-----------------------------------------------------------------------------------\n 9. ls_out_check_port_sec (northd.c:5843): 1, priority 0, uuid 79358872\n    reg0[15] = check_out_port_sec();\n    next;\n10. ls_out_apply_port_sec (northd.c:5848): 1, priority 0, uuid 12ba0dbe\n    output;\n    /* output to \"tstor-ovn-worker\", type \"remote\" */\n\novn-trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\nI0823 21:33:18.858296 2457963 ovnkube-trace.go:704] Search string matched:\noutput to \"tstor-ovn-worker\"\n(...)\nI0823 21:33:19.169751 2457963 ovnkube-trace.go:693] ovs-appctl ofproto/trace source pod to destination pod Output:\nFlow: udp,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\n\nbridge(\"br-int\")\n----------------\n 0. in_port=7, priority 100, cookie 0x6c1d0b4a\n    set_field:0x13-&gt;reg13\n    set_field:0xb-&gt;reg11\n    set_field:0x6-&gt;reg12\n    set_field:0x3-&gt;metadata\n    set_field:0x3-&gt;reg14\n    resubmit(,8)\n 8. metadata=0x3, priority 50, cookie 0xde664d3a\n    set_field:0/0x1000-&gt;reg10\n    resubmit(,73)\n    73. ip,reg14=0x3,metadata=0x3,dl_src=0a:58:0a:f4:02:03,nw_src=10.244.2.3, priority 90, cookie 0x6c1d0b4a\n            set_field:0/0x1000-&gt;reg10\n    move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n     -&gt; NXM_NX_XXREG0[111] is now 0\n    resubmit(,9)\n 9. metadata=0x3, priority 0, cookie 0x57b52622\n    resubmit(,10)\n10. metadata=0x3, priority 0, cookie 0x98964ef0\n    resubmit(,11)\n11. metadata=0x3, priority 0, cookie 0x72c60524\n    resubmit(,12)\n12. ip,metadata=0x3, priority 100, cookie 0xd9a60156\n    set_field:0x1000000000000000000000000/0x1000000000000000000000000-&gt;xxreg0\n    resubmit(,13)\n13. ip,metadata=0x3, priority 100, cookie 0x4f6825b1\n    set_field:0x4000000000000000000000000/0x4000000000000000000000000-&gt;xxreg0\n    resubmit(,14)\n14. ip,reg0=0x4/0x4,metadata=0x3, priority 110, cookie 0x82c039a6\n    ct(table=15,zone=NXM_NX_REG13[0..15],nat)\n    nat\n     -&gt; A clone of the packet is forked to recirculate. The forked pipeline will be resumed at table 15.\n     -&gt; Sets the packet to an untracked state, and clears all the conntrack fields.\n\nFinal flow: udp,reg0=0x5,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\nMegaflow: recirc_id=0,eth,udp,in_port=7,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.128.0.0/9,nw_frag=no,tp_src=0x2000/0xe000\nDatapath actions: ct(zone=19,nat),recirc(0x12)\n\n===============================================================================\nrecirc(0x12) - resume conntrack with default ct_state=trk|new (use --ct-next to customize)\nReplacing src/dst IP/ports to simulate NAT:\n Initial flow: \n Modified flow: \n===============================================================================\n\nFlow: recirc_id=0x12,ct_state=new|trk,ct_zone=19,eth,udp,reg0=0x5,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\n\nbridge(\"br-int\")\n----------------\n    thaw\n        Resuming from table 15\n15. ct_state=+new-est+trk,metadata=0x3, priority 7, cookie 0xf14d8019\n    set_field:0x80000000000000000000000000/0x80000000000000000000000000-&gt;xxreg0\n    set_field:0x200000000000000000000000000/0x200000000000000000000000000-&gt;xxreg0\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0x3eec76bb\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0xb327f7af\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,16)\n16. ct_state=-est+trk,ip,metadata=0x3, priority 1, cookie 0x8dfd1f3e\n    set_field:0x2000000000000000000000000/0x2000000000000000000000000-&gt;xxreg0\n    resubmit(,17)\n17. metadata=0x3, priority 0, cookie 0xb0f710dc\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,18)\n18. metadata=0x3, priority 0, cookie 0xcae3d5b1\n    resubmit(,19)\n19. metadata=0x3, priority 0, cookie 0x66860608\n    resubmit(,20)\n20. metadata=0x3, priority 0, cookie 0x9db4b75e\n    resubmit(,21)\n21. metadata=0x3, priority 0, cookie 0x414a15a0\n    resubmit(,22)\n22. metadata=0x3, priority 0, cookie 0xebbc94cf\n    resubmit(,23)\n23. ct_state=+trk,ip,metadata=0x3, priority 100, cookie 0x88ef5011\n    set_field:0/0x80-&gt;reg10\n    resubmit(,68)\n    68. No match.\n            drop\n    move:NXM_NX_REG10[7]-&gt;NXM_NX_XXREG0[102]\n     -&gt; NXM_NX_XXREG0[102] is now 0\n    set_field:0/0x80-&gt;reg10\n    resubmit(,69)\n    69. No match.\n            drop\n    move:NXM_NX_REG10[7]-&gt;NXM_NX_XXREG0[108]\n     -&gt; NXM_NX_XXREG0[108] is now 0\n    resubmit(,24)\n24. metadata=0x3, priority 0, cookie 0x7fd98606\n    resubmit(,25)\n25. metadata=0x3, priority 0, cookie 0xd3d8976\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0x1461328\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0xa22021af\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,26)\n26. metadata=0x3, priority 0, cookie 0x1ecb6f2e\n    resubmit(,27)\n27. metadata=0x3, priority 0, cookie 0x2c98ee3d\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,28)\n28. ip,reg0=0x2/0x2002,metadata=0x3, priority 100, cookie 0xd8583947\n    ct(commit,zone=NXM_NX_REG13[0..15],nat(src),exec(set_field:0/0x1-&gt;ct_mark))\n    nat(src)\n    set_field:0/0x1-&gt;ct_mark\n     -&gt; Sets the packet to an untracked state, and clears all the conntrack fields.\n    resubmit(,29)\n29. metadata=0x3, priority 0, cookie 0x41080b67\n    resubmit(,30)\n30. metadata=0x3, priority 0, cookie 0x936e8520\n    resubmit(,31)\n31. metadata=0x3, priority 0, cookie 0x6d369d0e\n    resubmit(,32)\n32. metadata=0x3, priority 0, cookie 0x119a6138\n    resubmit(,33)\n33. metadata=0x3, priority 0, cookie 0x1d30f590\n    resubmit(,34)\n34. metadata=0x3, priority 0, cookie 0x74ef3d9\n    resubmit(,35)\n35. metadata=0x3,dl_dst=0a:58:0a:f4:02:01, priority 50, cookie 0xb29511a2\n    set_field:0x1-&gt;reg15\n    resubmit(,37)\n37. priority 0\n    resubmit(,38)\n38. priority 0\n    resubmit(,40)\n40. priority 0\n    resubmit(,41)\n41. reg15=0x1,metadata=0x3, priority 100, cookie 0x7c79d0e2\n    set_field:0xb-&gt;reg11\n    set_field:0x6-&gt;reg12\n    resubmit(,42)\n42. priority 0\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,43)\n43. ip,reg15=0x1,metadata=0x3, priority 110, cookie 0x8b1bef96\n    resubmit(,44)\n44. ip,reg15=0x1,metadata=0x3, priority 110, cookie 0xd7c53e71\n    resubmit(,45)\n45. metadata=0x3, priority 0, cookie 0xe59d971f\n    resubmit(,46)\n46. ct_state=-trk,metadata=0x3, priority 5, cookie 0xd4c65410\n    set_field:0x100000000000000000000000000/0x100000000000000000000000000-&gt;xxreg0\n    set_field:0x200000000000000000000000000/0x200000000000000000000000000-&gt;xxreg0\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. reg8=0/0xc0000000,metadata=0x3, priority 500, cookie 0xea58bb8e\n    set_field:0x4000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. reg8=0x40000000/0xc0000000,metadata=0x3, priority 500, cookie 0x3897328\n    set_field:0x8000000000000000/0xc000000000000000-&gt;xreg4\n    resubmit(,47)\n47. metadata=0x3, priority 0, cookie 0x17ae0ddf\n    resubmit(,48)\n48. metadata=0x3, priority 0, cookie 0xbcfbe611\n    set_field:0/0x1000000000000-&gt;xreg4\n    set_field:0/0x2000000000000-&gt;xreg4\n    set_field:0/0x4000000000000-&gt;xreg4\n    set_field:0/0xc000000000000000-&gt;xreg4\n    resubmit(,49)\n49. metadata=0x3, priority 0, cookie 0x39bbcf9\n    resubmit(,50)\n50. metadata=0x3, priority 0, cookie 0xaf9ffaea\n    resubmit(,51)\n51. metadata=0x3, priority 0, cookie 0xbee9000e\n    resubmit(,52)\n52. metadata=0x3, priority 0, cookie 0x79358872\n    set_field:0/0x1000-&gt;reg10\n    resubmit(,75)\n    75. No match.\n            drop\n    move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n     -&gt; NXM_NX_XXREG0[111] is now 0\n    resubmit(,53)\n53. metadata=0x3, priority 0, cookie 0x12ba0dbe\n    resubmit(,64)\n64. priority 0\n    resubmit(,65)\n65. reg15=0x1,metadata=0x3, priority 100, cookie 0x7c79d0e2\n    clone(ct_clear,set_field:0-&gt;reg11,set_field:0-&gt;reg12,set_field:0-&gt;reg13,set_field:0x8-&gt;reg11,set_field:0xe-&gt;reg12,set_field:0x1-&gt;metadata,set_field:0x2-&gt;reg14,set_field:0-&gt;reg10,set_field:0-&gt;reg15,set_field:0-&gt;reg0,set_field:0-&gt;reg1,set_field:0-&gt;reg2,set_field:0-&gt;reg3,set_field:0-&gt;reg4,set_field:0-&gt;reg5,set_field:0-&gt;reg6,set_field:0-&gt;reg7,set_field:0-&gt;reg8,set_field:0-&gt;reg9,resubmit(,8))\n    ct_clear\n    set_field:0-&gt;reg11\n    set_field:0-&gt;reg12\n    set_field:0-&gt;reg13\n    set_field:0x8-&gt;reg11\n    set_field:0xe-&gt;reg12\n    set_field:0x1-&gt;metadata\n    set_field:0x2-&gt;reg14\n    set_field:0-&gt;reg10\n    set_field:0-&gt;reg15\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,8)\n 8. reg14=0x2,metadata=0x1,dl_dst=0a:58:0a:f4:02:01, priority 50, cookie 0xe40942af\n    set_field:0xa580af402010000000000000000/0xffffffffffff0000000000000000-&gt;xxreg0\n    resubmit(,9)\n 9. metadata=0x1, priority 0, cookie 0x897d00f0\n    set_field:0x4/0x4-&gt;xreg4\n    resubmit(,10)\n10. reg9=0/0x8,metadata=0x1, priority 100, cookie 0x234da6ee\n    resubmit(,11)\n11. metadata=0x1, priority 0, cookie 0x7f28e0ff\n    resubmit(,12)\n12. metadata=0x1, priority 0, cookie 0x1ee2fb50\n    resubmit(,13)\n13. metadata=0x1, priority 0, cookie 0x17a7cfa8\n    resubmit(,14)\n14. metadata=0x1, priority 0, cookie 0x56f6cf85\n    resubmit(,15)\n15. metadata=0x1, priority 0, cookie 0x600f694f\n    resubmit(,16)\n16. metadata=0x1, priority 0, cookie 0xda085702\n    resubmit(,17)\n17. metadata=0x1, priority 0, cookie 0x30578698\n    resubmit(,18)\n18. metadata=0x1, priority 0, cookie 0x58ec3bea\n    resubmit(,19)\n19. metadata=0x1, priority 0, cookie 0xe6096bd7\n    resubmit(,20)\n20. metadata=0x1, priority 0, cookie 0xb1a3a6af\n    set_field:0/0xffffffff-&gt;xxreg1\n    resubmit(,21)\n21. ip,reg7=0,metadata=0x1,nw_dst=10.244.1.0/24, priority 73, cookie 0x1ed4e720\n    dec_ttl()\n    set_field:0/0xffff00000000-&gt;xreg4\n    set_field:0xa8fe0004000000000000000000000000/0xffffffff000000000000000000000000-&gt;xxreg0\n    set_field:0xa8fe00020000000000000000/0xffffffff0000000000000000-&gt;xxreg0\n    set_field:0a:58:a8:fe:00:02-&gt;eth_src\n    set_field:0x4-&gt;reg15\n    set_field:0x1/0x1-&gt;reg10\n    resubmit(,22)\n22. reg8=0/0xffff,metadata=0x1, priority 150, cookie 0xa1ea724a\n    resubmit(,23)\n23. ip,metadata=0x1,nw_src=10.244.0.0/16,nw_dst=10.244.0.0/16, priority 102, cookie 0x1c6af09a\n    set_field:0/0xffff00000000-&gt;xreg4\n    resubmit(,24)\n24. reg8=0/0xffff,metadata=0x1, priority 150, cookie 0x3841a2fc\n    resubmit(,25)\n25. reg0=0xa8fe0004,reg15=0x4,metadata=0x1, priority 100, cookie 0x792c14a1\n    set_field:0a:58:a8:fe:00:04-&gt;eth_dst\n    resubmit(,26)\n26. metadata=0x1, priority 0, cookie 0x8be057a3\n    resubmit(,27)\n27. metadata=0x1, priority 0, cookie 0x5ea30a7d\n    resubmit(,28)\n28. metadata=0x1, priority 0, cookie 0x79529c6a\n    resubmit(,29)\n29. metadata=0x1, priority 0, cookie 0xf21b210a\n    resubmit(,37)\n37. priority 0\n    resubmit(,38)\n38. priority 0\n    resubmit(,40)\n40. priority 0\n    resubmit(,41)\n41. reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n    set_field:0x8-&gt;reg11\n    set_field:0xe-&gt;reg12\n    resubmit(,42)\n42. priority 0\n    set_field:0-&gt;reg0\n    set_field:0-&gt;reg1\n    set_field:0-&gt;reg2\n    set_field:0-&gt;reg3\n    set_field:0-&gt;reg4\n    set_field:0-&gt;reg5\n    set_field:0-&gt;reg6\n    set_field:0-&gt;reg7\n    set_field:0-&gt;reg8\n    set_field:0-&gt;reg9\n    resubmit(,43)\n43. metadata=0x1, priority 0, cookie 0x2f6e84ed\n    set_field:0/0x10-&gt;xreg4\n    resubmit(,44)\n44. metadata=0x1, priority 0, cookie 0x90d7e25c\n    resubmit(,45)\n45. metadata=0x1, priority 0, cookie 0xe36440a6\n    resubmit(,46)\n46. metadata=0x1, priority 0, cookie 0xd3dbde6f\n    resubmit(,47)\n47. metadata=0x1, priority 0, cookie 0x519ebaf8\n    resubmit(,48)\n48. metadata=0x1, priority 0, cookie 0xc8fb3dc1\n    resubmit(,49)\n49. reg15=0x4,metadata=0x1, priority 100, cookie 0x81cdee53\n    resubmit(,64)\n64. reg10=0x1/0x1,reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n    push:NXM_OF_IN_PORT[]\n    set_field:ANY-&gt;in_port\n    resubmit(,65)\n    65. reg15=0x4,metadata=0x1, priority 100, cookie 0x93ec5715\n            clone(ct_clear,set_field:0-&gt;reg11,set_field:0-&gt;reg12,set_field:0-&gt;reg13,set_field:0x3-&gt;reg11,set_field:0x11-&gt;reg12,set_field:0xff0003-&gt;metadata,set_field:0x2-&gt;reg14,set_field:0-&gt;reg10,set_field:0-&gt;reg15,set_field:0-&gt;reg0,set_field:0-&gt;reg1,set_field:0-&gt;reg2,set_field:0-&gt;reg3,set_field:0-&gt;reg4,set_field:0-&gt;reg5,set_field:0-&gt;reg6,set_field:0-&gt;reg7,set_field:0-&gt;reg8,set_field:0-&gt;reg9,resubmit(,8))\n            ct_clear\n            set_field:0-&gt;reg11\n            set_field:0-&gt;reg12\n            set_field:0-&gt;reg13\n            set_field:0x3-&gt;reg11\n            set_field:0x11-&gt;reg12\n            set_field:0xff0003-&gt;metadata\n            set_field:0x2-&gt;reg14\n            set_field:0-&gt;reg10\n            set_field:0-&gt;reg15\n            set_field:0-&gt;reg0\n            set_field:0-&gt;reg1\n            set_field:0-&gt;reg2\n            set_field:0-&gt;reg3\n            set_field:0-&gt;reg4\n            set_field:0-&gt;reg5\n            set_field:0-&gt;reg6\n            set_field:0-&gt;reg7\n            set_field:0-&gt;reg8\n            set_field:0-&gt;reg9\n            resubmit(,8)\n         8. metadata=0xff0003, priority 50, cookie 0xde664d3a\n            set_field:0/0x1000-&gt;reg10\n            resubmit(,73)\n            73. No match.\n                    drop\n            move:NXM_NX_REG10[12]-&gt;NXM_NX_XXREG0[111]\n             -&gt; NXM_NX_XXREG0[111] is now 0\n            resubmit(,9)\n         9. metadata=0xff0003, priority 0, cookie 0x57b52622\n            resubmit(,10)\n        10. metadata=0xff0003, priority 0, cookie 0x98964ef0\n            resubmit(,11)\n        11. metadata=0xff0003, priority 0, cookie 0x72c60524\n            resubmit(,12)\n        12. metadata=0xff0003, priority 0, cookie 0x351dd7a3\n            resubmit(,13)\n        13. ip,reg14=0x2,metadata=0xff0003, priority 110, cookie 0x69169a39\n            resubmit(,14)\n        14. metadata=0xff0003, priority 0, cookie 0x5c78cf83\n            resubmit(,15)\n        15. metadata=0xff0003, priority 65535, cookie 0x8ac9010\n            resubmit(,16)\n        16. metadata=0xff0003, priority 65535, cookie 0x973723d7\n            resubmit(,17)\n        17. metadata=0xff0003, priority 0, cookie 0xc722ae8c\n            resubmit(,18)\n        18. metadata=0xff0003, priority 0, cookie 0xcae3d5b1\n            resubmit(,19)\n        19. metadata=0xff0003, priority 0, cookie 0x66860608\n            resubmit(,20)\n        20. metadata=0xff0003, priority 0, cookie 0x9db4b75e\n            resubmit(,21)\n        21. metadata=0xff0003, priority 0, cookie 0x414a15a0\n            resubmit(,22)\n        22. metadata=0xff0003, priority 0, cookie 0xebbc94cf\n            resubmit(,23)\n        23. metadata=0xff0003, priority 0, cookie 0x74d6cf40\n            resubmit(,24)\n        24. metadata=0xff0003, priority 0, cookie 0x7fd98606\n            resubmit(,25)\n        25. metadata=0xff0003, priority 0, cookie 0xd3d8976\n            resubmit(,26)\n        26. metadata=0xff0003, priority 0, cookie 0x1ecb6f2e\n            resubmit(,27)\n        27. metadata=0xff0003, priority 0, cookie 0xba824d52\n            resubmit(,28)\n        28. metadata=0xff0003, priority 0, cookie 0xa5e04afc\n            resubmit(,29)\n        29. metadata=0xff0003, priority 0, cookie 0x41080b67\n            resubmit(,30)\n        30. metadata=0xff0003, priority 0, cookie 0x936e8520\n            resubmit(,31)\n        31. metadata=0xff0003, priority 0, cookie 0x6d369d0e\n            resubmit(,32)\n        32. metadata=0xff0003, priority 0, cookie 0x119a6138\n            resubmit(,33)\n        33. metadata=0xff0003, priority 0, cookie 0x1d30f590\n            resubmit(,34)\n        34. metadata=0xff0003, priority 0, cookie 0x74ef3d9\n            resubmit(,35)\n        35. metadata=0xff0003,dl_dst=0a:58:a8:fe:00:04, priority 50, cookie 0xda101703\n            set_field:0x4-&gt;reg15\n            resubmit(,37)\n        37. priority 0\n            resubmit(,38)\n        38. priority 0\n            resubmit(,40)\n        40. reg15=0x4,metadata=0xff0003, priority 100, cookie 0xb6badb74\n            set_field:0xff0003/0xffffff-&gt;tun_id\n            set_field:0x4-&gt;tun_metadata0\n            move:NXM_NX_REG14[0..14]-&gt;NXM_NX_TUN_METADATA0[16..30]\n             -&gt; NXM_NX_TUN_METADATA0[16..30] is now 0x2\n            output:4\n             -&gt; output to kernel tunnel\n            resubmit(,41)\n        41. priority 0\n            drop\n    pop:NXM_OF_IN_PORT[]\n     -&gt; NXM_OF_IN_PORT[] is now 7\n\nFinal flow: recirc_id=0x12,eth,udp,reg0=0x300,reg11=0xb,reg12=0x6,reg13=0x13,reg14=0x3,reg15=0x1,metadata=0x3,in_port=7,vlan_tci=0x0000,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.6,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,tp_src=12345,tp_dst=53\nMegaflow: recirc_id=0x12,ct_state=+new-est-rel-rpl-inv+trk,ct_mark=0/0xf,eth,udp,in_port=7,dl_src=0a:58:0a:f4:02:03,dl_dst=0a:58:0a:f4:02:01,nw_src=10.244.2.3,nw_dst=10.244.1.0/24,nw_ecn=0,nw_ttl=64,nw_frag=no\nDatapath actions: ct(commit,zone=19,mark=0/0x1,nat(src)),set(tunnel(tun_id=0xff0003,dst=172.18.0.2,ttl=64,tp_dst=6081,geneve({class=0x102,type=0x80,len=4,0x20004}),flags(df|csum|key))),set(eth(src=0a:58:a8:fe:00:02,dst=0a:58:a8:fe:00:04)),set(ipv4(ttl=63)),5\n\novs-appctl ofproto/trace source pod to destination pod indicates success from fedora-deployment-7d49fddf69-chmvh to fedora-deployment-7d49fddf69-t4hqw\nI0823 21:33:19.170205 2457963 ovnkube-trace.go:704] Search string matched:\n-&gt; output to kernel tunnel\n(...)\n</code></pre></p>"},{"location":"troubleshooting/udn/","title":"User Defined Networks","text":"<p>To debug UDN the ovnkube-trace can dump multiple elements of the topology to  make easier to match to what network they belong.</p>"},{"location":"troubleshooting/udn/#local-gateway-vrf-table-id-numbers-for-networks","title":"Local gateway VRF table ID numbers for networks","text":"<p>Following command will dump the VRF table IDs from a local gateway system for UDNs.</p> <pre><code>ovnkube-trace -dump-udn-vrf-table-ids\n</code></pre> <p>The output will be tableIDs indexed by node and network name  <pre><code>{\n    \"ovn-control-plane\": {\n       \"net-blue\": 1,\n       \"net-red\": 2\n    },\n    \"ovn-worker\": {\n       \"net-blue\": 3,\n       \"net-red\": 4\n    }\n}\n</code></pre></p>"}]}