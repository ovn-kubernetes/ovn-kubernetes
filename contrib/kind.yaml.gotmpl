kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  # kube proxy will be disabled
  kubeProxyMode: "none"
  # the default CNI will not be installed
  disableDefaultCNI: true
{{- if .NetCIDR }}
  podSubnet: "{{ .NetCIDR }}"
{{- end }}
{{- if .SvcCIDR }}
  serviceSubnet: "{{ .SvcCIDR }}"
{{- end }}
{{- if .IPFamily }}
  ipFamily: {{ .IPFamily }}
{{- end }}
{{- if .UseLocalRegistry }}
containerdConfigPatches:
  - |-
    [plugins."io.containerd.grpc.v1.cri".registry.mirrors."localhost:{{ .KindLocalRegistryPort }}"]
      endpoint = ["http://{{ .KindLocalRegistryName }}:5000"]
{{- end }}
kubeadmConfigPatches:
- |
  kind: ClusterConfiguration
  metadata:
    name: config
  apiServer:
    extraArgs:
      "v": "{{ .ClusterLogLevel }}"
  controllerManager:
    extraArgs:
      "v": "{{ .ClusterLogLevel }}"
      # Disable service-lb-controller for now
      # https://github.com/kubernetes/kubernetes/issues/128121
      # Once the upstream issue is fixed we can remove this controller
      # customization fully. Tracked with
      # https://github.com/ovn-org/ovn-kubernetes/issues/4785
      "controllers": "*,bootstrap-signer-controller,token-cleaner-controller,-service-lb-controller"
  scheduler:
    extraArgs:
      "v": "{{ .ClusterLogLevel }}"
  networking: 
    dnsDomain: {{ .DNSDomain }}
  ---
  kind: InitConfiguration
  nodeRegistration:
    kubeletExtraArgs:
      "v": "{{ .ClusterLogLevel }}"
  ---
  kind: JoinConfiguration
  nodeRegistration:
    kubeletExtraArgs:
      "v": "{{ .ClusterLogLevel }}"
nodes:
 - role: control-plane
   kubeadmConfigPatches:
   - |
     kind: InitConfiguration
     nodeRegistration:
       kubeletExtraArgs:
         node-labels: "ingress-ready=true"
         authorization-mode: "AlwaysAllow"
{{- if .OVNHA }}
{{- range $i := .AdditionalMasterNodes }}
 - role: worker
{{- end }}
{{- end }}
{{- range $i := .WorkerNodes }}
 - role: worker
{{- end }}
